{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b45b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2023 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8790d7ed",
   "metadata": {},
   "source": [
    "# AutoMLOps\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/automlops/blob/main/example/automlops_example_notebook.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/automlops/blob/main/example/automlops_example_notebook.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/automlops/main/example/automlops_example_notebook.ipynb\">\n",
    "        <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "      Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</table>\n",
    "<br/><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f938540",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "In this tutorial, you will build two [Vertex AI](https://cloud.google.com/vertex-ai) pipelines, complete with an integrated CI/CD pipeline. This tutorial will walk you through how to use AutoMLOps to define, create and run pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22881a6a",
   "metadata": {},
   "source": [
    "# Objective\n",
    "In this tutorial, you will learn how to create and run MLOps pipelines integrated with CI/CD. This tutorial goes through an example pipeline that is defined two ways: first using a custom python syntax, and second using Kubeflow syntax (either option is valid, whichever is preferred is up to you). The example pipeline builds and deploys a classification model; the pipeline go through a very basic workflow:\n",
    "1. create_dataset: A custom component that will export the dataset from BQ to GCS as a csv.\n",
    "2. train_model: A custom component that will train a decision tree classifier on the training data.\n",
    "3. deploy_model: A custom component that will upload the saved_model to Vertex AI Model Registry and deploy it to an endpoint.\n",
    "\n",
    "# Prerequisites\n",
    "\n",
    "In order to use AutoMLOps, the following are required:\n",
    "\n",
    "- Jupyter (or Jupyter-compatible) notebook environment\n",
    "- [Notebooks API](https://console.cloud.google.com/marketplace/product/google/notebooks.googleapis.com) enabled\n",
    "- Python 3.0 - 3.10\n",
    "- [Google Cloud SDK 407.0.0](https://cloud.google.com/sdk/gcloud/reference)\n",
    "- [beta 2022.10.21](https://cloud.google.com/sdk/gcloud/reference/beta)\n",
    "- `git` installed\n",
    "- `git` logged-in:\n",
    "```\n",
    "  git config --global user.email \"you@example.com\"\n",
    "  git config --global user.name \"Your Name\"\n",
    "```\n",
    "- [Application Default Credentials (ADC)](https://cloud.google.com/docs/authentication/provide-credentials-adc) are setup. This can be done through the following commands:\n",
    "```\n",
    "gcloud auth application-default login\n",
    "gcloud config set account <account@example.com>\n",
    "```\n",
    "\n",
    "# Dependencies\n",
    "- `autoflake==2.0.0`,\n",
    "- `docopt==0.6.2`,\n",
    "- `ipython==7.34.0`,\n",
    "- `pipreqs==0.4.11`,\n",
    "- `pyflakes==3.0.1`,\n",
    "- `PyYAML==5.4.1`,\n",
    "- `yarg==0.1.9`\n",
    "\n",
    "# APIs & IAM\n",
    "AutoMLOps will enable the following APIs:\n",
    "- [cloudresourcemanager.googleapis.com](https://cloud.google.com/resource-manager/reference/rest)\n",
    "- [aiplatform.googleapis.com](https://cloud.google.com/vertex-ai/docs/reference/rest)\n",
    "- [artifactregistry.googleapis.com](https://cloud.google.com/artifact-registry/docs/reference/rest)\n",
    "- [cloudbuild.googleapis.com](https://cloud.google.com/build/docs/api/reference/rest)\n",
    "- [cloudscheduler.googleapis.com](https://cloud.google.com/scheduler/docs/reference/rest)\n",
    "- [cloudtasks.googleapis.com](https://cloud.google.com/tasks/docs/reference/rest)\n",
    "- [compute.googleapis.com](https://cloud.google.com/compute/docs/reference/rest/v1)\n",
    "- [iam.googleapis.com](https://cloud.google.com/iam/docs/reference/rest)\n",
    "- [iamcredentials.googleapis.com](https://cloud.google.com/iam/docs/reference/credentials/rest)\n",
    "- [ml.googleapis.com](https://cloud.google.com/ai-platform/training/docs/reference/rest)\n",
    "- [run.googleapis.com](https://cloud.google.com/run/docs/reference/rest)\n",
    "- [storage.googleapis.com](https://cloud.google.com/storage/docs/apis)\n",
    "- [sourcerepo.googleapis.com](https://cloud.google.com/source-repositories/docs/reference/rest)\n",
    "\n",
    "AutoMLOps will update [IAM privileges](https://cloud.google.com/iam/docs/understanding-roles) for the following accounts:\n",
    "1. Pipeline Runner Service Account (one is created if it does exist, defaults to: vertex-pipelines@PROJECT_ID.iam.gserviceaccount.com). Roles added:\n",
    "- roles/aiplatform.user\n",
    "- roles/artifactregistry.reader\n",
    "- roles/bigquery.user\n",
    "- roles/bigquery.dataEditor\n",
    "- roles/iam.serviceAccountUser\n",
    "- roles/storage.admin\n",
    "- roles/run.admin\n",
    "2. Cloudbuild Default Service Account (PROJECT_NUMBER@cloudbuild.gserviceaccount.com). Roles added:\n",
    "- roles/run.admin\n",
    "- roles/iam.serviceAccountUser\n",
    "- roles/cloudtasks.enqueuer\n",
    "- roles/cloudscheduler.admin\n",
    "\n",
    "# User Guide\n",
    "\n",
    "For a user-guide, please view these [slides](../AutoMLOps_Implementation_Guide_External.pdf).\n",
    "\n",
    "# Costs\n",
    "\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "- Vertex AI\n",
    "- Artifact Registry\n",
    "- Cloud Storage\n",
    "- Cloud Source Repository\n",
    "- Cloud Build\n",
    "- Cloud Run\n",
    "- Cloud Scheduler\n",
    "\n",
    "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing), and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage.\n",
    "\n",
    "# Ground-rules for using AutoMLOps\n",
    "1. Do not use variables, functions, code, etc. not defined within the scope of a custom component. These custom components will become containers and will have no reference to the out of scope code.\n",
    "2. Import statements and helper functions must be added inside the function. Provide parameter type hints.\n",
    "3. Test each of your components for accuracy and correctness before running them using AutoMLOps. We cannot fix bugs automatically; bugs are much more difficult to fix once they are made into pipelines.\n",
    "4. If you are using Kubeflow, be sure to define all the requirements needed to run the custom component - it can be easy to leave out packages which will cause the container to fail when running within a pipeline. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12381413",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "For training data, we are using the [dry beans dataset](https://archive.ics.uci.edu/ml/datasets/dry+bean+dataset) which contains images of seven different types of dry beans taken with a high-resolution camera. The raw dataset can be found [here](https://github.com/GoogleCloudPlatform/automlops/blob/main/example/data/Dry_Beans_Dataset.csv)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a231b629",
   "metadata": {},
   "source": [
    "# Setup Git\n",
    "Set up your git configuration below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f90b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git config --global user.email \"you@example.com\"\n",
    "!git config --global user.name \"Your Name\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e4d190",
   "metadata": {},
   "source": [
    "# Install AutoMLOps\n",
    "\n",
    "Install AutoMLOps from [PyPI](https://pypi.org/project/google-cloud-automlops/), or locally by cloning the repo and running `pip install .`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94451868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-cloud-automlops\n",
      "  Downloading google_cloud_automlops-1.0.5-py3-none-any.whl (35 kB)\n",
      "Requirement already satisfied: PyYAML==5.4.1 in /home/jupyter/.local/lib/python3.7/site-packages (from google-cloud-automlops) (5.4.1)\n",
      "Requirement already satisfied: yarg==0.1.9 in /home/jupyter/.local/lib/python3.7/site-packages (from google-cloud-automlops) (0.1.9)\n",
      "Requirement already satisfied: ipython==7.34.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-automlops) (7.34.0)\n",
      "Requirement already satisfied: docopt==0.6.2 in /home/jupyter/.local/lib/python3.7/site-packages (from google-cloud-automlops) (0.6.2)\n",
      "Requirement already satisfied: pipreqs==0.4.11 in /home/jupyter/.local/lib/python3.7/site-packages (from google-cloud-automlops) (0.4.11)\n",
      "Requirement already satisfied: autoflake==2.0.0 in /home/jupyter/.local/lib/python3.7/site-packages (from google-cloud-automlops) (2.0.0)\n",
      "Requirement already satisfied: pyflakes==3.0.1 in /home/jupyter/.local/lib/python3.7/site-packages (from google-cloud-automlops) (3.0.1)\n",
      "Requirement already satisfied: tomli>=2.0.1 in /opt/conda/lib/python3.7/site-packages (from autoflake==2.0.0->google-cloud-automlops) (2.0.1)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.7/site-packages (from ipython==7.34.0->google-cloud-automlops) (4.8.0)\n",
      "Requirement already satisfied: traitlets>=4.2 in /opt/conda/lib/python3.7/site-packages (from ipython==7.34.0->google-cloud-automlops) (5.7.1)\n",
      "Requirement already satisfied: pygments in /opt/conda/lib/python3.7/site-packages (from ipython==7.34.0->google-cloud-automlops) (2.13.0)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.7/site-packages (from ipython==7.34.0->google-cloud-automlops) (5.1.1)\n",
      "Requirement already satisfied: setuptools>=18.5 in /opt/conda/lib/python3.7/site-packages (from ipython==7.34.0->google-cloud-automlops) (65.5.1)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.7/site-packages (from ipython==7.34.0->google-cloud-automlops) (0.7.5)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.7/site-packages (from ipython==7.34.0->google-cloud-automlops) (0.2.0)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from ipython==7.34.0->google-cloud-automlops) (3.0.36)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.7/site-packages (from ipython==7.34.0->google-cloud-automlops) (0.1.6)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.7/site-packages (from ipython==7.34.0->google-cloud-automlops) (0.18.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from yarg==0.1.9->google-cloud-automlops) (2.28.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.7/site-packages (from jedi>=0.16->ipython==7.34.0->google-cloud-automlops) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.7/site-packages (from pexpect>4.3->ipython==7.34.0->google-cloud-automlops) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.7/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython==7.34.0->google-cloud-automlops) (0.2.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->yarg==0.1.9->google-cloud-automlops) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->yarg==0.1.9->google-cloud-automlops) (1.26.13)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->yarg==0.1.9->google-cloud-automlops) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->yarg==0.1.9->google-cloud-automlops) (2.1.1)\n",
      "Installing collected packages: google-cloud-automlops\n",
      "Successfully installed google-cloud-automlops-1.0.5\n"
     ]
    }
   ],
   "source": [
    "!pip3 install google-cloud-automlops --user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17db55d5",
   "metadata": {},
   "source": [
    "# Restart the kernel\n",
    "Once you've installed the AutoMLOps package, you need to restart the notebook kernel so it can find the package.\n",
    "\n",
    "**Note: Once this cell has finished running, continue on. You do not need to re-run any of the cells above.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57c53b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.getenv(\"IS_TESTING\"):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250d511b",
   "metadata": {},
   "source": [
    "# Set your project ID\n",
    "Set your project ID below. If you don't know your project ID, leave the field blank and the following cells may be able to find it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "931ff517",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be0be295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project ID: automlops-sandbox\n"
     ]
    }
   ],
   "source": [
    "if PROJECT_ID == \"\" or PROJECT_ID is None or PROJECT_ID == \"[your-project-id]\":\n",
    "    # Get your GCP project id from gcloud\n",
    "    shell_output = !gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID:\", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c36482d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\r\n"
     ]
    }
   ],
   "source": [
    "! gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503f4a85-c357-4ec4-8ea5-0eeb2c8afd39",
   "metadata": {},
   "source": [
    "# Upload Data\n",
    "This will create a BQ table and upload the Dry Beans csv. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28557b2d-598c-466b-8116-bc1e34ba090d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset automlops-sandbox.test_dataset already exists\n",
      "Table test_dataset.dry-beans already exists\n"
     ]
    }
   ],
   "source": [
    "!python3 -m data.load_data_to_bq --project $PROJECT_ID --file data/Dry_Beans_Dataset.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3fe994",
   "metadata": {},
   "source": [
    "# 1. AutoMLOps Pipeline\n",
    "This workflow will define and generate a pipeline without using Kubeflow spec. `generate()` will create all the necessary files but not run them. `go()` will create all the necessary files, resources, push the code to the source repo to trigger the build, and then submit a Pipeline training job to Vertex AI. Please see the [readme](https://github.com/GoogleCloudPlatform/automlops/blob/main/README.md) for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2219ee67",
   "metadata": {},
   "source": [
    "## Import AutoMLOps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd5ac7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from AutoMLOps import AutoMLOps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac0c712-acdd-4830-a9bd-44f41eaa590c",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "Define a custom component for loading and creating a dataset. Import statements and helper functions must be added inside the function. Provide parameter type hints.\n",
    "\n",
    "**Note: we currently only support python primitive types for component parameters. If you would like to use something more advanced, please use the Kubeflow spec instead (see below in this notebook).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94a96074-761f-40f0-84bb-adfcb9bf6911",
   "metadata": {},
   "outputs": [],
   "source": [
    "@AutoMLOps.component\n",
    "def create_dataset(\n",
    "    bq_table: str,\n",
    "    data_path: str,\n",
    "    project_id: str\n",
    "):\n",
    "    \"\"\"Custom component that takes in a BQ table and writes it to GCS.\n",
    "\n",
    "    Args:\n",
    "        bq_table: The source biquery table.\n",
    "        data_path: The gcs location to write the csv.\n",
    "        project_id: The project ID.\n",
    "    \"\"\"\n",
    "    from google.cloud import bigquery\n",
    "    import pandas as pd\n",
    "    \n",
    "    bq_client = bigquery.Client(project=project_id)\n",
    "\n",
    "    def get_query(bq_input_table: str) -> str:\n",
    "        \"\"\"Generates BQ Query to read data.\n",
    "\n",
    "        Args:\n",
    "        bq_input_table: The full name of the bq input table to be read into\n",
    "        the dataframe (e.g. <project>.<dataset>.<table>)\n",
    "        Returns: A BQ query string.\n",
    "        \"\"\"\n",
    "        return f\"\"\"\n",
    "        SELECT *\n",
    "        FROM `{bq_input_table}`\n",
    "        \"\"\"\n",
    "\n",
    "    def load_bq_data(query: str, client: bigquery.Client) -> pd.DataFrame:\n",
    "        \"\"\"Loads data from bq into a Pandas Dataframe for EDA.\n",
    "        Args:\n",
    "        query: BQ Query to generate data.\n",
    "        client: BQ Client used to execute query.\n",
    "        Returns:\n",
    "        pd.DataFrame: A dataframe with the requested data.\n",
    "        \"\"\"\n",
    "        df = client.query(query).to_dataframe()\n",
    "        return df\n",
    "\n",
    "    dataframe = load_bq_data(get_query(bq_table), bq_client)\n",
    "    dataframe.to_csv(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4476cb4-91c5-42ff-a500-8cc275fedbd1",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "Define a custom component for training a model. Import statements and helper functions must be added inside the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5371be73-db3f-4d79-bde7-94fcd5ea13b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@AutoMLOps.component\n",
    "def train_model(\n",
    "    data_path: str,\n",
    "    model_directory: str\n",
    "):\n",
    "    \"\"\"Custom component that trains a decision tree on the training data.\n",
    "\n",
    "    Args:\n",
    "        data_path: GS location where the training data.\n",
    "        model_directory: GS location of saved model.\n",
    "    \"\"\"\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    import pandas as pd\n",
    "    import tensorflow as tf\n",
    "    import pickle\n",
    "    import os\n",
    "    \n",
    "    def save_model(model, uri):\n",
    "        \"\"\"Saves a model to uri.\"\"\"\n",
    "        with tf.io.gfile.GFile(uri, 'w') as f:\n",
    "            pickle.dump(model, f)\n",
    "    \n",
    "    df = pd.read_csv(data_path)\n",
    "    labels = df.pop(\"Class\").tolist()\n",
    "    data = df.values.tolist()\n",
    "    x_train, x_test, y_train, y_test = train_test_split(data, labels)\n",
    "    skmodel = DecisionTreeClassifier()\n",
    "    skmodel.fit(x_train,y_train)\n",
    "    score = skmodel.score(x_test,y_test)\n",
    "    print('accuracy is:',score)\n",
    "\n",
    "    output_uri = os.path.join(model_directory, f'model.pkl')\n",
    "    save_model(skmodel, output_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ed1eaa",
   "metadata": {},
   "source": [
    "## Uploading & Deploying the Model\n",
    "Define a custom component for uploading and deploying a model in Vertex AI. Import statements and helper functions must be added inside the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "47377d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@AutoMLOps.component\n",
    "def deploy_model(\n",
    "    model_directory: str,\n",
    "    project_id: str,\n",
    "    region: str\n",
    "):\n",
    "    \"\"\"Custom component that trains a decision tree on the training data.\n",
    "\n",
    "    Args:\n",
    "        model_directory: GS location of saved model.\n",
    "        project_id: Project_id.\n",
    "        region: Region.\n",
    "    \"\"\"\n",
    "    from google.cloud import aiplatform\n",
    "\n",
    "    aiplatform.init(project=project_id, location=region)\n",
    "    deployed_model = aiplatform.Model.upload(\n",
    "        artifact_uri = model_directory,\n",
    "        parent_model=\"beans-model-pipeline\",\n",
    "        is_default_version=False,\n",
    "        version_aliases=[\"experimental\", \"challenger\", \"custom-training\", \"naive-bayes\"],\n",
    "        version_description=\"Simple Decision Tree classifier\",\n",
    "        serving_container_image_uri=\"us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.0-24:latest\",\n",
    "        serving_container_health_route=\"/health\",\n",
    "        serving_container_predict_route=\"/predict\",\n",
    "        serving_container_ports=[9999],\n",
    "        labels={\"created_by\": \"inardini\", \"team\": \"advocacy\"},\n",
    "    )\n",
    "    endpoint = deployed_model.deploy(machine_type=\"n1-standard-4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d96dcb-a020-4bab-b0e3-1e32f6b2aecf",
   "metadata": {},
   "source": [
    "## Define the Pipeline\n",
    "Define your pipeline. You can optionally give the pipeline a name and description. Define the structure by listing the components to be called in your pipeline; use `.after` to specify the order of execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a36e0154",
   "metadata": {},
   "outputs": [],
   "source": [
    "@AutoMLOps.pipeline #(name='automlops-pipeline', description='This is an optional description')\n",
    "def pipeline(bq_table: str,\n",
    "             model_directory: str,\n",
    "             data_path: str,\n",
    "             project_id: str,\n",
    "             region: str,\n",
    "            ):\n",
    "\n",
    "    create_dataset_task = create_dataset(\n",
    "        bq_table=bq_table,\n",
    "        data_path=data_path,\n",
    "        project_id=project_id)\n",
    "\n",
    "    train_model_task = train_model(\n",
    "        model_directory=model_directory,\n",
    "        data_path=data_path).after(create_dataset_task)\n",
    "\n",
    "    deploy_model_task = deploy_model(\n",
    "        model_directory=model_directory,\n",
    "        project_id=project_id,\n",
    "        region=region).after(train_model_task)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3874f4f6",
   "metadata": {},
   "source": [
    "## Define the Pipeline Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4cb3786b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "pipeline_params = {\n",
    "    \"bq_table\": f\"{PROJECT_ID}.test_dataset.dry-beans\",\n",
    "    \"model_directory\": f\"gs://{PROJECT_ID}-bucket/trained_models/{datetime.datetime.now()}\",\n",
    "    \"data_path\": f\"gs://{PROJECT_ID}-bucket/data\",\n",
    "    \"project_id\": f\"{PROJECT_ID}\",\n",
    "    \"region\": \"us-central1\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e0aa05",
   "metadata": {},
   "source": [
    "## Generate and Run the pipeline\n",
    "`AutoMLOps.generate` generates the code for the MLOps pipeline. `AutoMLOps.go` generates the code and runs the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db51a8d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Successfully saved requirements file in AutoMLOps/components/component_base/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "AutoMLOps.generate(project_id=PROJECT_ID, pipeline_params=pipeline_params, use_kfp_spec=False, run_local=False, schedule_pattern='0 */12 * * *')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3ff5ba03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Model\n",
      "Creating Model\n",
      "Create Model backing LRO: projects/45373616427/locations/us-central1/models/beans-model/operations/6901924552461254656\n",
      "Create Model backing LRO: projects/45373616427/locations/us-central1/models/beans-model/operations/6901924552461254656\n",
      "Model created. Resource name: projects/45373616427/locations/us-central1/models/beans-model@3\n",
      "Model created. Resource name: projects/45373616427/locations/us-central1/models/beans-model@3\n",
      "To use this Model in another session:\n",
      "To use this Model in another session:\n",
      "model = aiplatform.Model('projects/45373616427/locations/us-central1/models/beans-model@3')\n",
      "model = aiplatform.Model('projects/45373616427/locations/us-central1/models/beans-model@3')\n",
      "Creating Endpoint\n",
      "Creating Endpoint\n",
      "Create Endpoint backing LRO: projects/45373616427/locations/us-central1/endpoints/6660168539950809088/operations/4379908761133776896\n",
      "Create Endpoint backing LRO: projects/45373616427/locations/us-central1/endpoints/6660168539950809088/operations/4379908761133776896\n",
      "Endpoint created. Resource name: projects/45373616427/locations/us-central1/endpoints/6660168539950809088\n",
      "Endpoint created. Resource name: projects/45373616427/locations/us-central1/endpoints/6660168539950809088\n",
      "To use this Endpoint in another session:\n",
      "To use this Endpoint in another session:\n",
      "endpoint = aiplatform.Endpoint('projects/45373616427/locations/us-central1/endpoints/6660168539950809088')\n",
      "endpoint = aiplatform.Endpoint('projects/45373616427/locations/us-central1/endpoints/6660168539950809088')\n",
      "Deploying model to Endpoint : projects/45373616427/locations/us-central1/endpoints/6660168539950809088\n",
      "Deploying model to Endpoint : projects/45373616427/locations/us-central1/endpoints/6660168539950809088\n",
      "Deploy Endpoint model backing LRO: projects/45373616427/locations/us-central1/endpoints/6660168539950809088/operations/7648396190697914368\n",
      "Deploy Endpoint model backing LRO: projects/45373616427/locations/us-central1/endpoints/6660168539950809088/operations/7648396190697914368\n"
     ]
    },
    {
     "ename": "TimeoutError",
     "evalue": "Operation did not complete within the designated timeout of 900 seconds.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_InactiveRpcError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/google/api_core/grpc_helpers.py\u001b[0m in \u001b[0;36merror_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcallable_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRpcError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/grpc/_channel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m    945\u001b[0m                                       wait_for_ready, compression)\n\u001b[0;32m--> 946\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_end_unary_response_blocking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/grpc/_channel.py\u001b[0m in \u001b[0;36m_end_unary_response_blocking\u001b[0;34m(state, call, with_call, deadline)\u001b[0m\n\u001b[1;32m    848\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 849\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0m_InactiveRpcError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31m_InactiveRpcError\u001b[0m: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.UNAVAILABLE\n\tdetails = \"No route to host\"\n\tdebug_error_string = \"UNKNOWN:Error received from peer ipv6:%5B2607:f8b0:4004:c1b::5f%5D:443 {created_time:\"2023-04-25T17:36:24.409634-04:00\", grpc_status:14, grpc_message:\"No route to host\"}\"\n>",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mServiceUnavailable\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/google/api_core/retry.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, **kwargs)\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/google/api_core/timeout.py\u001b[0m in \u001b[0;36mfunc_with_timeout\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/google/api_core/grpc_helpers.py\u001b[0m in \u001b[0;36merror_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRpcError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_grpc_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mServiceUnavailable\u001b[0m: 503 No route to host",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRetryError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/google/api_core/future/polling.py\u001b[0m in \u001b[0;36m_blocking_poll\u001b[0;34m(self, timeout, retry, polling)\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m             \u001b[0mpolling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_done_or_raise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRetryError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/google/api_core/retry.py\u001b[0m in \u001b[0;36mretry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    348\u001b[0m             )\n\u001b[0;32m--> 349\u001b[0;31m             return retry_target(\n\u001b[0m\u001b[1;32m    350\u001b[0m                 \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/google/api_core/retry.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, **kwargs)\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/google/api_core/future/polling.py\u001b[0m in \u001b[0;36m_done_or_raise\u001b[0;34m(self, retry)\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;34m\"\"\"Check if the future is done and raise if it's not.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0m_OperationNotComplete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/google/api_core/operation.py\u001b[0m in \u001b[0;36mdone\u001b[0;34m(self, retry)\u001b[0m\n\u001b[1;32m    173\u001b[0m         \"\"\"\n\u001b[0;32m--> 174\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_refresh_and_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_operation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/google/api_core/operation.py\u001b[0m in \u001b[0;36m_refresh_and_update\u001b[0;34m(self, retry)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_operation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_operation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_refresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mretry\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_refresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_result_from_operation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/google/api_core/operations_v1/operations_client.py\u001b[0m in \u001b[0;36mget_operation\u001b[0;34m(self, name, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         return self._get_operation(\n\u001b[0m\u001b[1;32m    150\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/google/api_core/gapic_v1/method.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, timeout, retry, *args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/google/api_core/retry.py\u001b[0m in \u001b[0;36mretry_wrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    348\u001b[0m             )\n\u001b[0;32m--> 349\u001b[0;31m             return retry_target(\n\u001b[0m\u001b[1;32m    350\u001b[0m                 \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/google/api_core/retry.py\u001b[0m in \u001b[0;36mretry_target\u001b[0;34m(target, predicate, sleep_generator, timeout, on_error, **kwargs)\u001b[0m\n\u001b[1;32m    206\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mnext_attempt_time\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m                 raise exceptions.RetryError(\n\u001b[0m\u001b[1;32m    208\u001b[0m                     \"Deadline of {:.1f}s exceeded while calling target function\".format(\n",
      "\u001b[0;31mRetryError\u001b[0m: Deadline of 600.0s exceeded while calling target function, last exception: 503 No route to host",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/sj/j1_95l2508gd97m887kx1vk000ypk3/T/ipykernel_25371/2094045794.py\u001b[0m in \u001b[0;36m<cell line: 36>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m )\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m endpoint = uploaded_model.deploy(\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0mmachine_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'n1-standard-4'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     deployed_model_display_name='deployed-beans-model')\n",
      "\u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/google/cloud/aiplatform/models.py\u001b[0m in \u001b[0;36mdeploy\u001b[0;34m(self, endpoint, deployed_model_display_name, traffic_percentage, traffic_split, machine_type, min_replica_count, max_replica_count, accelerator_type, accelerator_count, service_account, explanation_metadata, explanation_parameters, metadata, encryption_spec_key_name, network, sync, deploy_request_timeout, autoscaling_target_cpu_utilization, autoscaling_target_accelerator_duty_cycle)\u001b[0m\n\u001b[1;32m   3293\u001b[0m                 )\n\u001b[1;32m   3294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3295\u001b[0;31m         return self._deploy(\n\u001b[0m\u001b[1;32m   3296\u001b[0m             \u001b[0mendpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mendpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3297\u001b[0m             \u001b[0mdeployed_model_display_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeployed_model_display_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/google/cloud/aiplatform/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    805\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m                     \u001b[0mVertexAiResourceNounWithFutureManager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0;31m# callbacks to call within the Future (in same Thread)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/google/cloud/aiplatform/models.py\u001b[0m in \u001b[0;36m_deploy\u001b[0;34m(self, endpoint, deployed_model_display_name, traffic_percentage, traffic_split, machine_type, min_replica_count, max_replica_count, accelerator_type, accelerator_count, service_account, explanation_metadata, explanation_parameters, metadata, encryption_spec_key_name, network, sync, deploy_request_timeout, autoscaling_target_cpu_utilization, autoscaling_target_accelerator_duty_cycle)\u001b[0m\n\u001b[1;32m   3468\u001b[0m         \u001b[0m_LOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_action_start_against_resource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Deploying model to\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mendpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3469\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3470\u001b[0;31m         endpoint._deploy_call(\n\u001b[0m\u001b[1;32m   3471\u001b[0m             \u001b[0mendpoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_client\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3472\u001b[0m             \u001b[0mendpoint\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresource_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/google/cloud/aiplatform/models.py\u001b[0m in \u001b[0;36m_deploy_call\u001b[0;34m(cls, api_client, endpoint_resource_name, model, endpoint_resource_traffic_split, network, deployed_model_display_name, traffic_percentage, traffic_split, machine_type, min_replica_count, max_replica_count, accelerator_type, accelerator_count, service_account, explanation_metadata, explanation_parameters, metadata, deploy_request_timeout, autoscaling_target_cpu_utilization, autoscaling_target_accelerator_duty_cycle)\u001b[0m\n\u001b[1;32m   1239\u001b[0m         )\n\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1241\u001b[0;31m         \u001b[0moperation_future\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m     def undeploy(\n",
      "\u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/google/api_core/future/polling.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout, retry, polling)\u001b[0m\n\u001b[1;32m    254\u001b[0m         \"\"\"\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_blocking_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolling\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpolling\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/google/api_core/future/polling.py\u001b[0m in \u001b[0;36m_blocking_poll\u001b[0;34m(self, timeout, retry, polling)\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mpolling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_done_or_raise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRetryError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             raise concurrent.futures.TimeoutError(\n\u001b[0m\u001b[1;32m    140\u001b[0m                 \u001b[0;34mf\"Operation did not complete within the designated timeout of \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m                 \u001b[0;34mf\"{polling.timeout} seconds.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTimeoutError\u001b[0m: Operation did not complete within the designated timeout of 900 seconds."
     ]
    }
   ],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "# Check if model exists\n",
    "models = aiplatform.Model.list()\n",
    "model_name = 'beans-model'\n",
    "if 'beans-model' in (m.name for m in models):\n",
    "    parent_model = model_name\n",
    "    model_id = None\n",
    "    is_default_version=False\n",
    "    version_aliases=['experimental', 'challenger', 'custom-training', 'decision-tree']\n",
    "    version_description='challenger version'\n",
    "else:\n",
    "    parent_model = None\n",
    "    model_id = model_name\n",
    "    is_default_version=True\n",
    "    version_aliases=['champion', 'custom-training', 'decision-tree']\n",
    "    version_description='first version'\n",
    "\n",
    "serving_container = 'us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.0-24:latest'\n",
    "aiplatform.init(project=PROJECT_ID)\n",
    "uploaded_model = aiplatform.Model.upload(\n",
    "    artifact_uri = 'gs://automlops-sandbox-bucket/trained_models/2023-04-25 15:34:29.443225',\n",
    "    model_id=model_id,\n",
    "    display_name=model_name,\n",
    "    parent_model=parent_model,\n",
    "    is_default_version=is_default_version,\n",
    "    version_aliases=version_aliases,\n",
    "    version_description=version_description,\n",
    "    serving_container_image_uri=serving_container,\n",
    "    serving_container_health_route='/health',\n",
    "    serving_container_predict_route='/predict',\n",
    "    serving_container_ports=[9999],\n",
    "    labels={'created_by': 'automlops-team'},\n",
    ")\n",
    "\n",
    "endpoint = uploaded_model.deploy(\n",
    "    machine_type='n1-standard-4',\n",
    "    deployed_model_display_name='deployed-beans-model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ef279e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Successfully saved requirements file in AutoMLOps/components/component_base/requirements.txt\n",
      "\u001b[0;32m Updating required API services in project automlops-sandbox \u001b[0m\n",
      "Operation \"operations/acat.p2-45373616427-2b05b58a-c714-4e38-abbc-9882737f1247\" finished successfully.\n",
      "\u001b[0;32m Checking for Artifact Registry: vertex-mlops-af in project automlops-sandbox \u001b[0m\n",
      "Listing items under project automlops-sandbox, location us-central1.\n",
      "\n",
      "vertex-mlops-af  DOCKER  STANDARD_REPOSITORY  Artifact Registry vertex-mlops-af in us-central1.  us-central1          Google-managed key  2023-01-11T17:12:26  2023-04-24T11:05:36  25045.307\n",
      "Artifact Registry: vertex-mlops-af already exists in project automlops-sandbox\n",
      "\u001b[0;32m Checking for GS Bucket: automlops-sandbox-bucket in project automlops-sandbox \u001b[0m\n",
      "gs://automlops-sandbox-bucket/\n",
      "GS Bucket: automlops-sandbox-bucket already exists in project automlops-sandbox\n",
      "\u001b[0;32m Checking for Service Account: vertex-pipelines in project automlops-sandbox \u001b[0m\n",
      "Pipeline Runner Service Account         vertex-pipelines@automlops-sandbox.iam.gserviceaccount.com  False\n",
      "Service Account: vertex-pipelines already exists in project automlops-sandbox\n",
      "\u001b[0;32m Updating required IAM roles in project automlops-sandbox \u001b[0m\n",
      "\u001b[0;32m Checking for Cloud Source Repository: AutoMLOps-repo in project automlops-sandbox \u001b[0m\n",
      "AutoMLOps-repo  automlops-sandbox  https://source.developers.google.com/p/automlops-sandbox/r/AutoMLOps-repo\n",
      "Cloud Source Repository: AutoMLOps-repo already exists in project automlops-sandbox\n",
      "\u001b[0;32m Checking for Cloud Tasks Queue: queueing-svc in project automlops-sandbox \u001b[0m\n",
      "queueing-svc       RUNNING  1000              500.0            100\n",
      "Cloud Tasks Queue: queueing-svc already exists in project automlops-sandbox\n",
      "\u001b[0;32m Checking for Cloudbuild Trigger: automlops-trigger in project automlops-sandbox \u001b[0m\n",
      "name: automlops-trigger\n",
      "Cloudbuild Trigger already exists in project automlops-sandbox for repo AutoMLOps-repo\n",
      "[automlops f805e6f] Run AutoMLOps\n",
      " 16 files changed, 333 insertions(+), 174 deletions(-)\n",
      "To https://source.developers.google.com/p/automlops-sandbox/r/AutoMLOps-repo\n",
      "   86fb0cd..f805e6f  automlops -> automlops\n",
      "Pushing code to automlops branch, triggering cloudbuild...\n",
      "Cloudbuild job running at: https://console.cloud.google.com/cloud-build/builds;region=us-central1\n",
      "\n",
      "#################################################################\n",
      "#                                                               #\n",
      "#                       RESOURCES MANIFEST                      #\n",
      "#---------------------------------------------------------------#\n",
      "#     Generated resources can be found at the following urls    #\n",
      "#                                                               #\n",
      "#################################################################\n",
      "\n",
      "Google Cloud Storage Bucket: https://console.cloud.google.com/storage/automlops-sandbox-bucket\n",
      "Artifact Registry: https://console.cloud.google.com/artifacts/docker/automlops-sandbox/us-central1/vertex-mlops-af\n",
      "Service Accounts: https://console.cloud.google.com/iam-admin/serviceaccounts?project=automlops-sandbox\n",
      "APIs: https://console.cloud.google.com/apis\n",
      "Cloud Source Repository: https://source.cloud.google.com/automlops-sandbox/AutoMLOps-repo/+/automlops:\n",
      "Cloud Build Jobs: https://console.cloud.google.com/cloud-build/builds;region=us-central1\n",
      "Vertex AI Pipeline Runs: https://console.cloud.google.com/vertex-ai/pipelines/runs\n",
      "Cloud Build Trigger: https://console.cloud.google.com/cloud-build/triggers;region=us-central1\n",
      "Cloud Run Service: https://console.cloud.google.com/run/detail/us-central1/run-pipeline\n",
      "Cloud Tasks Queue: https://console.cloud.google.com/cloudtasks/queue/us-central1/queueing-svc/tasks\n",
      "Cloud Scheduler Job: https://console.cloud.google.com/cloudscheduler\n"
     ]
    }
   ],
   "source": [
    "AutoMLOps.go(project_id=PROJECT_ID, pipeline_params=pipeline_params, use_kfp_spec=False, run_local=False, schedule_pattern='0 */12 * * *')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf929353",
   "metadata": {},
   "source": [
    "## Set Pipeline Compute Resources\n",
    "Use the `custom_training_job_specs` parameter to specify custom resources for any custom component in the pipeline. The example below uses a GPU for accelerated training.\n",
    "See [Machine types](https://cloud.google.com/vertex-ai/docs/training/configure-compute#machine-types) and [GPUs](https://cloud.google.com/vertex-ai/docs/training/configure-compute#specifying_gpus)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89f42645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Successfully saved requirements file in AutoMLOps/components/component_base/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "AutoMLOps.generate(project_id=PROJECT_ID, \n",
    "                   pipeline_params=pipeline_params, \n",
    "                   use_kfp_spec=False, \n",
    "                   run_local=False, \n",
    "                   schedule_pattern='0 */12 * * *',\n",
    "                   custom_training_job_specs = [{\n",
    "                       'component_spec': 'train_model',\n",
    "                       'display_name': 'train-model-accelerated',\n",
    "                       'machine_type': 'a2-highgpu-1g',\n",
    "                       'accelerator_type': 'NVIDIA_TESLA_A100',\n",
    "                       'accelerator_count': '1'\n",
    "                   }]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccca3907",
   "metadata": {},
   "source": [
    "## Default Run Settings\n",
    "Below are the default parameters for running `AutoMLOps`. Note there are only two required parameters:\n",
    "1. project_id\n",
    "2. pipeline_params\n",
    "\n",
    "The other parameters are optional. You can customize the output of `AutoMLOps` by specify the resources you'd like to use (or specifying the name of resources you'd like `AutoMLOps` to create if they don't currently exist). A description of the parameters is below:\n",
    "- `project_id`: The project ID.\n",
    "- `pipeline_params`: Dictionary containing runtime pipeline parameters.\n",
    "- `af_registry_location`: Region of the Artifact Registry.\n",
    "- `af_registry_name`: Artifact Registry name where components are stored.\n",
    "- `cb_trigger_location`: The location of the cloudbuild trigger.\n",
    "- `cb_trigger_name`: The name of the cloudbuild trigger.\n",
    "- `cloud_run_location`: The location of the cloud runner service.\n",
    "- `cloud_run_name`: The name of the cloud runner service.\n",
    "- `cloud_tasks_queue_location`: The location of the cloud tasks queue.\n",
    "- `cloud_tasks_queue_name`: The name of the cloud tasks queue.\n",
    "- `csr_branch_name`: The name of the csr branch to push to to trigger cb job.\n",
    "- `csr_name`: The name of the cloud source repo to use.\n",
    "- `custom_training_job_specs`: Specifies the specs to run the training job with.\n",
    "- `gs_bucket_location`: Region of the GS bucket.\n",
    "- `gs_bucket_name`: GS bucket name where pipeline run metadata is stored.\n",
    "- `pipeline_runner_sa`: Service Account to runner PipelineJobs.\n",
    "- `run_local`: Flag that determines whether to use Cloud Run CI/CD.\n",
    "- `schedule_location`: The location of the scheduler resource.\n",
    "- `schedule_name`: The name of the scheduler resource.\n",
    "- `schedule_pattern`: Cron formatted value used to create a Scheduled retrain job.\n",
    "- `use_kfp_spec`: Flag that specifies whether you are using Kubeflow spec or not.\n",
    "- `vpc_connector`: The name of the vpc connector to use.\n",
    "\n",
    "The `run_local` parameter specifies whether to use the generated `scripts/run_all.sh` local script to submit the build job and PipelineJob. If this parameter is set to False, `AutoMLOps` will use the cloud [CI/CD workflow](https://github.com/GoogleCloudPlatform/automlops#cloud-continuous-integration-and-continuous-deployment-workflow). The run above uses `run_local=False`, and the run below uses `run_local=True`, notice the differences in output (`run_local=True` means you will not use Cloud Source Repository to trigger build jobs on push). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c92f80a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Successfully saved requirements file in AutoMLOps/components/component_base/requirements.txt\n",
      "\u001b[0;32m Updating required API services in project automlops-sandbox \u001b[0m\n",
      "Operation \"operations/acat.p2-45373616427-53cf77d8-4efc-400c-80f5-ba4b73a09099\" finished successfully.\n",
      "\u001b[0;32m Checking for Artifact Registry: vertex-mlops-af in project automlops-sandbox \u001b[0m\n",
      "Listing items under project automlops-sandbox, location us-central1.\n",
      "\n",
      "vertex-mlops-af  DOCKER  STANDARD_REPOSITORY  Artifact Registry vertex-mlops-af in us-central1.  us-central1          Google-managed key  2023-01-11T22:12:26  2023-02-03T17:06:10  9945.322\n",
      "Artifact Registry: vertex-mlops-af already exists in project automlops-sandbox\n",
      "\u001b[0;32m Checking for GS Bucket: automlops-sandbox-bucket in project automlops-sandbox \u001b[0m\n",
      "gs://automlops-sandbox-bucket/\n",
      "GS Bucket: automlops-sandbox-bucket already exists in project automlops-sandbox\n",
      "\u001b[0;32m Checking for Service Account: vertex-pipelines in project automlops-sandbox \u001b[0m\n",
      "Pipeline Runner Service Account         vertex-pipelines@automlops-sandbox.iam.gserviceaccount.com  False\n",
      "Service Account: vertex-pipelines already exists in project automlops-sandbox\n",
      "\u001b[0;32m Updating required IAM roles in project automlops-sandbox \u001b[0m\n",
      "\u001b[0;32m Checking for Cloud Source Repository: AutoMLOps-repo in project automlops-sandbox \u001b[0m\n",
      "AutoMLOps-repo  automlops-sandbox  https://source.developers.google.com/p/automlops-sandbox/r/AutoMLOps-repo\n",
      "Cloud Source Repository: AutoMLOps-repo already exists in project automlops-sandbox\n",
      "\u001b[0;32m BUILDING COMPONENTS \u001b[0m\n",
      "Creating temporary tarball archive of 36 file(s) totalling 2.7 MiB before compression.\n",
      "Uploading tarball of [..] to [gs://automlops-sandbox_cloudbuild/source/1675444046.369203-8df7800c806b4d2a83b076cdd417cfba.tgz]\n",
      "Created [https://cloudbuild.googleapis.com/v1/projects/automlops-sandbox/locations/global/builds/3c614e0e-c487-40cc-aa73-a9d2a891bb3e].\n",
      "Logs are available at [ https://console.cloud.google.com/cloud-build/builds/3c614e0e-c487-40cc-aa73-a9d2a891bb3e?project=45373616427 ].\n",
      "----------------------------- REMOTE BUILD OUTPUT ------------------------------\n",
      "starting build \"3c614e0e-c487-40cc-aa73-a9d2a891bb3e\"\n",
      "\n",
      "FETCHSOURCE\n",
      "Fetching storage object: gs://automlops-sandbox_cloudbuild/source/1675444046.369203-8df7800c806b4d2a83b076cdd417cfba.tgz#1675444047865526\n",
      "Copying gs://automlops-sandbox_cloudbuild/source/1675444046.369203-8df7800c806b4d2a83b076cdd417cfba.tgz#1675444047865526...\n",
      "/ [1 files][  1.1 MiB/  1.1 MiB]                                                \n",
      "Operation completed over 1 objects/1.1 MiB.\n",
      "BUILD\n",
      "Starting Step #0 - \"build_component_base\"\n",
      "Step #0 - \"build_component_base\": Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Step #0 - \"build_component_base\": Sending build context to Docker daemon  13.82kB\n",
      "Step #0 - \"build_component_base\": Step 1/6 : FROM python:3.9\n",
      "Step #0 - \"build_component_base\": 3.9: Pulling from library/python\n",
      "Step #0 - \"build_component_base\": bbeef03cda1f: Already exists\n",
      "Step #0 - \"build_component_base\": f049f75f014e: Already exists\n",
      "Step #0 - \"build_component_base\": 56261d0e6b05: Already exists\n",
      "Step #0 - \"build_component_base\": 9bd150679dbd: Already exists\n",
      "Step #0 - \"build_component_base\": 5b282ee9da04: Already exists\n",
      "Step #0 - \"build_component_base\": 03f027d5e312: Already exists\n",
      "Step #0 - \"build_component_base\": 79903339cfdb: Pulling fs layer\n",
      "Step #0 - \"build_component_base\": efbad12427dd: Pulling fs layer\n",
      "Step #0 - \"build_component_base\": 862894708010: Pulling fs layer\n",
      "Step #0 - \"build_component_base\": efbad12427dd: Verifying Checksum\n",
      "Step #0 - \"build_component_base\": efbad12427dd: Download complete\n",
      "Step #0 - \"build_component_base\": 862894708010: Verifying Checksum\n",
      "Step #0 - \"build_component_base\": 862894708010: Download complete\n",
      "Step #0 - \"build_component_base\": 79903339cfdb: Verifying Checksum\n",
      "Step #0 - \"build_component_base\": 79903339cfdb: Download complete\n",
      "Step #0 - \"build_component_base\": 79903339cfdb: Pull complete\n",
      "Step #0 - \"build_component_base\": efbad12427dd: Pull complete\n",
      "Step #0 - \"build_component_base\": 862894708010: Pull complete\n",
      "Step #0 - \"build_component_base\": Digest: sha256:7af616b934168e213d469bff23bd8e4f07d09ccbe87e82c464cacd8e2fb244bf\n",
      "Step #0 - \"build_component_base\": Status: Downloaded newer image for python:3.9\n",
      "Step #0 - \"build_component_base\":  ---> 0a3ee4bd701a\n",
      "Step #0 - \"build_component_base\": Step 2/6 : RUN python -m pip install --upgrade pip\n",
      "Step #0 - \"build_component_base\":  ---> Running in ba6142b36fd6\n",
      "Step #0 - \"build_component_base\": Requirement already satisfied: pip in /usr/local/lib/python3.9/site-packages (22.0.4)\n",
      "Step #0 - \"build_component_base\": Collecting pip\n",
      "Step #0 - \"build_component_base\":   Downloading pip-23.0-py3-none-any.whl (2.1 MB)\n",
      "Step #0 - \"build_component_base\":       2.1/2.1 MB 51.6 MB/s eta 0:00:00\n",
      "Step #0 - \"build_component_base\": Installing collected packages: pip\n",
      "Step #0 - \"build_component_base\":   Attempting uninstall: pip\n",
      "Step #0 - \"build_component_base\":     Found existing installation: pip 22.0.4\n",
      "Step #0 - \"build_component_base\":     Uninstalling pip-22.0.4:\n",
      "Step #0 - \"build_component_base\":       Successfully uninstalled pip-22.0.4\n",
      "Step #0 - \"build_component_base\": Successfully installed pip-23.0\n",
      "Step #0 - \"build_component_base\": \u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "Step #0 - \"build_component_base\": \u001b[0mRemoving intermediate container ba6142b36fd6\n",
      "Step #0 - \"build_component_base\":  ---> 8e3ee47295c0\n",
      "Step #0 - \"build_component_base\": Step 3/6 : COPY requirements.txt .\n",
      "Step #0 - \"build_component_base\":  ---> 0e5c9b491686\n",
      "Step #0 - \"build_component_base\": Step 4/6 : RUN python -m pip install -r     requirements.txt --quiet --no-cache-dir     && rm -f requirements.txt\n",
      "Step #0 - \"build_component_base\":  ---> Running in 1acc35d94e9c\n",
      "Step #0 - \"build_component_base\": \u001b[91mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n",
      "Step #0 - \"build_component_base\": \u001b[0mRemoving intermediate container 1acc35d94e9c\n",
      "Step #0 - \"build_component_base\":  ---> b6a9eccf6613\n",
      "Step #0 - \"build_component_base\": Step 5/6 : COPY ./src /pipelines/component/src\n",
      "Step #0 - \"build_component_base\":  ---> 854ac30dea40\n",
      "Step #0 - \"build_component_base\": Step 6/6 : ENTRYPOINT [\"/bin/bash\"]\n",
      "Step #0 - \"build_component_base\":  ---> Running in 1f5afde1b2c8\n",
      "Step #0 - \"build_component_base\": Removing intermediate container 1f5afde1b2c8\n",
      "Step #0 - \"build_component_base\":  ---> e991b7536d83\n",
      "Step #0 - \"build_component_base\": Successfully built e991b7536d83\n",
      "Step #0 - \"build_component_base\": Successfully tagged us-central1-docker.pkg.dev/automlops-sandbox/vertex-mlops-af/components/component_base:latest\n",
      "Finished Step #0 - \"build_component_base\"\n",
      "Starting Step #1 - \"push_component_base\"\n",
      "Step #1 - \"push_component_base\": Already have image (with digest): gcr.io/cloud-builders/docker\n",
      "Step #1 - \"push_component_base\": The push refers to repository [us-central1-docker.pkg.dev/automlops-sandbox/vertex-mlops-af/components/component_base]\n",
      "Step #1 - \"push_component_base\": b62502977a2b: Preparing\n",
      "Step #1 - \"push_component_base\": a9b5c2847cc8: Preparing\n",
      "Step #1 - \"push_component_base\": 02aca1a33542: Preparing\n",
      "Step #1 - \"push_component_base\": 888df4909ca1: Preparing\n",
      "Step #1 - \"push_component_base\": e42ec6c46a91: Preparing\n",
      "Step #1 - \"push_component_base\": d5b17d98dbd1: Preparing\n",
      "Step #1 - \"push_component_base\": 52400b17e008: Preparing\n",
      "Step #1 - \"push_component_base\": dc6462f7bb8b: Preparing\n",
      "Step #1 - \"push_component_base\": a4db1a405763: Preparing\n",
      "Step #1 - \"push_component_base\": 9f4f964da727: Preparing\n",
      "Step #1 - \"push_component_base\": 49b333f7bad4: Preparing\n",
      "Step #1 - \"push_component_base\": a463dbda4664: Preparing\n",
      "Step #1 - \"push_component_base\": a9099c3159f5: Preparing\n",
      "Step #1 - \"push_component_base\": dc6462f7bb8b: Waiting\n",
      "Step #1 - \"push_component_base\": a4db1a405763: Waiting\n",
      "Step #1 - \"push_component_base\": 9f4f964da727: Waiting\n",
      "Step #1 - \"push_component_base\": 49b333f7bad4: Waiting\n",
      "Step #1 - \"push_component_base\": a463dbda4664: Waiting\n",
      "Step #1 - \"push_component_base\": a9099c3159f5: Waiting\n",
      "Step #1 - \"push_component_base\": d5b17d98dbd1: Waiting\n",
      "Step #1 - \"push_component_base\": 52400b17e008: Waiting\n",
      "Step #1 - \"push_component_base\": e42ec6c46a91: Layer already exists\n",
      "Step #1 - \"push_component_base\": d5b17d98dbd1: Layer already exists\n",
      "Step #1 - \"push_component_base\": 52400b17e008: Layer already exists\n",
      "Step #1 - \"push_component_base\": dc6462f7bb8b: Layer already exists\n",
      "Step #1 - \"push_component_base\": b62502977a2b: Pushed\n",
      "Step #1 - \"push_component_base\": 02aca1a33542: Pushed\n",
      "Step #1 - \"push_component_base\": 9f4f964da727: Layer already exists\n",
      "Step #1 - \"push_component_base\": a4db1a405763: Layer already exists\n",
      "Step #1 - \"push_component_base\": 49b333f7bad4: Layer already exists\n",
      "Step #1 - \"push_component_base\": a9099c3159f5: Layer already exists\n",
      "Step #1 - \"push_component_base\": a463dbda4664: Layer already exists\n",
      "Step #1 - \"push_component_base\": 888df4909ca1: Pushed\n",
      "Step #1 - \"push_component_base\": a9b5c2847cc8: Pushed\n",
      "Step #1 - \"push_component_base\": latest: digest: sha256:6c82d04363ca3e55e73fd92134c1979207cd206ad3364c8d9949172f827d7862 size: 3057\n",
      "Finished Step #1 - \"push_component_base\"\n",
      "PUSH\n",
      "Pushing us-central1-docker.pkg.dev/automlops-sandbox/vertex-mlops-af/components/component_base:latest\n",
      "The push refers to repository [us-central1-docker.pkg.dev/automlops-sandbox/vertex-mlops-af/components/component_base]\n",
      "b62502977a2b: Preparing\n",
      "a9b5c2847cc8: Preparing\n",
      "02aca1a33542: Preparing\n",
      "888df4909ca1: Preparing\n",
      "e42ec6c46a91: Preparing\n",
      "d5b17d98dbd1: Preparing\n",
      "52400b17e008: Preparing\n",
      "dc6462f7bb8b: Preparing\n",
      "a4db1a405763: Preparing\n",
      "9f4f964da727: Preparing\n",
      "49b333f7bad4: Preparing\n",
      "a463dbda4664: Preparing\n",
      "a9099c3159f5: Preparing\n",
      "d5b17d98dbd1: Waiting\n",
      "52400b17e008: Waiting\n",
      "dc6462f7bb8b: Waiting\n",
      "a4db1a405763: Waiting\n",
      "9f4f964da727: Waiting\n",
      "a9099c3159f5: Waiting\n",
      "49b333f7bad4: Waiting\n",
      "a463dbda4664: Waiting\n",
      "e42ec6c46a91: Layer already exists\n",
      "b62502977a2b: Layer already exists\n",
      "a9b5c2847cc8: Layer already exists\n",
      "02aca1a33542: Layer already exists\n",
      "888df4909ca1: Layer already exists\n",
      "d5b17d98dbd1: Layer already exists\n",
      "52400b17e008: Layer already exists\n",
      "a4db1a405763: Layer already exists\n",
      "9f4f964da727: Layer already exists\n",
      "dc6462f7bb8b: Layer already exists\n",
      "49b333f7bad4: Layer already exists\n",
      "a9099c3159f5: Layer already exists\n",
      "a463dbda4664: Layer already exists\n",
      "latest: digest: sha256:6c82d04363ca3e55e73fd92134c1979207cd206ad3364c8d9949172f827d7862 size: 3057\n",
      "DONE\n",
      "--------------------------------------------------------------------------------\n",
      "ID                                    CREATE_TIME                DURATION  SOURCE                                                                                           IMAGES                                                                                            STATUS\n",
      "3c614e0e-c487-40cc-aa73-a9d2a891bb3e  2023-02-03T17:07:28+00:00  2M4S      gs://automlops-sandbox_cloudbuild/source/1675444046.369203-8df7800c806b4d2a83b076cdd417cfba.tgz  us-central1-docker.pkg.dev/automlops-sandbox/vertex-mlops-af/components/component_base (+1 more)  SUCCESS\n",
      "\u001b[0;32m BUILDING PIPELINE SPEC \u001b[0m\n",
      "/home/jupyter/.local/lib/python3.7/site-packages/kfp/v2/compiler/compiler.py:1293: FutureWarning: APIs imported from the v1 namespace (e.g. kfp.dsl, kfp.components, etc) will not be supported by the v2 compiler since v2.0.0\n",
      "  category=FutureWarning,\n",
      "\u001b[0;32m RUNNING PIPELINE JOB \u001b[0m\n",
      "Creating PipelineJob\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:Creating PipelineJob\n",
      "PipelineJob created. Resource name: projects/45373616427/locations/us-central1/pipelineJobs/training-pipeline-20230203170941\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:PipelineJob created. Resource name: projects/45373616427/locations/us-central1/pipelineJobs/training-pipeline-20230203170941\n",
      "To use this PipelineJob in another session:\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:To use this PipelineJob in another session:\n",
      "pipeline_job = aiplatform.PipelineJob.get('projects/45373616427/locations/us-central1/pipelineJobs/training-pipeline-20230203170941')\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:pipeline_job = aiplatform.PipelineJob.get('projects/45373616427/locations/us-central1/pipelineJobs/training-pipeline-20230203170941')\n",
      "View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/training-pipeline-20230203170941?project=45373616427\n",
      "INFO:google.cloud.aiplatform.pipeline_jobs:View Pipeline Job:\n",
      "https://console.cloud.google.com/vertex-ai/locations/us-central1/pipelines/runs/training-pipeline-20230203170941?project=45373616427\n",
      "\n",
      "#################################################################\n",
      "#                                                               #\n",
      "#                       RESOURCES MANIFEST                      #\n",
      "#---------------------------------------------------------------#\n",
      "#     Generated resources can be found at the following urls    #\n",
      "#                                                               #\n",
      "#################################################################\n",
      "\n",
      "Google Cloud Storage Bucket: https://console.cloud.google.com/storage/automlops-sandbox-bucket\n",
      "Artifact Registry: https://console.cloud.google.com/artifacts/docker/automlops-sandbox/us-central1/vertex-mlops-af\n",
      "Service Accounts: https://console.cloud.google.com/iam-admin/serviceaccounts?project=automlops-sandbox\n",
      "APIs: https://console.cloud.google.com/apis\n",
      "Cloud Source Repository: https://source.cloud.google.com/automlops-sandbox/AutoMLOps-repo/+/automlops:\n",
      "Cloud Build Jobs: https://console.cloud.google.com/cloud-build/builds;region=us-central1\n",
      "Vertex AI Pipeline Runs: https://console.cloud.google.com/vertex-ai/pipelines/runs\n"
     ]
    }
   ],
   "source": [
    "AutoMLOps.go(project_id=PROJECT_ID, # required\n",
    "             pipeline_params=pipeline_params, # required\n",
    "             af_registry_location='us-central1', # default\n",
    "             af_registry_name='vertex-mlops-af', # default\n",
    "             cb_trigger_location='us-central1', # default\n",
    "             cb_trigger_name='automlops-trigger', # default\n",
    "             cloud_run_location='us-central1', # default\n",
    "             cloud_run_name='run-pipeline', # default\n",
    "             cloud_tasks_queue_location='us-central1', # default\n",
    "             cloud_tasks_queue_name='queueing-svc', # default\n",
    "             csr_branch_name='automlops', # default\n",
    "             csr_name='AutoMLOps-repo', # default\n",
    "             custom_training_job_specs=None, # default\n",
    "             gs_bucket_location='us-central1', # default\n",
    "             gs_bucket_name=None, # default\n",
    "             pipeline_runner_sa=None, # default\n",
    "             run_local=True, # default\n",
    "             schedule_location='us-central1', # default\n",
    "             schedule_name='AutoMLOps-schedule', # default\n",
    "             schedule_pattern='No Schedule Specified', # default\n",
    "             use_kfp_spec=False, # default\n",
    "             vpc_connector='No VPC Specified' # default\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715a8a22",
   "metadata": {},
   "source": [
    "# 2. AutoMLOps Pipeline - Using Kubeflow components\n",
    "This workflow will generate a pipeline using Kubeflow spec. `generate()` will create all the necessary files but not run them. `go()` will create all the necessary files, resources, push the code to the source repo to trigger the build, and then submit a Pipeline training job to Vertex AI. Please see the [readme](https://github.com/GoogleCloudPlatform/automlops/blob/main/README.md) for more information.\n",
    "\n",
    "**Note: This workflow requires python packages `kfp` and `google-cloud-aiplatform`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f82908",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install kfp google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0856cbc0",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36329f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from kfp.v2 import compiler, dsl\n",
    "from kfp.v2.dsl import pipeline, component, Artifact, Dataset, Input, Metrics, Model, Output, InputPath, OutputPath\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud import aiplatform_v1\n",
    "import datetime\n",
    "\n",
    "from AutoMLOps import AutoMLOps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb34204",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "Define a Kubeflow custom component for loading and creating a dataset. You must specify the `output_component_file` with the name of your component. For `AutoMLOps` to know where to find the Kubeflow component spec, set this variable to the following string `f\"{AutoMLOps.OUTPUT_DIR}/your_component_name.yaml\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89b82a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=[\n",
    "        \"google-cloud-bigquery\", \n",
    "        \"pandas\",\n",
    "        \"pyarrow\",\n",
    "        \"db_dtypes\"\n",
    "    ],\n",
    "    base_image=\"python:3.9\",\n",
    "    output_component_file=f\"{AutoMLOps.OUTPUT_DIR}/create_dataset.yaml\"\n",
    ")\n",
    "def create_dataset(\n",
    "    bq_table: str,\n",
    "    output_data_path: OutputPath(\"Dataset\"),\n",
    "    project: str\n",
    "):\n",
    "    \"\"\"Custom component that takes in a BQ table and writes it to GCS.\n",
    "\n",
    "    Args:\n",
    "        bq_table: The source biquery table.\n",
    "        output_data_path: The gcs location to write the csv.\n",
    "        project: The project ID.\n",
    "    \"\"\"\n",
    "    from google.cloud import bigquery\n",
    "    import pandas as pd\n",
    "    bq_client = bigquery.Client(project=project)\n",
    "\n",
    "    def get_query(bq_input_table: str) -> str:\n",
    "        \"\"\"Generates BQ Query to read data.\n",
    "\n",
    "        Args:\n",
    "        bq_input_table: The full name of the bq input table to be read into\n",
    "        the dataframe (e.g. <project>.<dataset>.<table>)\n",
    "        Returns: A BQ query string.\n",
    "        \"\"\"\n",
    "        return f\"\"\"\n",
    "        SELECT *\n",
    "        FROM `{bq_input_table}`\n",
    "        \"\"\"\n",
    "\n",
    "    def load_bq_data(query: str, client: bigquery.Client) -> pd.DataFrame:\n",
    "        \"\"\"Loads data from bq into a Pandas Dataframe for EDA.\n",
    "        Args:\n",
    "        query: BQ Query to generate data.\n",
    "        client: BQ Client used to execute query.\n",
    "        Returns:\n",
    "        pd.DataFrame: A dataframe with the requested data.\n",
    "        \"\"\"\n",
    "        df = client.query(query).to_dataframe()\n",
    "        return df\n",
    "\n",
    "    dataframe = load_bq_data(get_query(bq_table), bq_client)\n",
    "    dataframe.to_csv(output_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03abc18d",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "Define a Kubeflow custom component for training a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f1a3660",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=[\n",
    "        \"scikit-learn\",\n",
    "        \"pandas\",\n",
    "        \"joblib\",\n",
    "        \"tensorflow\"\n",
    "    ],\n",
    "    base_image=\"python:3.9\",\n",
    "    output_component_file=f\"{AutoMLOps.OUTPUT_DIR}/train_model.yaml\",\n",
    ")\n",
    "def train_model(\n",
    "    output_model_directory: str,\n",
    "    dataset: Input[Dataset],\n",
    "    metrics: Output[Metrics],\n",
    "    model: Output[Model]\n",
    "):\n",
    "    from sklearn.tree import DecisionTreeClassifier\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    import pandas as pd\n",
    "    import tensorflow as tf\n",
    "    import pickle\n",
    "    import os\n",
    "    \n",
    "    def save_model(model, uri):\n",
    "        \"\"\"Saves a model to uri.\"\"\"\n",
    "        with tf.io.gfile.GFile(uri, 'w') as f:\n",
    "            pickle.dump(model, f)\n",
    "    \n",
    "    df = pd.read_csv(dataset.path)\n",
    "    labels = df.pop(\"Class\").tolist()\n",
    "    data = df.values.tolist()\n",
    "    x_train, x_test, y_train, y_test = train_test_split(data, labels)\n",
    "    skmodel = DecisionTreeClassifier()\n",
    "    skmodel.fit(x_train,y_train)\n",
    "    score = skmodel.score(x_test,y_test)\n",
    "    print('accuracy is:',score)\n",
    "    metrics.log_metric(\"accuracy\",(score * 100.0))\n",
    "    metrics.log_metric(\"framework\", \"Scikit Learn\")\n",
    "    metrics.log_metric(\"dataset_size\", len(df))\n",
    "\n",
    "    output_uri = os.path.join(output_model_directory, f'model.pkl')\n",
    "    save_model(skmodel, output_uri)\n",
    "    model.path = output_model_directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae46f9f",
   "metadata": {},
   "source": [
    "## Uploading & Deploying the Model\n",
    "Define a Kubeflow custom component for uploading and deploying a model in Vertex AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa8047d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=[\n",
    "        \"google-cloud-aiplatform\"\n",
    "    ],\n",
    "    base_image=\"python:3.9\",\n",
    "    output_component_file=f\"{AutoMLOps.OUTPUT_DIR}/deploy_model.yaml\",\n",
    ")\n",
    "def deploy_model(\n",
    "    model: Input[Model],\n",
    "    project: str,\n",
    "    region: str,\n",
    "    vertex_endpoint: Output[Artifact],\n",
    "    vertex_model: Output[Model]\n",
    "):\n",
    "    from google.cloud import aiplatform\n",
    "    aiplatform.init(project=project, location=region)\n",
    "    deployed_model = aiplatform.Model.upload(\n",
    "        display_name=\"beans-model-pipeline\",\n",
    "        artifact_uri = model.uri,\n",
    "        serving_container_image_uri=\"us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.0-24:latest\"\n",
    "    )\n",
    "    endpoint = deployed_model.deploy(machine_type=\"n1-standard-4\")\n",
    "    vertex_endpoint.uri = endpoint.resource_name\n",
    "    vertex_model.uri = deployed_model.resource_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3602954f",
   "metadata": {},
   "source": [
    "## Define the Pipeline\n",
    "When using Kubeflow spec, place the `%%define_kfp_pipeline` cell magic decorator at the top of your pipeline cell. This cell magic decorator comes built-in with AutoMLOps. Do not change the name of the function `def pipeline` - AutoMLOps expects the function name to be `pipeline`. Everything else is configurable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01996d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@AutoMLOps.pipeline\n",
    "def pipeline(bq_table: str,\n",
    "             output_model_directory: str,\n",
    "             project: str,\n",
    "             region: str,\n",
    "            ):\n",
    "\n",
    "    dataset_task = create_dataset(\n",
    "        bq_table=bq_table, \n",
    "        project=project)\n",
    "\n",
    "    model_task = train_model(\n",
    "        output_model_directory=output_model_directory,\n",
    "        dataset=dataset_task.output)\n",
    "\n",
    "    deploy_task = deploy_model(\n",
    "        model=model_task.outputs[\"model\"],\n",
    "        project=project,\n",
    "        region=region)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe0b0f2",
   "metadata": {},
   "source": [
    "## Define the Pipeline Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc244ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_params = {\n",
    "    \"bq_table\": f\"{PROJECT_ID}.test_dataset.dry-beans\",\n",
    "    \"output_model_directory\": f\"gs://{PROJECT_ID}-bucket/trained_models/{datetime.datetime.now()}\",\n",
    "    \"project\": f\"{PROJECT_ID}\",\n",
    "    \"region\": \"us-central1\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0c815c",
   "metadata": {},
   "source": [
    "## Generate and Run the pipeline\n",
    "`AutoMLOps.generate` generates the code for the MLOps pipeline. `AutoMLOps.go` generates the code and runs the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36433503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Deploy model', 'inputs': [{'name': 'model', 'type': 'Model'}, {'name': 'project', 'type': 'String'}, {'name': 'region', 'type': 'String'}], 'outputs': [{'name': 'vertex_endpoint', 'type': 'Artifact'}, {'name': 'vertex_model', 'type': 'Model'}], 'implementation': {'container': {'image': 'python:3.9', 'command': ['sh', '-c', '\\nif ! [ -x \"$(command -v pip)\" ]; then\\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\\nfi\\n\\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location \\'google-cloud-aiplatform\\' \\'kfp==1.8.18\\' && \"$0\" \"$@\"\\n', 'sh', '-ec', 'program_path=$(mktemp -d)\\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\\n', '\\nimport kfp\\nfrom kfp.v2 import dsl\\nfrom kfp.v2.dsl import *\\nfrom typing import *\\n\\ndef deploy_model(\\n    model: Input[Model],\\n    project: str,\\n    region: str,\\n    vertex_endpoint: Output[Artifact],\\n    vertex_model: Output[Model]\\n):\\n    from google.cloud import aiplatform\\n    aiplatform.init(project=project, location=region)\\n    deployed_model = aiplatform.Model.upload(\\n        display_name=\"beans-model-pipeline\",\\n        artifact_uri = model.uri,\\n        serving_container_image_uri=\"us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.0-24:latest\"\\n    )\\n    endpoint = deployed_model.deploy(machine_type=\"n1-standard-4\")\\n    vertex_endpoint.uri = endpoint.resource_name\\n    vertex_model.uri = deployed_model.resource_name\\n\\n'], 'args': ['--executor_input', {'executorInput': None}, '--function_to_execute', 'deploy_model']}}}\n",
      "{'name': 'Train model', 'inputs': [{'name': 'output_model_directory', 'type': 'String'}, {'name': 'dataset', 'type': 'Dataset'}], 'outputs': [{'name': 'metrics', 'type': 'Metrics'}, {'name': 'model', 'type': 'Model'}], 'implementation': {'container': {'image': 'python:3.9', 'command': ['sh', '-c', '\\nif ! [ -x \"$(command -v pip)\" ]; then\\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\\nfi\\n\\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location \\'scikit-learn\\' \\'pandas\\' \\'joblib\\' \\'tensorflow\\' \\'kfp==1.8.18\\' && \"$0\" \"$@\"\\n', 'sh', '-ec', 'program_path=$(mktemp -d)\\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\\n', '\\nimport kfp\\nfrom kfp.v2 import dsl\\nfrom kfp.v2.dsl import *\\nfrom typing import *\\n\\ndef train_model(\\n    output_model_directory: str,\\n    dataset: Input[Dataset],\\n    metrics: Output[Metrics],\\n    model: Output[Model]\\n):\\n    from sklearn.tree import DecisionTreeClassifier\\n    from sklearn.model_selection import train_test_split\\n    import pandas as pd\\n    import tensorflow as tf\\n    import pickle\\n    import os\\n\\n    def save_model(model, uri):\\n        \"\"\"Saves a model to uri.\"\"\"\\n        with tf.io.gfile.GFile(uri, \\'w\\') as f:\\n            pickle.dump(model, f)\\n\\n    df = pd.read_csv(dataset.path)\\n    labels = df.pop(\"Class\").tolist()\\n    data = df.values.tolist()\\n    x_train, x_test, y_train, y_test = train_test_split(data, labels)\\n    skmodel = DecisionTreeClassifier()\\n    skmodel.fit(x_train,y_train)\\n    score = skmodel.score(x_test,y_test)\\n    print(\\'accuracy is:\\',score)\\n    metrics.log_metric(\"accuracy\",(score * 100.0))\\n    metrics.log_metric(\"framework\", \"Scikit Learn\")\\n    metrics.log_metric(\"dataset_size\", len(df))\\n\\n    output_uri = os.path.join(output_model_directory, f\\'model.pkl\\')\\n    save_model(skmodel, output_uri)\\n    model.path = output_model_directory\\n\\n'], 'args': ['--executor_input', {'executorInput': None}, '--function_to_execute', 'train_model']}}}\n",
      "{'name': 'Create dataset', 'description': 'Custom component that takes in a BQ table and writes it to GCS.', 'inputs': [{'name': 'bq_table', 'type': 'String', 'description': 'The source biquery table.'}, {'name': 'project', 'type': 'String', 'description': 'The project ID.'}], 'outputs': [{'name': 'output_data_path', 'type': 'Dataset', 'description': 'The gcs location to write the csv.'}], 'implementation': {'container': {'image': 'python:3.9', 'command': ['sh', '-c', '\\nif ! [ -x \"$(command -v pip)\" ]; then\\n    python3 -m ensurepip || python3 -m ensurepip --user || apt-get install python3-pip\\nfi\\n\\nPIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet     --no-warn-script-location \\'google-cloud-bigquery\\' \\'pandas\\' \\'pyarrow\\' \\'db_dtypes\\' \\'kfp==1.8.18\\' && \"$0\" \"$@\"\\n', 'sh', '-ec', 'program_path=$(mktemp -d)\\nprintf \"%s\" \"$0\" > \"$program_path/ephemeral_component.py\"\\npython3 -m kfp.v2.components.executor_main                         --component_module_path                         \"$program_path/ephemeral_component.py\"                         \"$@\"\\n', '\\nimport kfp\\nfrom kfp.v2 import dsl\\nfrom kfp.v2.dsl import *\\nfrom typing import *\\n\\ndef create_dataset(\\n    bq_table: str,\\n    output_data_path: OutputPath(\"Dataset\"),\\n    project: str\\n):\\n    \"\"\"Custom component that takes in a BQ table and writes it to GCS.\\n\\n    Args:\\n        bq_table: The source biquery table.\\n        output_data_path: The gcs location to write the csv.\\n        project: The project ID.\\n    \"\"\"\\n    from google.cloud import bigquery\\n    import pandas as pd\\n    bq_client = bigquery.Client(project=project)\\n\\n    def get_query(bq_input_table: str) -> str:\\n        \"\"\"Generates BQ Query to read data.\\n\\n        Args:\\n        bq_input_table: The full name of the bq input table to be read into\\n        the dataframe (e.g. <project>.<dataset>.<table>)\\n        Returns: A BQ query string.\\n        \"\"\"\\n        return f\"\"\"\\n        SELECT *\\n        FROM `{bq_input_table}`\\n        \"\"\"\\n\\n    def load_bq_data(query: str, client: bigquery.Client) -> pd.DataFrame:\\n        \"\"\"Loads data from bq into a Pandas Dataframe for EDA.\\n        Args:\\n        query: BQ Query to generate data.\\n        client: BQ Client used to execute query.\\n        Returns:\\n        pd.DataFrame: A dataframe with the requested data.\\n        \"\"\"\\n        df = client.query(query).to_dataframe()\\n        return df\\n\\n    dataframe = load_bq_data(get_query(bq_table), bq_client)\\n    dataframe.to_csv(output_data_path)\\n\\n'], 'args': ['--executor_input', {'executorInput': None}, '--function_to_execute', 'create_dataset']}}}\n"
     ]
    }
   ],
   "source": [
    "AutoMLOps.generate(project_id=PROJECT_ID, pipeline_params=pipeline_params, use_kfp_spec=True, run_local=False, schedule_pattern='0 */12 * * *')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "25cfeff0-c5e9-4861-83f0-844dac5e153a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;32m Updating required API services in project automlops-sandbox \u001b[0m\n",
      "Operation \"operations/acat.p2-45373616427-eef6b2d3-5043-4a97-85b2-87c2b4261387\" finished successfully.\n",
      "\u001b[0;32m Checking for Artifact Registry: vertex-mlops-af in project automlops-sandbox \u001b[0m\n",
      "Listing items under project automlops-sandbox, location us-central1.\n",
      "\n",
      "vertex-mlops-af  DOCKER  STANDARD_REPOSITORY  Artifact Registry vertex-mlops-af in us-central1.  us-central1          Google-managed key  2023-01-11T22:12:26  2023-03-06T19:27:12  13170.761\n",
      "Artifact Registry: vertex-mlops-af already exists in project automlops-sandbox\n",
      "\u001b[0;32m Checking for GS Bucket: automlops-sandbox-bucket in project automlops-sandbox \u001b[0m\n",
      "gs://automlops-sandbox-bucket/\n",
      "GS Bucket: automlops-sandbox-bucket already exists in project automlops-sandbox\n",
      "\u001b[0;32m Checking for Service Account: vertex-pipelines in project automlops-sandbox \u001b[0m\n",
      "Pipeline Runner Service Account         vertex-pipelines@automlops-sandbox.iam.gserviceaccount.com  False\n",
      "Service Account: vertex-pipelines already exists in project automlops-sandbox\n",
      "\u001b[0;32m Updating required IAM roles in project automlops-sandbox \u001b[0m\n",
      "\u001b[0;32m Checking for Cloud Source Repository: AutoMLOps-repo in project automlops-sandbox \u001b[0m\n",
      "AutoMLOps-repo  automlops-sandbox  https://source.developers.google.com/p/automlops-sandbox/r/AutoMLOps-repo\n",
      "Cloud Source Repository: AutoMLOps-repo already exists in project automlops-sandbox\n",
      "\u001b[0;32m Checking for Cloud Tasks Queue: queueing-svc in project automlops-sandbox \u001b[0m\n",
      "queueing-svc       RUNNING  1000              500.0            100\n",
      "Cloud Tasks Queue: queueing-svc already exists in project automlops-sandbox\n",
      "\u001b[0;32m Checking for Cloudbuild Trigger: automlops-trigger in project automlops-sandbox \u001b[0m\n",
      "name: automlops-trigger\n",
      "Cloudbuild Trigger already exists in project automlops-sandbox for repo AutoMLOps-repo\n",
      "[automlops 0acbebb] Run AutoMLOps\n",
      " 15 files changed, 443 insertions(+), 196 deletions(-)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Pushing code to automlops branch, triggering cloudbuild...\n",
      "INFO:root:Cloudbuild job running at: https://console.cloud.google.com/cloud-build/builds;region=us-central1\n",
      "INFO:root:\n",
      "#################################################################\n",
      "#                                                               #\n",
      "#                       RESOURCES MANIFEST                      #\n",
      "#---------------------------------------------------------------#\n",
      "#     Generated resources can be found at the following urls    #\n",
      "#                                                               #\n",
      "#################################################################\n",
      "\n",
      "INFO:root:Google Cloud Storage Bucket: https://console.cloud.google.com/storage/automlops-sandbox-bucket\n",
      "INFO:root:Artifact Registry: https://console.cloud.google.com/artifacts/docker/automlops-sandbox/us-central1/vertex-mlops-af\n",
      "INFO:root:Service Accounts: https://console.cloud.google.com/iam-admin/serviceaccounts?project=automlops-sandbox\n",
      "INFO:root:APIs: https://console.cloud.google.com/apis\n",
      "INFO:root:Cloud Source Repository: https://source.cloud.google.com/automlops-sandbox/AutoMLOps-repo/+/automlops:\n",
      "INFO:root:Cloud Build Jobs: https://console.cloud.google.com/cloud-build/builds;region=us-central1\n",
      "INFO:root:Vertex AI Pipeline Runs: https://console.cloud.google.com/vertex-ai/pipelines/runs\n",
      "INFO:root:Cloud Build Trigger: https://console.cloud.google.com/cloud-build/triggers;region=us-central1\n",
      "INFO:root:Cloud Run Service: https://console.cloud.google.com/run/detail/us-central1/run-pipeline\n",
      "INFO:root:Cloud Tasks Queue: https://console.cloud.google.com/cloudtasks/queue/us-central1/queueing-svc/tasks\n",
      "INFO:root:Cloud Scheduler Job: https://console.cloud.google.com/cloudscheduler\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To https://source.developers.google.com/p/automlops-sandbox/r/AutoMLOps-repo\n",
      "   3f65b93..0acbebb  automlops -> automlops\n"
     ]
    }
   ],
   "source": [
    "AutoMLOps.go(project_id=PROJECT_ID, \n",
    "             pipeline_params=pipeline_params, \n",
    "             use_kfp_spec=True, \n",
    "             run_local=False, \n",
    "             schedule_pattern='0 */12 * * *',\n",
    "             custom_training_job_specs = [{\n",
    "                'component_spec': 'train_model',\n",
    "                'display_name': 'train-model-accelerated',\n",
    "                'machine_type': 'a2-highgpu-1g',\n",
    "                'accelerator_type': 'NVIDIA_TESLA_A100',\n",
    "                'accelerator_count': '1'\n",
    "             }]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f570be2e-95ed-476e-a7b3-a9e3ec2a7fc4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m102",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m102"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
