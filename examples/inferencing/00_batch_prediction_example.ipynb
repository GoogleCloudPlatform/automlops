{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b45b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2023 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8790d7ed",
   "metadata": {},
   "source": [
    "# AutoMLOps - Batch Prediction Example\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/automlops/blob/main/examples/inference/00_batch_prediction_example.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/automlops/blob/main/examples/inference/00_batch_prediction_example.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/automlops/main/examples/inference/00_batch_prediction_example.ipynb\">\n",
    "        <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "      Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</table>\n",
    "<br/><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f938540",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "In this tutorial, you will build a [Vertex AI](https://cloud.google.com/vertex-ai) pipeline, complete with an integrated CI/CD pipeline. This tutorial will walk you through how to use AutoMLOps to define, create and run pipelines.\n",
    "\n",
    "This tutorial assumes you have gone through the [introduction training example](../training/00_introduction_training_example.ipynb) and are using the deployed model endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22881a6a",
   "metadata": {},
   "source": [
    "# Objective\n",
    "In this tutorial, you will learn how to create and run MLOps pipelines integrated with CI/CD. This tutorial goes through an example pipeline that is defined two ways: first using a custom python syntax, and second using Kubeflow syntax (either option is valid, whichever is preferred is up to you). The example pipeline runs batch prediction using a deployed classification model (see our training example for training this classification model); the pipeline runs the following component:\n",
    "1. create_dataset: A custom component that will export the dataset from BQ to GCS as a jsonl.\n",
    "1. batch_prediction: A custom component that will run a batch prediction job using a deployed model.\n",
    "\n",
    "# Prerequisites\n",
    "\n",
    "In order to use AutoMLOps, the following are required:\n",
    "\n",
    "- Python 3.0 - 3.10\n",
    "- [Google Cloud SDK 407.0.0](https://cloud.google.com/sdk/gcloud/reference)\n",
    "- [beta 2022.10.21](https://cloud.google.com/sdk/gcloud/reference/beta)\n",
    "- `git` installed\n",
    "- `git` logged-in:\n",
    "```\n",
    "  git config --global user.email \"you@example.com\"\n",
    "  git config --global user.name \"Your Name\"\n",
    "```\n",
    "- [Application Default Credentials (ADC)](https://cloud.google.com/docs/authentication/provide-credentials-adc) are setup. This can be done through the following commands:\n",
    "```\n",
    "gcloud auth application-default login\n",
    "gcloud config set account <account@example.com>\n",
    "```\n",
    "\n",
    "# Dependencies\n",
    "- `docopt==0.6.2`,\n",
    "- `docstring-parser==0.15`,\n",
    "- `pipreqs==0.4.11`,\n",
    "- `PyYAML==6.0.1`,\n",
    "- `yarg==0.1.9`\n",
    "\n",
    "# APIs & IAM\n",
    "AutoMLOps will enable the following APIs:\n",
    "- [cloudresourcemanager.googleapis.com](https://cloud.google.com/resource-manager/reference/rest)\n",
    "- [aiplatform.googleapis.com](https://cloud.google.com/vertex-ai/docs/reference/rest)\n",
    "- [artifactregistry.googleapis.com](https://cloud.google.com/artifact-registry/docs/reference/rest)\n",
    "- [cloudbuild.googleapis.com](https://cloud.google.com/build/docs/api/reference/rest)\n",
    "- [cloudscheduler.googleapis.com](https://cloud.google.com/scheduler/docs/reference/rest)\n",
    "- [cloudtasks.googleapis.com](https://cloud.google.com/tasks/docs/reference/rest)\n",
    "- [compute.googleapis.com](https://cloud.google.com/compute/docs/reference/rest/v1)\n",
    "- [iam.googleapis.com](https://cloud.google.com/iam/docs/reference/rest)\n",
    "- [iamcredentials.googleapis.com](https://cloud.google.com/iam/docs/reference/credentials/rest)\n",
    "- [ml.googleapis.com](https://cloud.google.com/ai-platform/training/docs/reference/rest)\n",
    "- [run.googleapis.com](https://cloud.google.com/run/docs/reference/rest)\n",
    "- [storage.googleapis.com](https://cloud.google.com/storage/docs/apis)\n",
    "- [sourcerepo.googleapis.com](https://cloud.google.com/source-repositories/docs/reference/rest)\n",
    "\n",
    "AutoMLOps will update [IAM privileges](https://cloud.google.com/iam/docs/understanding-roles) for the following accounts:\n",
    "1. Pipeline Runner Service Account (one is created if it does exist, defaults to: vertex-pipelines@PROJECT_ID.iam.gserviceaccount.com). Roles added:\n",
    "- roles/aiplatform.user\n",
    "- roles/artifactregistry.reader\n",
    "- roles/bigquery.user\n",
    "- roles/bigquery.dataEditor\n",
    "- roles/iam.serviceAccountUser\n",
    "- roles/storage.admin\n",
    "- roles/run.admin\n",
    "2. Cloudbuild Default Service Account (PROJECT_NUMBER@cloudbuild.gserviceaccount.com). Roles added:\n",
    "- roles/run.admin\n",
    "- roles/iam.serviceAccountUser\n",
    "- roles/cloudtasks.enqueuer\n",
    "- roles/cloudscheduler.admin\n",
    "\n",
    "# User Guide\n",
    "\n",
    "For a user-guide, please view these [slides](../AutoMLOps_Implementation_Guide_External.pdf).\n",
    "\n",
    "# Costs\n",
    "\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "- Vertex AI\n",
    "- Artifact Registry\n",
    "- Cloud Storage\n",
    "- Cloud Source Repository\n",
    "- Cloud Build\n",
    "- Cloud Run\n",
    "- Cloud Scheduler\n",
    "\n",
    "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing), and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage.\n",
    "\n",
    "# Ground-rules for using AutoMLOps\n",
    "1. Do not use variables, functions, code, etc. not defined within the scope of a custom component. These custom components will become containers and will have no reference to the out of scope code.\n",
    "2. Import statements and helper functions must be added inside the function. Provide parameter type hints.\n",
    "3. Test each of your components for accuracy and correctness before running them using AutoMLOps. We cannot fix bugs automatically; bugs are much more difficult to fix once they are made into pipelines.\n",
    "4. If you are using Kubeflow, be sure to define all the requirements needed to run the custom component - it can be easy to leave out packages which will cause the container to fail when running within a pipeline. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12381413",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "For training data, we are using the [dry beans dataset](https://archive.ics.uci.edu/ml/datasets/dry+bean+dataset) which contains metadata on images of seven different types of dry beans taken with a high-resolution camera. The raw dataset can be found [here](https://github.com/GoogleCloudPlatform/automlops/blob/main/example/data/Dry_Beans_Dataset.csv). We will take a subset of this data to show an inferencing example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a231b629",
   "metadata": {},
   "source": [
    "# Setup Git\n",
    "Set up your git configuration below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f90b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git config --global user.email 'you@example.com'\n",
    "!git config --global user.name 'Your Name'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e4d190",
   "metadata": {},
   "source": [
    "# Install AutoMLOps\n",
    "\n",
    "Install AutoMLOps from [PyPI](https://pypi.org/project/google-cloud-automlops/), or locally by cloning the repo and running `pip install .`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94451868",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install google-cloud-automlops --user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17db55d5",
   "metadata": {},
   "source": [
    "# Restart the kernel\n",
    "Once you've installed the AutoMLOps package, you need to restart the notebook kernel so it can find the package.\n",
    "\n",
    "**Note: Once this cell has finished running, continue on. You do not need to re-run any of the cells above.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c53b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.getenv('IS_TESTING'):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250d511b",
   "metadata": {},
   "source": [
    "# Set your project ID\n",
    "Set your project ID below. If you don't know your project ID, leave the field blank and the following cells may be able to find it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "931ff517",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = '[your-project-id]'  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be0be295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project ID: automlops-sandbox\n"
     ]
    }
   ],
   "source": [
    "if PROJECT_ID == '' or PROJECT_ID is None or PROJECT_ID == '[your-project-id]':\n",
    "    # Get your GCP project id from gcloud\n",
    "    shell_output = !gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print('Project ID:', PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c36482d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\r\n"
     ]
    }
   ],
   "source": [
    "! gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503f4a85-c357-4ec4-8ea5-0eeb2c8afd39",
   "metadata": {},
   "source": [
    "# Upload Data\n",
    "This will create a BQ table and upload the Dry Beans csv. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28557b2d-598c-466b-8116-bc1e34ba090d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset automlops-sandbox.test_dataset already exists\n",
      "Table test_dataset.dry-beans-inferencing already exists\n"
     ]
    }
   ],
   "source": [
    "!python3 -m data.load_data_to_bq --project $PROJECT_ID --file data/Dry_Beans_Dataset_Inferencing.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3fe994",
   "metadata": {},
   "source": [
    "# 1. AutoMLOps Pipeline\n",
    "This workflow will define and generate a pipeline using AutoMLOps. `generate()` will create all the necessary files but not run them. `go()` will create all the necessary files, resources, push the code to the source repo to trigger the build, and then submit a Pipeline training job to Vertex AI. Please see the [readme](https://github.com/GoogleCloudPlatform/automlops/blob/main/README.md) for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2219ee67",
   "metadata": {},
   "source": [
    "## Import AutoMLOps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d02ea3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from AutoMLOps import AutoMLOps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc81099",
   "metadata": {},
   "source": [
    "## Clear the cache\n",
    "`AutoMLOps.clear_cache` will remove previous instantiations of AutoMLOps components and pipelines. Use this function if you have previously defined a component that you no longer need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85d5e260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache cleared.\n"
     ]
    }
   ],
   "source": [
    "AutoMLOps.clear_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4476cb4-91c5-42ff-a500-8cc275fedbd1",
   "metadata": {},
   "source": [
    "## Batch Prediction\n",
    "Define a custom component for batch prediction. Import statements and helper functions must be added inside the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5371be73-db3f-4d79-bde7-94fcd5ea13b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@AutoMLOps.component\n",
    "def batch_predict(\n",
    "    project_id: str,\n",
    "    bigquery_destination: str,\n",
    "    bq_dataset_path: str,\n",
    "    instances_format: str,\n",
    "    predictions_format: str,\n",
    "    model_resource_name: str,\n",
    "    endpoint_resource_name: str,\n",
    "    machine_type: str,\n",
    "    accelerator_count: int,\n",
    "    accelerator_type: str,\n",
    "    max_replica_count: int,\n",
    "    starting_replica_count: int\n",
    "):\n",
    "    \"\"\"Runs a batch prediction job.\n",
    "\n",
    "    Args:\n",
    "        bigquery_destination: The BQ uri to store the prediction results.\n",
    "        bq_dataset_path: The BQ uri of the input data to run predictions on.\n",
    "        instances_format: The format in which instances are given, must be one of 'jsonl', 'csv', 'bigquery', 'tf-record', 'tf-record-gzip', or 'file-list'.\n",
    "        predictions_format: The format to output the predictions, must be one of 'jsonl', 'csv', or 'bigquery'.\n",
    "        model_resource_name: The fully-qualified resource name or ID for model e.g. projects/297370817971/locations/{region}/models/4540613586807947264\n",
    "        endpoint_resource_name: The fully-qualified resource name or ID for endpoint e.g. projects/297370817971/locations/{region}/endpoints/1242430547200835584\n",
    "        machine_type: The machine type to serve the prediction requests.\n",
    "        accelerator_count: The number of accelerators to attach to the `machine_type`.\n",
    "        accelerator_type: The type of accelerators that may be attached to the machine as per `accelerator_count`.\n",
    "        max_replica_count: The maximum number of machine replicas the batch operation may be scaled to.\n",
    "        starting_replica_count: The number of machine replicas used at the start of the batch operation.\n",
    "    \"\"\"\n",
    "    import logging\n",
    "    \n",
    "    from google.cloud import aiplatform\n",
    "    from google.cloud.aiplatform.compat.types import job_state_v1\n",
    "    \n",
    "    def _get_endpoint(resource_name: str) -> aiplatform.Endpoint:\n",
    "        return aiplatform.Endpoint(resource_name)\n",
    "\n",
    "    def _get_model(resource_name: str) -> aiplatform.Model:\n",
    "        return aiplatform.Model(resource_name)\n",
    "\n",
    "    def _get_model_from_endpoint(endpoint: aiplatform.Endpoint) -> aiplatform.Model:\n",
    "        current_deployed_model_id = None\n",
    "\n",
    "        traffic_split = endpoint.gca_resource.traffic_split\n",
    "        for key in traffic_split:\n",
    "            if traffic_split[key] == 100:\n",
    "                current_deployed_model_id = key\n",
    "            break\n",
    "\n",
    "        if current_deployed_model_id:\n",
    "            for deployed_model in endpoint.gca_resource.deployed_models:\n",
    "                if deployed_model.id == current_deployed_model_id:\n",
    "                    return aiplatform.Model(deployed_model.model)\n",
    "\n",
    "\n",
    "    logging.info(f'input dataset URI: {bq_dataset_path}')\n",
    "\n",
    "    # Call Vertex AI custom job in another region\n",
    "    aiplatform.init(project=project_id)\n",
    "\n",
    "    if model_resource_name:\n",
    "        model = _get_model(model_resource_name)\n",
    "    elif endpoint_resource_name:\n",
    "        model = _get_model_from_endpoint(_get_endpoint(endpoint_resource_name))\n",
    "    else:\n",
    "        raise ValueError('model or endpoint resource name must be provided!')\n",
    "\n",
    "    logging.info(f'retrieved model URI: {model.uri}')\n",
    "\n",
    "    batch_pred_job = model.batch_predict(\n",
    "        job_display_name='batch-prediction',\n",
    "        bigquery_source=bq_dataset_path,\n",
    "        bigquery_destination_prefix=bigquery_destination,\n",
    "        instances_format=instances_format,\n",
    "        predictions_format=predictions_format,\n",
    "        machine_type=machine_type,\n",
    "        accelerator_count=accelerator_count,\n",
    "        accelerator_type=accelerator_type,\n",
    "        starting_replica_count=starting_replica_count,\n",
    "        max_replica_count=max_replica_count,\n",
    "        sync=True)\n",
    "\n",
    "    logging.info(f'batch prediction job: {batch_pred_job.resource_name}')\n",
    "\n",
    "    batch_pred_job.wait()\n",
    "    if batch_pred_job.state == job_state_v1.JobState.JOB_STATE_SUCCEEDED:\n",
    "        logging.info(f'batch prediction job has finished with info: '\n",
    "                     f'{batch_pred_job.completion_stats}')\n",
    "        logging.info(f'Predictions can be found at: '\n",
    "                     f'{batch_pred_job.output_info.gcs_output_directory}')\n",
    "    else:\n",
    "        raise RuntimeError(batch_pred_job.error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d96dcb-a020-4bab-b0e3-1e32f6b2aecf",
   "metadata": {},
   "source": [
    "## Define the Pipeline\n",
    "Define your pipeline. You can optionally give the pipeline a name and description. Define the structure by listing the components to be called in your pipeline; use `.after` to specify the order of execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a36e0154",
   "metadata": {},
   "outputs": [],
   "source": [
    "@AutoMLOps.pipeline #(name='automlops-pipeline', description='This is an optional description')\n",
    "def pipeline(bq_table: str,\n",
    "             data_path: str,\n",
    "             project_id: str,\n",
    "             bigquery_destination: str,\n",
    "             bq_dataset_path: str,\n",
    "             instances_format: str,\n",
    "             predictions_format: str,\n",
    "             model_resource_name: str,\n",
    "             endpoint_resource_name: str,\n",
    "             machine_type: str,\n",
    "             accelerator_count: int,\n",
    "             accelerator_type: str,\n",
    "             max_replica_count: int,\n",
    "             starting_replica_count: int\n",
    "            ):\n",
    "\n",
    "    batch_predict_task = batch_predict(\n",
    "             project_id=project_id,\n",
    "             bigquery_destination=bigquery_destination,\n",
    "             bq_dataset_path=bq_dataset_path,\n",
    "             instances_format=instances_format,\n",
    "             predictions_format=predictions_format,\n",
    "             model_resource_name=model_resource_name,\n",
    "             endpoint_resource_name=endpoint_resource_name,\n",
    "             machine_type=machine_type,\n",
    "             accelerator_count=accelerator_count,\n",
    "             accelerator_type=accelerator_type,\n",
    "             max_replica_count=max_replica_count,\n",
    "             starting_replica_count=starting_replica_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3874f4f6",
   "metadata": {},
   "source": [
    "## Define the Pipeline Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4cb3786b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "pipeline_params = {\n",
    "    'bq_table': f'{PROJECT_ID}.test_dataset.dry-beans-inferencing',\n",
    "    'data_path': f'gs://{PROJECT_ID}-bucket/data.csv',\n",
    "    'project_id': PROJECT_ID,\n",
    "    'bigquery_destination': f'bq://{PROJECT_ID}.test_dataset.dry-beans-inferencing-results',\n",
    "    'bq_dataset_path': f'bq://{PROJECT_ID}.test_dataset.dry-beans-inferencing',\n",
    "    'instances_format': 'bigquery',\n",
    "    'predictions_format': 'bigquery',\n",
    "    'model_resource_name': '',\n",
    "    'endpoint_resource_name': 'projects/45373616427/locations/us-central1/endpoints/2255296260661575680',\n",
    "    'machine_type': 'n1-standard-8',\n",
    "    'accelerator_count': 0,\n",
    "    'accelerator_type': 'ACCELERATOR_TYPE_UNSPECIFIED',\n",
    "    'max_replica_count': 2,\n",
    "    'starting_replica_count': 1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e0aa05",
   "metadata": {},
   "source": [
    "## Generate and Run the pipeline\n",
    "`AutoMLOps.generate` generates the code for the MLOps pipeline. `AutoMLOps.go` generates the code and runs the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db51a8d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Successfully saved requirements file in AutoMLOps/components/component_base/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "AutoMLOps.generate(project_id=PROJECT_ID,\n",
    "                   pipeline_params=pipeline_params,\n",
    "                   run_local=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ef279e8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Successfully saved requirements file in AutoMLOps/components/component_base/requirements.txt\n",
      "\u001b[0;32m Updating required API services in project automlops-sandbox \u001b[0m\n",
      "Operation \"operations/acat.p2-45373616427-79a68013-c0ab-4ea0-b749-caa21aa87a80\" finished successfully.\n",
      "\u001b[0;32m Checking for Artifact Registry: vertex-mlops-af in project automlops-sandbox \u001b[0m\n",
      "Listing items under project automlops-sandbox, location us-central1.\n",
      "\n",
      "vertex-mlops-af  DOCKER  STANDARD_REPOSITORY  Artifact Registry vertex-mlops-af in us-central1.  us-central1          Google-managed key  2023-01-11T17:12:26  2023-05-15T13:15:35  40493.254\n",
      "Artifact Registry: vertex-mlops-af already exists in project automlops-sandbox\n",
      "\u001b[0;32m Checking for GS Bucket: automlops-sandbox-bucket in project automlops-sandbox \u001b[0m\n",
      "gs://automlops-sandbox-bucket/\n",
      "GS Bucket: automlops-sandbox-bucket already exists in project automlops-sandbox\n",
      "\u001b[0;32m Checking for Service Account: vertex-pipelines in project automlops-sandbox \u001b[0m\n",
      "Pipeline Runner Service Account         vertex-pipelines@automlops-sandbox.iam.gserviceaccount.com  False\n",
      "Service Account: vertex-pipelines already exists in project automlops-sandbox\n",
      "\u001b[0;32m Updating required IAM roles in project automlops-sandbox \u001b[0m\n",
      "\u001b[0;32m Checking for Cloud Source Repository: AutoMLOps-repo in project automlops-sandbox \u001b[0m\n",
      "AutoMLOps-repo  automlops-sandbox  https://source.developers.google.com/p/automlops-sandbox/r/AutoMLOps-repo\n",
      "Cloud Source Repository: AutoMLOps-repo already exists in project automlops-sandbox\n",
      "\u001b[0;32m Checking for Cloud Tasks Queue: queueing-svc in project automlops-sandbox \u001b[0m\n",
      "queueing-svc       RUNNING  1000              500.0            100\n",
      "Cloud Tasks Queue: queueing-svc already exists in project automlops-sandbox\n",
      "\u001b[0;32m Checking for Cloudbuild Trigger: automlops-trigger in project automlops-sandbox \u001b[0m\n",
      "name: automlops-trigger\n",
      "Cloudbuild Trigger already exists in project automlops-sandbox for repo AutoMLOps-repo\n",
      "Initialized empty Git repository in /Users/srastatter/Documents/2023/MLOps-graduation/AutoMLOps-github/examples/inferencing/.git/\n",
      "Switched to a new branch 'automlops'\n",
      "[automlops (root-commit) fbf3dfe] Run AutoMLOps\n",
      " 38 files changed, 6876 insertions(+)\n",
      " create mode 100644 .ipynb_checkpoints/00_batch_prediction_example-checkpoint.ipynb\n",
      " create mode 100644 .ipynb_checkpoints/automlops_batch_prediction_pipeline-checkpoint.ipynb\n",
      " create mode 100644 .ipynb_checkpoints/automlops_example_batch_predict-checkpoint.ipynb\n",
      " create mode 100644 .ipynb_checkpoints/automlops_example_notebook-checkpoint.ipynb\n",
      " create mode 100644 .tmpfiles/batch_predict.yaml\n",
      " create mode 100644 .tmpfiles/batch_prediction.yaml\n",
      " create mode 100644 .tmpfiles/create_dataset.yaml\n",
      " create mode 100644 .tmpfiles/imports.py\n",
      " create mode 100644 .tmpfiles/pipeline_scaffold.py\n",
      " create mode 100644 00_batch_prediction_example.ipynb\n",
      " create mode 100644 AutoMLOps/cloud_run/queueing_svc/main.py\n",
      " create mode 100644 AutoMLOps/cloud_run/queueing_svc/pipeline_parameter_values.json\n",
      " create mode 100644 AutoMLOps/cloud_run/queueing_svc/requirements.txt\n",
      " create mode 100644 AutoMLOps/cloud_run/run_pipeline/Dockerfile\n",
      " create mode 100644 AutoMLOps/cloud_run/run_pipeline/main.py\n",
      " create mode 100644 AutoMLOps/cloud_run/run_pipeline/requirements.txt\n",
      " create mode 100644 AutoMLOps/cloudbuild.yaml\n",
      " create mode 100644 AutoMLOps/components/batch_predict/component.yaml\n",
      " create mode 100644 AutoMLOps/components/batch_prediction/component.yaml\n",
      " create mode 100644 AutoMLOps/components/component_base/Dockerfile\n",
      " create mode 100644 AutoMLOps/components/component_base/requirements.txt\n",
      " create mode 100644 AutoMLOps/components/component_base/src/batch_predict.py\n",
      " create mode 100644 AutoMLOps/components/component_base/src/batch_prediction.py\n",
      " create mode 100644 AutoMLOps/components/component_base/src/create_dataset.py\n",
      " create mode 100644 AutoMLOps/components/create_dataset/component.yaml\n",
      " create mode 100644 AutoMLOps/configs/defaults.yaml\n",
      " create mode 100644 AutoMLOps/pipelines/pipeline.py\n",
      " create mode 100644 AutoMLOps/pipelines/pipeline_runner.py\n",
      " create mode 100644 AutoMLOps/pipelines/runtime_parameters/pipeline_parameter_values.json\n",
      " create mode 100755 AutoMLOps/scripts/build_components.sh\n",
      " create mode 100755 AutoMLOps/scripts/build_pipeline_spec.sh\n",
      " create mode 100755 AutoMLOps/scripts/create_resources.sh\n",
      " create mode 100644 AutoMLOps/scripts/pipeline_spec/.gitkeep\n",
      " create mode 100755 AutoMLOps/scripts/run_all.sh\n",
      " create mode 100755 AutoMLOps/scripts/run_pipeline.sh\n",
      " create mode 100644 data/.DS_Store\n",
      " create mode 100644 data/Dry_Beans_Dataset_Inferencing.csv\n",
      " create mode 100644 data/load_data_to_bq.py\n",
      "To https://source.developers.google.com/p/automlops-sandbox/r/AutoMLOps-repo\n",
      " + b905e9a...fbf3dfe automlops -> automlops (forced update)\n",
      "Pushing code to automlops branch, triggering cloudbuild...\n",
      "Cloudbuild job running at: https://console.cloud.google.com/cloud-build/builds;region=us-central1\n",
      "\n",
      "#################################################################\n",
      "#                                                               #\n",
      "#                       RESOURCES MANIFEST                      #\n",
      "#---------------------------------------------------------------#\n",
      "#     Generated resources can be found at the following urls    #\n",
      "#                                                               #\n",
      "#################################################################\n",
      "\n",
      "Google Cloud Storage Bucket: https://console.cloud.google.com/storage/automlops-sandbox-bucket\n",
      "Artifact Registry: https://console.cloud.google.com/artifacts/docker/automlops-sandbox/us-central1/vertex-mlops-af\n",
      "Service Accounts: https://console.cloud.google.com/iam-admin/serviceaccounts?project=automlops-sandbox\n",
      "APIs: https://console.cloud.google.com/apis\n",
      "Cloud Source Repository: https://source.cloud.google.com/automlops-sandbox/AutoMLOps-repo/+/automlops:\n",
      "Cloud Build Jobs: https://console.cloud.google.com/cloud-build/builds;region=us-central1\n",
      "Vertex AI Pipeline Runs: https://console.cloud.google.com/vertex-ai/pipelines/runs\n",
      "Cloud Build Trigger: https://console.cloud.google.com/cloud-build/triggers;region=us-central1\n",
      "Cloud Run Service: https://console.cloud.google.com/run/detail/us-central1/run-pipeline\n",
      "Cloud Tasks Queue: https://console.cloud.google.com/cloudtasks/queue/us-central1/queueing-svc/tasks\n"
     ]
    }
   ],
   "source": [
    "AutoMLOps.go(project_id=PROJECT_ID,\n",
    "             pipeline_params=pipeline_params,\n",
    "             run_local=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53c06c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m102",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m102"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
