{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2023 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fsv4jGuU89rX"
   },
   "source": [
    "# AutoMLOps - Customer Churn Model Monitoring Example\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/automlops/blob/main/examples/inference/01_customer_churn_model_monitoring.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/automlops/blob/main/examples/inference/01_customer_churn_model_monitoring.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/automlops/main/examples/inference/01_customer_churn_model_monitoring.ipynb\">\n",
    "        <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "      Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</table>\n",
    "<br/><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lA32H1oKGgpf"
   },
   "source": [
    "# Overview\n",
    "\n",
    "This tutorial explores using AutoMLOps for model monitoring on a pre-trained customer churn model. The tutorial will walk you through how to use AutoMLOps to define, create and run inference pipelines, and create a model monitoring job around a Vertex AI Endpoint.\n",
    "\n",
    "### What is Model Monitoring?\n",
    "\n",
    "Modern applications rely on a well established set of capabilities to monitor the health of their services. Examples include:\n",
    "\n",
    "* software versioning\n",
    "* rigorous deployment processes\n",
    "* event logging\n",
    "* alerting/notication of situations requiring intervention\n",
    "* on-demand and automated diagnostic tracing\n",
    "* automated performance and functional testing\n",
    "\n",
    "You should be able to manage your ML services with the same degree of power and flexibility with which you can manage your applications. That's what MLOps is all about - managing ML services with the best practices Google and the broader computing industry have learned from generations of experience deploying well engineered, reliable, and scalable services.\n",
    "\n",
    "Model monitoring is only one piece of the MLOps puzzle - it helps answer the following questions:\n",
    "\n",
    "* How well do recent service requests match the training data used to build your model? This is called **training-serving skew**.\n",
    "* How significantly are service requests evolving over time? This is called **drift detection**.\n",
    "\n",
    "[Vertex Explainable AI](https://cloud.google.com/vertex-ai/docs/explainable-ai/overview) adds another facet to model monitoring, which we call feature attribution monitoring. Explainable AI enables you to understand the relative contribution of each feature to a resulting prediction. In essence, it assesses the magnitude of each feature's influence.\n",
    "\n",
    "If production traffic differs from  training data, or varies substantially over time, **either in terms of model predictions or feature attributions**, that's likely to impact the quality of the answers your model produces. When that happens, you'd like to be alerted automatically and responsively, so that **you can anticipate problems before they affect your customer experiences or your revenue streams**.\n",
    "\n",
    "Learn more about [Vertex AI Model Monitoring](https://cloud.google.com/vertex-ai/docs/model-monitoring)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t6Cd51FkG09E"
   },
   "source": [
    "### Objective\n",
    "\n",
    "In this tutorial, you learn to use the `Vertex AI Model Monitoring` service to detect drift and anomalies in prediction requests from a deployed `Vertex AI Model` resource. You will then learn how to create and run MLOps pipelines integrated with CI/CD. The pipeline goes through the following steps:\n",
    "\n",
    "1. deploy_and_test_model: Upload a pretrained model and deploy it to an Endpoint. Runs tests for predictions and explainability. \n",
    "2. create_monitoring_job: Creates a model monitoring job and sends alerts to specified emails. \n",
    "3. test_monitoring_job: Generate synthetic prediction requests and analyze monitoring. \n",
    "\n",
    "# Prerequisites\n",
    "\n",
    "In order to use AutoMLOps, the following are required:\n",
    "\n",
    "- Python 3.7 - 3.10\n",
    "- [Google Cloud SDK 407.0.0](https://cloud.google.com/sdk/gcloud/reference)\n",
    "- [beta 2022.10.21](https://cloud.google.com/sdk/gcloud/reference/beta)\n",
    "- `git` installed\n",
    "- `git` logged-in:\n",
    "```\n",
    "  git config --global user.email \"you@example.com\"\n",
    "  git config --global user.name \"Your Name\"\n",
    "```\n",
    "- [Application Default Credentials (ADC)](https://cloud.google.com/docs/authentication/provide-credentials-adc) are setup. This can be done through the following commands:\n",
    "```\n",
    "gcloud auth application-default login\n",
    "gcloud config set account <account@example.com>\n",
    "```\n",
    "\n",
    "# APIs & IAM\n",
    "Based on the user options selection, AutoMLOps will enable up to the following APIs during the provision step:\n",
    "- [aiplatform.googleapis.com](https://cloud.google.com/vertex-ai/docs/reference/rest)\n",
    "- [artifactregistry.googleapis.com](https://cloud.google.com/artifact-registry/docs/reference/rest)\n",
    "- [cloudbuild.googleapis.com](https://cloud.google.com/build/docs/api/reference/rest)\n",
    "- [cloudfunctions.googleapis.com](https://cloud.google.com/functions/docs/reference/rest)\n",
    "- [cloudresourcemanager.googleapis.com](https://cloud.google.com/resource-manager/reference/rest)\n",
    "- [cloudscheduler.googleapis.com](https://cloud.google.com/scheduler/docs/reference/rest)\n",
    "- [compute.googleapis.com](https://cloud.google.com/compute/docs/reference/rest/v1)\n",
    "- [iam.googleapis.com](https://cloud.google.com/iam/docs/reference/rest)\n",
    "- [iamcredentials.googleapis.com](https://cloud.google.com/iam/docs/reference/credentials/rest)\n",
    "- [pubsub.googleapis.com](https://cloud.google.com/pubsub/docs/reference/rest)\n",
    "- [run.googleapis.com](https://cloud.google.com/run/docs/reference/rest)\n",
    "- [storage.googleapis.com](https://cloud.google.com/storage/docs/apis)\n",
    "- [sourcerepo.googleapis.com](https://cloud.google.com/source-repositories/docs/reference/rest)\n",
    "\n",
    "\n",
    "AutoMLOps will create the following service account and update [IAM permissions](https://cloud.google.com/iam/docs/understanding-roles) during the provision step:\n",
    "1. Pipeline Runner Service Account (defaults to: vertex-pipelines@PROJECT_ID.iam.gserviceaccount.com). Roles added:\n",
    "- roles/aiplatform.user\n",
    "- roles/artifactregistry.reader\n",
    "- roles/bigquery.user\n",
    "- roles/bigquery.dataEditor\n",
    "- roles/iam.serviceAccountUser\n",
    "- roles/storage.admin\n",
    "- roles/cloudfunctions.admin\n",
    "\n",
    "# User Guide\n",
    "\n",
    "For a user-guide, please view these [slides](../../AutoMLOps_User_Guide.pdf).\n",
    "\n",
    "# Costs\n",
    "\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "- Vertex AI\n",
    "- Artifact Registry\n",
    "- Cloud Storage\n",
    "- Cloud Source Repository\n",
    "- Cloud Build\n",
    "- Cloud Run\n",
    "- Cloud Scheduler\n",
    "- Cloud Pub/Sub\n",
    "\n",
    "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing), and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage.\n",
    "\n",
    "# Ground-rules for using AutoMLOps\n",
    "1. Do not use variables, functions, code, etc. not defined within the scope of a custom component. These custom components will become containers and will have no reference to the out of scope code.\n",
    "2. Import statements and helper functions must be added inside the function. Provide parameter type hints.\n",
    "3. Test each of your components for accuracy and correctness before running them using AutoMLOps. We cannot fix bugs automatically; bugs are much more difficult to fix once they are made into pipelines.\n",
    "4. If you are using Kubeflow, be sure to define all the requirements needed to run the custom component - it can be easy to leave out packages which will cause the container to fail when running within a pipeline.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "edba71dc9840"
   },
   "source": [
    "# Model\n",
    "\n",
    "This tutorial uses a pre-trained model, where the model artifacts are stored in a public Cloud Storage bucket. The model predicts for an online gaming site, the probability that a player may churn, i.e. stop being an active player.\n",
    "\n",
    "The model you use in this notebook is based on [this blog post](https://cloud.google.com/blog/topics/developers-practitioners/churn-prediction-game-developers-using-google-analytics-4-ga4-and-bigquery-ml). The idea behind this model is that your company has extensive log data describing how your game users have interacted with the site. The raw data contains the following categories of information:\n",
    "\n",
    "- identity - unique player identitity numbers\n",
    "- demographic features - information about the player, such as the geographic region in which a player is located\n",
    "- behavioral features - counts of the number of times a  player has triggered certain game events, such as reaching a new level\n",
    "- churn propensity - this is the label or target feature, it provides an estimated probability that this player will churn, i.e. stop being an active player.\n",
    "\n",
    "The blog article referenced above explains how to use BigQuery to store the raw data, pre-process the data for machine learning, and train the corresponding model. Because this notebook focuses on model monitoring, rather than training models, you're going to reuse a pre-trained version of this model, which has been exported to Cloud Storage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "For training data, we are using the [Google Analytics 4 (GA4)](https://cloud.google.com/blog/topics/developers-practitioners/churn-prediction-game-developers-using-google-analytics-4-ga4-and-bigquery-ml) BQML train dataset which is a publicly available dataset that contains a sample of obfuscated BiqQuery event export data using Google Analytics 4's standard web ecommerce implementation on [Google Merchandise Store](https://shop.googlemerchandisestore.com/). This is the same public BigQuery table that was used to train the pre-trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup Git\n",
    "Set up your git configuration below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git config --global user.email 'you@example.com'\n",
    "!git config --global user.name 'Your Name'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install AutoMLOps\n",
    "\n",
    "Install AutoMLOps from [PyPI](https://pypi.org/project/google-cloud-automlops/), or locally by cloning the repo and running `pip install .`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install google-cloud-automlops --user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restart the kernel\n",
    "Once you've installed the AutoMLOps package, you need to restart the notebook kernel so it can find the package.\n",
    "\n",
    "**Note: Once this cell has finished running, continue on. You do not need to re-run any of the cells above.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.getenv('IS_TESTING'):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set your project ID\n",
    "Set your project ID below. If you don't know your project ID, leave the field blank and the following cells may be able to find it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = '[your-project-id]'  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project ID: automlops-sandbox\n"
     ]
    }
   ],
   "source": [
    "if PROJECT_ID == '' or PROJECT_ID is None or PROJECT_ID == '[your-project-id]':\n",
    "    # Get your GCP project id from gcloud\n",
    "    shell_output = !gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print('Project ID:', PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\r\n"
     ]
    }
   ],
   "source": [
    "! gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set your Model_ID below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = 'GA4-BQML-Monitoring'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "42c8a7c56abd"
   },
   "source": [
    "# Monitoring Emails\n",
    "\n",
    "Set your user email address to receive monitoring alerts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ce2589511bb6"
   },
   "outputs": [],
   "source": [
    "ALERT_EMAILS = ['noreply@google.com']  # Update with your emails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. AutoMLOps Pipeline\n",
    "This workflow will define and generate a pipeline using AutoMLOps. `generate()` will create all the necessary files but not run them. `go()` will create all the necessary files, resources, push the code to the source repo to trigger the build, and then submit a Pipeline training job to Vertex AI. Please see the [readme](https://github.com/GoogleCloudPlatform/automlops/blob/main/README.md) for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import AutoMLOps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google_cloud_automlops import AutoMLOps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clear the cache\n",
    "`AutoMLOps.clear_cache` will remove previous instantiations of AutoMLOps components and pipelines. Use this function if you have previously defined a component that you no longer need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache cleared.\n"
     ]
    }
   ],
   "source": [
    "AutoMLOps.clear_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lAOk8UqvCL0S"
   },
   "source": [
    "## Deploy and Test your model\n",
    "\n",
    "\n",
    "The churn propensity model you use in this notebook has been trained in BigQuery ML and exported to a Cloud Storage bucket. Define a custom component for deploying this pretrained model and testing it for predictions and explanability. Import statements and helper functions must be added inside the function. Provide parameter type hints.\n",
    "\n",
    "**Note: we currently only support python primitive types for component parameters. If you would like to use something more advanced, please use the Kubeflow spec instead.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@AutoMLOps.component(\n",
    "    packages_to_install=[\n",
    "        'explainable_ai_sdk',\n",
    "        'google-cloud-aiplatform'\n",
    "    ]\n",
    ")\n",
    "def deploy_and_test_model(\n",
    "    model_directory: str,\n",
    "    project_id: str,\n",
    "    region: str\n",
    "):\n",
    "    \"\"\"Custom component that uploads a saved model from GCS to Vertex Model Registry\n",
    "       and deploys the model to an endpoint for online prediction. Runs a prediction\n",
    "       and explanation test as well.\n",
    "\n",
    "    Args:\n",
    "        model_directory: GS location of saved model.\n",
    "        project_id: Project_id.\n",
    "        region: Region.\n",
    "    \"\"\"\n",
    "    from google.cloud import aiplatform\n",
    "    from google.cloud.aiplatform.explain.metadata.tf.v2 import \\\n",
    "    saved_model_metadata_builder\n",
    "    import pprint as pp\n",
    "\n",
    "    aiplatform.init(project=project_id, location=region)\n",
    "\n",
    "    MODEL_NAME = 'churn'\n",
    "    IMAGE = 'us-docker.pkg.dev/cloud-aiplatform/prediction/tf2-cpu.2-5:latest'\n",
    "    params = {'sampled_shapley_attribution': {'path_count': 10}}\n",
    "    EXPLAIN_PARAMS = aiplatform.explain.ExplanationParameters(params)\n",
    "    builder = saved_model_metadata_builder.SavedModelMetadataBuilder(\n",
    "        model_path=model_directory, outputs_to_explain=['churned_probs']\n",
    "    )\n",
    "    EXPLAIN_META = builder.get_metadata_protobuf()\n",
    "    DEFAULT_INPUT = {\n",
    "        'cnt_ad_reward': 0,\n",
    "        'cnt_challenge_a_friend': 0,\n",
    "        'cnt_completed_5_levels': 1,\n",
    "        'cnt_level_complete_quickplay': 3,\n",
    "        'cnt_level_end_quickplay': 5,\n",
    "        'cnt_level_reset_quickplay': 2,\n",
    "        'cnt_level_start_quickplay': 6,\n",
    "        'cnt_post_score': 34,\n",
    "        'cnt_spend_virtual_currency': 0,\n",
    "        'cnt_use_extra_steps': 0,\n",
    "        'cnt_user_engagement': 120,\n",
    "        'country': 'Denmark',\n",
    "        'dayofweek': 3,\n",
    "        'julianday': 254,\n",
    "        'language': 'da-dk',\n",
    "        'month': 9,\n",
    "        'operating_system': 'IOS',\n",
    "        'user_pseudo_id': '104B0770BAE16E8B53DF330C95881893',\n",
    "    }\n",
    "\n",
    "    model = aiplatform.Model.upload(\n",
    "        display_name=MODEL_NAME,\n",
    "        artifact_uri=model_directory,\n",
    "        serving_container_image_uri=IMAGE,\n",
    "        explanation_parameters=EXPLAIN_PARAMS,\n",
    "        explanation_metadata=EXPLAIN_META,\n",
    "        sync=True\n",
    "    )\n",
    "\n",
    "    endpoint = model.deploy(\n",
    "        machine_type='n1-standard-4',\n",
    "        deployed_model_display_name='deployed-churn-model')\n",
    "\n",
    "    # Test predictions\n",
    "    print('running prediction test...')\n",
    "    try:\n",
    "        resp = endpoint.predict([DEFAULT_INPUT])\n",
    "        for i in resp.predictions:\n",
    "            vals = i['churned_values']\n",
    "            probs = i['churned_probs']\n",
    "        for i in range(len(vals)):\n",
    "            print(vals[i], probs[i])\n",
    "        pp.pprint(resp)\n",
    "    except Exception as ex:\n",
    "        print('prediction request failed', ex)\n",
    "\n",
    "    # Test explanations\n",
    "    print('\\nrunning explanation test...')\n",
    "    try:\n",
    "        features = []\n",
    "        scores = []\n",
    "        resp = endpoint.explain([DEFAULT_INPUT])\n",
    "        for i in resp.explanations:\n",
    "            for j in i.attributions:\n",
    "                for k in j.feature_attributions:\n",
    "                    features.append(k)\n",
    "                    scores.append(j.feature_attributions[k])\n",
    "        features = [x for _, x in sorted(zip(scores, features))]\n",
    "        scores = sorted(scores)\n",
    "        for i in range(len(scores)):\n",
    "            print(scores[i], features[i])\n",
    "        pp.pprint(resp)\n",
    "    except Exception as ex:\n",
    "        print('explanation request failed', ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Monitoring Job\n",
    "\n",
    "Define a custom component for creating a model monitoring job around the previously deployed model. Import statements and helper functions must be added inside the function. Provide parameter type hints.\n",
    "\n",
    "The following code uses the Google Python client library to translate your configuration settings into a programmatic request to start a model monitoring job. Instantiating a monitoring job can take some time. If everything looks good with your request, you'll get a successful API response. Then, you'll need to check your email to receive a notification that the job is running.\n",
    "\n",
    "After a minute or two, you should receive email at the address you configured above for USER_EMAIL. This email confirms successful deployment of your monitoring job. Here's a sample of what this email might look like:\n",
    "<br>\n",
    "<br>\n",
    "<img src=\"https://storage.googleapis.com/mco-general/img/mm6.png\" />\n",
    "<br>\n",
    "As your monitoring job collects data, measurements are stored in Cloud Storage and you are free to examine your data at any time. The \"Statistics and Anomalies Root Path\" specifies the location of your measurements in Cloud Storage. Run the following cell to see an example of the layout of these measurements in Cloud Storage. If you substitute the Cloud Storage URL in your job creation email, you can view the structure and content of the data files for your own monitoring job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@AutoMLOps.component(\n",
    "    packages_to_install=[\n",
    "        'google-cloud-aiplatform'\n",
    "    ]\n",
    ")\n",
    "def create_monitoring_job(\n",
    "    alert_emails: list,\n",
    "    cnt_user_engagement_threshold_value: float,\n",
    "    country_threshold_value: float,\n",
    "    data_source: str,\n",
    "    log_sampling_rate: float,\n",
    "    monitor_interval: int,\n",
    "    project_id: str,\n",
    "    region: str,\n",
    "    target: str\n",
    "):\n",
    "    \"\"\"Custom component that creates a model monitoring job on the given model.\n",
    "\n",
    "    Args:\n",
    "        alert_emails: List of emails to send monitoring alerts.\n",
    "        cnt_user_engagement_threshold_value: Threshold value for the cnt_user_engagement feature.\n",
    "        country_threshold_value: Threshold value for the country feature.\n",
    "        data_source: BQ training data table.        \n",
    "        log_sampling_rate: Sampling rate.\n",
    "        monitor_interval: Monitoring interval in hours.\n",
    "        project_id: Project_id.\n",
    "        region: Region.\n",
    "        target: Prediction target column name in training dataset.\n",
    "    \"\"\"\n",
    "    from google.cloud import aiplatform\n",
    "    from google.cloud.aiplatform import model_monitoring\n",
    "\n",
    "    aiplatform.init(project=project_id, location=region)\n",
    "\n",
    "    JOB_NAME = 'churn'\n",
    "    SKEW_THRESHOLDS = {\n",
    "        'country': country_threshold_value,\n",
    "        'cnt_user_engagement': cnt_user_engagement_threshold_value,\n",
    "    }\n",
    "    DRIFT_THRESHOLDS = {\n",
    "        'country': country_threshold_value,\n",
    "        'cnt_user_engagement': cnt_user_engagement_threshold_value,\n",
    "    }\n",
    "    ATTRIB_SKEW_THRESHOLDS = {\n",
    "        'country': country_threshold_value,\n",
    "        'cnt_user_engagement': cnt_user_engagement_threshold_value,\n",
    "    }\n",
    "    ATTRIB_DRIFT_THRESHOLDS = {\n",
    "        'country': country_threshold_value,\n",
    "        'cnt_user_engagement': cnt_user_engagement_threshold_value,\n",
    "    }\n",
    "\n",
    "    skew_config = model_monitoring.SkewDetectionConfig(\n",
    "        data_source=data_source,\n",
    "        skew_thresholds=SKEW_THRESHOLDS,\n",
    "        attribute_skew_thresholds=ATTRIB_SKEW_THRESHOLDS,\n",
    "        target_field=target,\n",
    "    )\n",
    "\n",
    "    drift_config = model_monitoring.DriftDetectionConfig(\n",
    "        drift_thresholds=DRIFT_THRESHOLDS,\n",
    "        attribute_drift_thresholds=ATTRIB_DRIFT_THRESHOLDS,\n",
    "    )\n",
    "\n",
    "    explanation_config = model_monitoring.ExplanationConfig()\n",
    "    objective_config = model_monitoring.ObjectiveConfig(\n",
    "        skew_config, drift_config, explanation_config\n",
    "    )\n",
    "\n",
    "    # Create sampling configuration\n",
    "    random_sampling = model_monitoring.RandomSampleConfig(sample_rate=log_sampling_rate)\n",
    "\n",
    "    # Create schedule configuration\n",
    "    schedule_config = model_monitoring.ScheduleConfig(monitor_interval=monitor_interval)\n",
    "\n",
    "    # Create alerting configuration.\n",
    "    alerting_config = model_monitoring.EmailAlertConfig(\n",
    "        user_emails=alert_emails, enable_logging=True\n",
    "    )\n",
    "\n",
    "    endpoint = aiplatform.Endpoint.list(filter='display_name=\"churn_endpoint\"')[0]\n",
    "    # Create the monitoring job.\n",
    "    job = aiplatform.ModelDeploymentMonitoringJob.create(\n",
    "        display_name=JOB_NAME,\n",
    "        logging_sampling_strategy=random_sampling,\n",
    "        schedule_config=schedule_config,\n",
    "        alert_config=alerting_config,\n",
    "        objective_configs=objective_config,\n",
    "        project=project_id,\n",
    "        location=region,\n",
    "        endpoint=endpoint,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test your Monitoring Job\n",
    "\n",
    "Send a first test prediction request. The model monitoring service will analyze the distribution of features and automatically create a baseline to monitor deviations from the baseline. After your `Endpoint` receives a 1000 prediction requests, the modeling service will automatically parse and create the `input schema`. In this example, the first 1000 entries in the BigQuery training data are used as the first 1000 prediction requests.\n",
    "\n",
    "Define a custom component for testing the monitoring job. Import statements and helper functions must be added inside the function. Provide parameter type hints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@AutoMLOps.component(\n",
    "    packages_to_install=[\n",
    "        'google-cloud-bigquery',\n",
    "        'google-cloud-aiplatform'\n",
    "    ]\n",
    ")\n",
    "def test_monitoring_job(\n",
    "    data_source: str,\n",
    "    project_id: str,\n",
    "    region: str,\n",
    "    target: str\n",
    "):\n",
    "    \"\"\"Custom component that uploads a saved model from GCS to Vertex Model Registry\n",
    "       and deploys the model to an endpoint for online prediction. Runs a prediction\n",
    "       and explanation test as well.\n",
    "\n",
    "    Args:\n",
    "        data_source: BQ training data table.\n",
    "        project_id: Project_id.\n",
    "        region: Region.\n",
    "        target: Prediction target column name in training dataset.\n",
    "    \"\"\"\n",
    "    import time\n",
    "\n",
    "    from google.cloud import aiplatform\n",
    "    from google.cloud import bigquery\n",
    "\n",
    "    bq_client = bigquery.Client(project=project_id)\n",
    "    # Download the table.\n",
    "    table = bigquery.TableReference.from_string(data_source[5:])\n",
    "\n",
    "    rows = bq_client.list_rows(table, max_results=1000)\n",
    "\n",
    "    instances = []\n",
    "    for row in rows:\n",
    "        instance = {}\n",
    "        for key, value in row.items():\n",
    "            if key == target:\n",
    "                continue\n",
    "            if value is None:\n",
    "                value = \"\"\n",
    "            instance[key] = value\n",
    "        instances.append(instance)\n",
    "\n",
    "    print(len(instances))\n",
    "\n",
    "    endpoint = aiplatform.Endpoint.list(filter='display_name=\"churn_endpoint\"')[0]\n",
    "    response = endpoint.predict(instances=instances)\n",
    "    prediction = response[0]\n",
    "    # print the predictions\n",
    "    print(prediction)\n",
    "\n",
    "    # Pause a bit for the baseline distribution to be calculated\n",
    "    time.sleep(120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Pipeline\n",
    "Define your pipeline. You can optionally give the pipeline a name and description. Define the structure by listing the components to be called in your pipeline; use `.after` to specify the order of execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@AutoMLOps.pipeline(\n",
    "    name='automlops-monitoring-pipeline',\n",
    "    description='This is an example model monitoring pipeline')\n",
    "def pipeline(alert_emails: list,\n",
    "             cnt_user_engagement_threshold_value: float,\n",
    "             country_threshold_value: float,\n",
    "             data_source: str,\n",
    "             log_sampling_rate: float,\n",
    "             model_directory: str,\n",
    "             monitor_interval: int,\n",
    "             project_id: str,\n",
    "             region: str,\n",
    "             target: str):\n",
    "\n",
    "    deploy_and_test_model_task = deploy_and_test_model(\n",
    "        model_directory=model_directory,\n",
    "        project_id=project_id,\n",
    "        region=region)\n",
    "    \n",
    "    create_monitoring_job_task = create_monitoring_job(\n",
    "        alert_emails=alert_emails,\n",
    "        cnt_user_engagement_threshold_value=cnt_user_engagement_threshold_value,\n",
    "        country_threshold_value=country_threshold_value,\n",
    "        data_source=data_source,\n",
    "        log_sampling_rate=log_sampling_rate,\n",
    "        monitor_interval=monitor_interval,\n",
    "        project_id=project_id,\n",
    "        region=region,\n",
    "        target=target).after(deploy_and_test_model_task)\n",
    "    \n",
    "    test_monitoring_job_task = test_monitoring_job(\n",
    "        data_source=data_source,\n",
    "        project_id=project_id,\n",
    "        region=region,\n",
    "        target=target).after(create_monitoring_job_task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Pipeline Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_params = {\n",
    "    'alert_emails': ALERT_EMAILS,\n",
    "    'cnt_user_engagement_threshold_value': 0.001,\n",
    "    'country_threshold_value': 0.001,\n",
    "    'data_source': 'bq://mco-mm.bqmlga4.train',\n",
    "    'log_sampling_rate': 0.8,\n",
    "    'model_directory': 'gs://mco-mm/churn',\n",
    "    'monitor_interval': 1,\n",
    "    'project_id': PROJECT_ID,\n",
    "    'region': 'us-central1',\n",
    "    'target': 'churned'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate and Run the pipeline\n",
    "`AutoMLOps.generate(...)` generates the MLOps codebase. Users can specify the tooling and technologies they would like to use in their MLOps pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing directories under AutoMLOps/\n",
      "Writing configurations to AutoMLOps/configs/defaults.yaml\n",
      "Writing Kubeflow Pipelines code to AutoMLOps/pipelines, AutoMLOps/components, AutoMLOps/services\n",
      "Writing README.md to AutoMLOps/README.md\n",
      "Writing scripts to AutoMLOps/scripts\n",
      "Writing CloudBuild config to AutoMLOps/cloudbuild.yaml\n",
      "Code Generation Complete.\n"
     ]
    }
   ],
   "source": [
    "AutoMLOps.generate(project_id=PROJECT_ID,\n",
    "                   pipeline_params=pipeline_params,\n",
    "                   use_ci=True,\n",
    "                   naming_prefix=MODEL_ID,\n",
    "                   schedule_pattern='59 11 * * 0' # retrain every Sunday at Midnight\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`AutoMLOps.provision(...)` runs provisioning scripts to create and maintain necessary infra for MLOps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Provisioning requires these permissions:\n",
      "-cloudfunctions.functions.get\n",
      "-serviceusage.services.use\n",
      "-serviceusage.services.enable\n",
      "-cloudfunctions.functions.create\n",
      "-pubsub.subscriptions.list\n",
      "-cloudscheduler.jobs.list\n",
      "-pubsub.topics.create\n",
      "-source.repos.list\n",
      "-artifactregistry.repositories.create\n",
      "-resourcemanager.projects.setIamPolicy\n",
      "-iam.serviceAccounts.listiam.serviceAccounts.create\n",
      "-pubsub.subscriptions.create\n",
      "-cloudscheduler.jobs.create\n",
      "-storage.buckets.create\n",
      "-source.repos.create\n",
      "-artifactregistry.repositories.list\n",
      "-cloudbuild.builds.create\n",
      "-cloudbuild.builds.list\n",
      "-pubsub.topics.list\n",
      "-storage.buckets.get\n",
      "\n",
      "You are currently using: srastatter@google.com. Please check your account permissions.\n",
      "The following are the recommended roles for provisioning:\n",
      "-roles/resourcemanager.projectIamAdmin\n",
      "-roles/cloudfunctions.admin\n",
      "-roles/artifactregistry.admin\n",
      "-roles/iam.serviceAccountAdmin\n",
      "-roles/serviceusage.serviceUsageAdmin\n",
      "-roles/aiplatform.serviceAgent\n",
      "-roles/cloudscheduler.admin\n",
      "-roles/pubsub.editor\n",
      "-roles/source.admin\n",
      "-roles/cloudbuild.builds.editor\n",
      "\n",
      "\u001b[0;32m Setting up API services in project automlops-sandbox \u001b[0m\n",
      "Operation \"operations/acat.p2-45373616427-990bb410-2998-4b37-b37f-09ed724e9519\" finished successfully.\n",
      "\u001b[0;32m Setting up Artifact Registry in project automlops-sandbox \u001b[0m\n",
      "Listing items under project automlops-sandbox, location us-central1.\n",
      "\n",
      "dry-beans-dt-artifact-registry  DOCKER  STANDARD_REPOSITORY  Artifact Registry dry-beans-dt-artifact-registry in us-central1.  us-central1          Google-managed key  2023-09-05T11:25:48  2023-09-05T14:47:39  3200.712\n",
      "Artifact Registry: dry-beans-dt-artifact-registry already exists in project automlops-sandbox\n",
      "\u001b[0;32m Setting up Storage Bucket in project automlops-sandbox \u001b[0m\n",
      "gs://automlops-sandbox-dry-beans-dt-bucket/\n",
      "GS Bucket: automlops-sandbox-dry-beans-dt-bucket already exists in project automlops-sandbox\n",
      "\u001b[0;32m Setting up Pipeline Job Runner Service Account in project automlops-sandbox \u001b[0m\n",
      "Pipeline Runner Service Account         vertex-pipelines@automlops-sandbox.iam.gserviceaccount.com           False\n",
      "Service Account: vertex-pipelines already exists in project automlops-sandbox\n",
      "\u001b[0;32m Setting up IAM roles for Pipeline Job Runner Service Account in project automlops-sandbox \u001b[0m\n",
      "\u001b[0;32m Setting up Cloud Source Repository in project automlops-sandbox \u001b[0m\n",
      "dry-beans-dt-repository  automlops-sandbox  https://source.developers.google.com/p/automlops-sandbox/r/dry-beans-dt-repository\n",
      "Cloud Source Repository: dry-beans-dt-repository already exists in project automlops-sandbox\n",
      "\u001b[0;32m Setting up Queueing Service in project automlops-sandbox \u001b[0m\n",
      "name: projects/automlops-sandbox/topics/dry-beans-dt-queueing-svc\n",
      "Pub/Sub Topic: dry-beans-dt-queueing-svc already exists in project automlops-sandbox\n",
      "\u001b[0;32m Deploying Cloud Functions: dry-beans-dt-job-submission-svc in project automlops-sandbox \u001b[0m\n",
      "Deploying function (may take a while - up to 2 minutes)...\n",
      "..\n",
      "For Cloud Build Logs, visit: https://console.cloud.google.com/cloud-build/builds;region=us-central1/0dcd7601-a440-4c7e-8ea9-28d1cf927991?project=45373616427\n",
      "........................................................................done.\n",
      "availableMemoryMb: 512\n",
      "buildId: 0dcd7601-a440-4c7e-8ea9-28d1cf927991\n",
      "buildName: projects/45373616427/locations/us-central1/builds/0dcd7601-a440-4c7e-8ea9-28d1cf927991\n",
      "dockerRegistry: CONTAINER_REGISTRY\n",
      "entryPoint: process_request\n",
      "eventTrigger:\n",
      "  eventType: google.pubsub.topic.publish\n",
      "  failurePolicy: {}\n",
      "  resource: projects/automlops-sandbox/topics/dry-beans-dt-queueing-svc\n",
      "  service: pubsub.googleapis.com\n",
      "ingressSettings: ALLOW_ALL\n",
      "labels:\n",
      "  deployment-tool: cli-gcloud\n",
      "maxInstances: 3000\n",
      "name: projects/automlops-sandbox/locations/us-central1/functions/dry-beans-dt-job-submission-svc\n",
      "runtime: python39\n",
      "serviceAccountEmail: vertex-pipelines@automlops-sandbox.iam.gserviceaccount.com\n",
      "sourceUploadUrl: https://storage.googleapis.com/uploads-961973632599.us-central1.cloudfunctions.appspot.com/a2ee896a-a9dc-41ee-a129-fc000fc59dc9.zip\n",
      "status: ACTIVE\n",
      "timeout: 540s\n",
      "updateTime: '2023-09-08T03:06:05.246Z'\n",
      "versionId: '2'\n",
      "\u001b[0;32m Setting up Cloud Build Trigger in project automlops-sandbox \u001b[0m\n",
      "name: dry-beans-dt-build-trigger\n",
      "Cloudbuild Trigger already exists in project automlops-sandbox for repo dry-beans-dt-repository\n",
      "\u001b[0;32m Setting up Cloud Scheduler Job in project automlops-sandbox \u001b[0m\n",
      "dry-beans-dt-schedule  us-central1  59 11 * * 0 (Etc/UTC)  Pub/Sub      ENABLED\n",
      "Cloud Scheduler Job: dry-beans-dt-schedule already exists in project automlops-sandbox\n"
     ]
    }
   ],
   "source": [
    "AutoMLOps.provision(hide_warnings=False)            # hide_warnings is optional, defaults to True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`AutoMLOps.deploy(...)` builds and pushes component container, then triggers the pipeline job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Running precheck for deploying requires these permissions:\n",
      "-artifactregistry.repositories.get\n",
      "-cloudbuild.builds.get\n",
      "-resourcemanager.projects.getIamPolicy\n",
      "-storage.buckets.update\n",
      "-serviceusage.services.get\n",
      "-cloudfunctions.functions.get\n",
      "-pubsub.topics.get\n",
      "-iam.serviceAccounts.get\n",
      "-source.repos.update\n",
      "-pubsub.subscriptions.get\n",
      "\n",
      "You are currently using: srastatter@google.com. Please check your account permissions.\n",
      "The following are the recommended roles for deploying with precheck:\n",
      "-roles/serviceusage.serviceUsageViewer\n",
      "-roles/iam.roleViewer\n",
      "-roles/pubsub.viewer\n",
      "-roles/storage.admin\n",
      "-roles/cloudbuild.builds.editor\n",
      "-roles/source.writer\n",
      "-roles/iam.serviceAccountUser\n",
      "-roles/cloudfunctions.viewer\n",
      "-roles/artifactregistry.reader\n",
      "\n",
      "Checking for required API services in project automlops-sandbox...\n",
      "Checking for Artifact Registry in project automlops-sandbox...\n",
      "Checking for Storage Bucket in project automlops-sandbox...\n",
      "Checking for Pipeline Runner Service Account in project automlops-sandbox...\n",
      "Checking for IAM roles on Pipeline Runner Service Account in project automlops-sandbox...\n",
      "Checking for Cloud Source Repo in project automlops-sandbox...\n",
      "Checking for Pub/Sub Topic in project automlops-sandbox...\n",
      "Checking for Pub/Sub Subscription in project automlops-sandbox...\n",
      "Checking for Cloud Functions Pipeline Job Submission Service in project automlops-sandbox...\n",
      "Checking for Cloud Build Trigger in project automlops-sandbox...\n",
      "Precheck successfully completed, continuing to deployment.\n",
      "\n",
      "[automlops 1fa5664] Run AutoMLOps\n",
      " 7 files changed, 34 insertions(+), 228 deletions(-)\n",
      " delete mode 100644 AutoMLOps/components/component_base/src/custom_train_model.py\n",
      " delete mode 100644 AutoMLOps/components/custom_train_model/component.yaml\n",
      "remote: Waiting for private key checker: 5/5 objects left        \n",
      "To https://source.developers.google.com/p/automlops-sandbox/r/dry-beans-dt-repository\n",
      "   70a23dd..1fa5664  automlops -> automlops\n",
      "Pushing code to automlops branch, triggering build...\n",
      "Cloud Build job running at: https://console.cloud.google.com/cloud-build/builds;region=us-central1\n",
      "Please wait for this build job to complete.\n",
      "\n",
      "#################################################################\n",
      "#                                                               #\n",
      "#                       RESOURCES MANIFEST                      #\n",
      "#---------------------------------------------------------------#\n",
      "#     Generated resources can be found at the following urls    #\n",
      "#                                                               #\n",
      "#################################################################\n",
      "\n",
      "Google Cloud Storage Bucket: https://console.cloud.google.com/storage/automlops-sandbox-dry-beans-dt-bucket\n",
      "Artifact Registry: https://console.cloud.google.com/artifacts/docker/automlops-sandbox/us-central1/dry-beans-dt-artifact-registry\n",
      "Service Accounts: https://console.cloud.google.com/iam-admin/serviceaccounts?project=automlops-sandbox\n",
      "APIs: https://console.cloud.google.com/apis\n",
      "Cloud Source Repository: https://source.cloud.google.com/automlops-sandbox/dry-beans-dt-repository/+/automlops:\n",
      "Cloud Build Jobs: https://console.cloud.google.com/cloud-build/builds;region=us-central1\n",
      "Vertex AI Pipeline Runs: https://console.cloud.google.com/vertex-ai/pipelines/runs\n",
      "Cloud Build Trigger: https://console.cloud.google.com/cloud-build/triggers;region=us-central1\n",
      "Pipeline Job Submission Service (Cloud Functions): https://console.cloud.google.com/functions/details/us-central1/dry-beans-dt-job-submission-svc\n",
      "Pub/Sub Queueing Service Topic: https://console.cloud.google.com/cloudpubsub/topic/detail/dry-beans-dt-queueing-svc\n",
      "Pub/Sub Queueing Service Subscriptions: https://console.cloud.google.com/cloudpubsub/subscription/list\n",
      "Cloud Scheduler Job: https://console.cloud.google.com/cloudscheduler\n"
     ]
    }
   ],
   "source": [
    "AutoMLOps.deploy(precheck=True,                     # precheck is optional, defaults to True\n",
    "                 hide_warnings=False)               # hide_warnings is optional, defaults to True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bQohDTJgLQlW"
   },
   "source": [
    "## Interpret your results\n",
    "\n",
    "Vertex AI Model Monitoring detects an anomaly when the threshold set for a feature is exceeded. The following cells give you a sense of the alerting and reporting experience after model monitoring anomalies have been detected.\n",
    "\n",
    "Vertex AI Model Monitoring automatically notifies you of detected anomalies through email, but you can also [set up alerts through Cloud Logging](https://cloud.google.com/vertex-ai/docs/model-monitoring/using-model-monitoring#monitor-job)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uGPI92qbOFUR"
   },
   "source": [
    "### Here's what a sample email alert looks like...\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/mco-general/img/mm7.png\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HoaqsxpaRs1m"
   },
   "source": [
    "This email is warning you that the *cnt_level_start_quickplay*, *cnt_user_engagement*, and *country* feature values seen in production have skewed above your threshold between training and serving your model. It's also telling you that the *cnt_user_engagement* and *country* feature attribution values are skewed relative to your training data, again, as per your threshold specification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w4jVIVq4VzB_"
   },
   "source": [
    "### Monitoring results in the Cloud Console\n",
    "\n",
    "You can examine your model monitoring data from the Cloud Console. Below is a screenshot of those capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2OdIMVBAPZi_"
   },
   "source": [
    "#### Monitoring Status\n",
    "\n",
    "You can verify that a given endpoint has an active model monitoring job via the Endpoint summary page:\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/mco-general/img/mm1.png\" />\n",
    "\n",
    "#### Monitoring Alerts\n",
    "\n",
    "You can examine the alert details by clicking into the endpoint of interest, and selecting the alerts panel:\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/mco-general/img/mm2.png\" />\n",
    "\n",
    "#### Feature Value Distributions\n",
    "\n",
    "You can also examine the recorded training and production feature distributions by drilling down into a given feature, like this:\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/mco-general/img/mm9.png\" />\n",
    "\n",
    "which yields graphical representations of the feature distrubution during both training and production, like this:\n",
    "\n",
    "<img src=\"https://storage.googleapis.com/mco-general/img/mm8.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j3Dh15h3-NoO"
   },
   "source": [
    "## Learn more about model monitoring\n",
    "\n",
    "**Congratulations!** You've now learned what model monitoring is, how to configure and enable it, and how to find and interpret the results. Check out the following resources to learn more about model monitoring and MLOps.\n",
    "\n",
    "- [TensorFlow Data Validation](https://www.tensorflow.org/tfx/guide/tfdv)\n",
    "- [Data Understanding, Validation, and Monitoring At Scale](https://blog.tensorflow.org/2018/09/introducing-tensorflow-data-validation.html)\n",
    "- [Vertex Product Documentation](https://cloud.google.com/vertex-ai)\n",
    "- [Vertex AI Model Monitoring Reference Docs](https://cloud.google.com/vertex-ai/docs/reference)\n",
    "- [Vertex AI Model Monitoring blog article](https://cloud.google.com/blog/topics/developers-practitioners/monitor-models-training-serving-skew-vertex-ai)\n",
    "- [Explainable AI Whitepaper](https://storage.googleapis.com/cloud-ai-whitepapers/AI%20Explainability%20Whitepaper.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "model_monitoring.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
