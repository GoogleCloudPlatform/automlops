{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b45b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2023 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8790d7ed",
   "metadata": {},
   "source": [
    "# AutoMLOps - BQML Introduction Training Example\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/automlops/main/examples/training/03_bqml_introduction_training_example.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/automlops/blob/main/examples/training/03_bqml_introduction_training_example.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/automlops/main/examples/training/03_bqml_introduction_training_example.ipynb\">\n",
    "        <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "      Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</table>\n",
    "<br/><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f938540",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "In this tutorial, you will build a [Vertex AI](https://cloud.google.com/vertex-ai) pipeline, complete with an integrated CI/CD pipeline. This tutorial will walk you through how to use AutoMLOps to define, create and run pipelines for BQML training jobs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22881a6a",
   "metadata": {},
   "source": [
    "# Objective\n",
    "In this tutorial, you will learn how to create and run MLOps pipelines integrated with CI/CD. This tutorial goes through an example pipeline that uses BQML for model training and evaluation. The pipeline has the following steps:\n",
    "1. create_dataset: A custom component that will create an empty BQ dataset resource.\n",
    "2. train_model: A custom component that will train a DNN classifier on the training data.\n",
    "3. evaluate_model: A custom component that will evaluate the performance of the classifier.\n",
    "3. deploy_model: A custom component that will take the classifier and deploy it to an endpoint.\n",
    "\n",
    "# Prerequisites\n",
    "\n",
    "In order to use AutoMLOps, the following are required:\n",
    "\n",
    "- Python 3.7 - 3.10\n",
    "- [Google Cloud SDK 407.0.0](https://cloud.google.com/sdk/gcloud/reference)\n",
    "- [beta 2022.10.21](https://cloud.google.com/sdk/gcloud/reference/beta)\n",
    "- `git` installed\n",
    "- `git` logged-in:\n",
    "```\n",
    "  git config --global user.email \"you@example.com\"\n",
    "  git config --global user.name \"Your Name\"\n",
    "```\n",
    "- [Application Default Credentials (ADC)](https://cloud.google.com/docs/authentication/provide-credentials-adc) are setup. This can be done through the following commands:\n",
    "```\n",
    "gcloud auth application-default login\n",
    "gcloud config set account <account@example.com>\n",
    "```\n",
    "\n",
    "# APIs & IAM\n",
    "Based on the user options selection, AutoMLOps will enable up to the following APIs during the provision step:\n",
    "- [aiplatform.googleapis.com](https://cloud.google.com/vertex-ai/docs/reference/rest)\n",
    "- [artifactregistry.googleapis.com](https://cloud.google.com/artifact-registry/docs/reference/rest)\n",
    "- [cloudbuild.googleapis.com](https://cloud.google.com/build/docs/api/reference/rest)\n",
    "- [cloudfunctions.googleapis.com](https://cloud.google.com/functions/docs/reference/rest)\n",
    "- [cloudresourcemanager.googleapis.com](https://cloud.google.com/resource-manager/reference/rest)\n",
    "- [cloudscheduler.googleapis.com](https://cloud.google.com/scheduler/docs/reference/rest)\n",
    "- [cloudtasks.googleapis.com](https://cloud.google.com/tasks/docs/reference/rest)\n",
    "- [compute.googleapis.com](https://cloud.google.com/compute/docs/reference/rest/v1)\n",
    "- [iam.googleapis.com](https://cloud.google.com/iam/docs/reference/rest)\n",
    "- [iamcredentials.googleapis.com](https://cloud.google.com/iam/docs/reference/credentials/rest)\n",
    "- [ml.googleapis.com](https://cloud.google.com/ai-platform/training/docs/reference/rest)\n",
    "- [pubsub.googleapis.com](https://cloud.google.com/pubsub/docs/reference/rest)\n",
    "- [run.googleapis.com](https://cloud.google.com/run/docs/reference/rest)\n",
    "- [storage.googleapis.com](https://cloud.google.com/storage/docs/apis)\n",
    "- [sourcerepo.googleapis.com](https://cloud.google.com/source-repositories/docs/reference/rest)\n",
    "\n",
    "\n",
    "AutoMLOps will create the following service account and update [IAM permissions](https://cloud.google.com/iam/docs/understanding-roles) during the provision step:\n",
    "1. Pipeline Runner Service Account (defaults to: vertex-pipelines@PROJECT_ID.iam.gserviceaccount.com). Roles added:\n",
    "- roles/aiplatform.serviceAgent\n",
    "\n",
    "# User Guide\n",
    "\n",
    "For a user-guide, please view these [slides](../../AutoMLOps_User_Guide.pdf).\n",
    "\n",
    "# Costs\n",
    "\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "- Vertex AI\n",
    "- Artifact Registry\n",
    "- Cloud Storage\n",
    "- Cloud Source Repository\n",
    "- Cloud Build\n",
    "- Cloud Run\n",
    "- Cloud Scheduler\n",
    "- Cloud Pub/Sub\n",
    "\n",
    "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing), and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage.\n",
    "\n",
    "# Ground-rules for using AutoMLOps\n",
    "1. Do not use variables, functions, code, etc. not defined within the scope of a custom component. These custom components will become containers and will have no reference to the out of scope code.\n",
    "2. Import statements and helper functions must be added inside the function. Provide parameter type hints.\n",
    "3. Test each of your components for accuracy and correctness before running them using AutoMLOps. We cannot fix bugs automatically; bugs are much more difficult to fix once they are made into pipelines.\n",
    "4. If you are using Kubeflow, be sure to define all the requirements needed to run the custom component - it can be easy to leave out packages which will cause the container to fail when running within a pipeline. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12381413",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "The dataset used for this tutorial is the Penguins dataset from [BigQuery public datasets](https://cloud.google.com/bigquery/public-data). This version of the dataset is used to predict the species of penguins from the available features like culmen-length, flipper-depth etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a231b629",
   "metadata": {},
   "source": [
    "# Setup Git\n",
    "Set up your git configuration below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f90b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git config --global user.email 'you@example.com'\n",
    "!git config --global user.name 'Your Name'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e4d190",
   "metadata": {},
   "source": [
    "# Install AutoMLOps\n",
    "\n",
    "Install AutoMLOps from [PyPI](https://pypi.org/project/google-cloud-automlops/), or locally by cloning the repo and running `pip install .`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94451868",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install google-cloud-automlops --user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17db55d5",
   "metadata": {},
   "source": [
    "# Restart the kernel\n",
    "Once you've installed the AutoMLOps package, you need to restart the notebook kernel so it can find the package.\n",
    "\n",
    "**Note: Once this cell has finished running, continue on. You do not need to re-run any of the cells above.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57c53b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.getenv('IS_TESTING'):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250d511b",
   "metadata": {},
   "source": [
    "# Set your project ID\n",
    "Set your project ID below. If you don't know your project ID, leave the field blank and the following cells may be able to find it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "931ff517",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = '[your-project-id]'  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be0be295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project ID: automlops-sandbox\n"
     ]
    }
   ],
   "source": [
    "if PROJECT_ID == '' or PROJECT_ID is None or PROJECT_ID == '[your-project-id]':\n",
    "    # Get your GCP project id from gcloud\n",
    "    shell_output = !gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print('Project ID:', PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c36482d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "! gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e62029",
   "metadata": {},
   "source": [
    "Set your Model_ID below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb980fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = 'penguins-dnn'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715a8a22",
   "metadata": {},
   "source": [
    "# AutoMLOps Pipeline - Using Kubeflow components\n",
    "This workflow will generate a pipeline using Kubeflow spec.  AutoMLOps provides 2 functions for defining MLOps pipelines:\n",
    "\n",
    "- `AutoMLOps.component(...)`: Defines a component, which is a containerized python function.\n",
    "- `AutoMLOps.pipeline(...)`: Defines a pipeline, which is a series of components.\n",
    "\n",
    "AutoMLOps provides 5 functions for building and maintaining MLOps pipelines:\n",
    "\n",
    "- `AutoMLOps.generate(...)`: Generates the MLOps codebase. Users can specify the tooling and technologies they would like to use in their MLOps pipeline.\n",
    "- `AutoMLOps.provision(...)`: Runs provisioning scripts to create and maintain necessary infra for MLOps.\n",
    "- `AutoMLOps.deprovision(...)`: Runs deprovisioning scripts to tear down MLOps infra created using AutoMLOps.\n",
    "- `AutoMLOps.deploy(...)`: Builds and pushes component container, then triggers the pipeline job.\n",
    "- `AutoMLOps.launchAll(...)`: Runs `generate()`, `provision()`, and `deploy()` all in succession. \n",
    "\n",
    "Please see the [readme](https://github.com/GoogleCloudPlatform/automlops/blob/main/README.md) for more information.\n",
    "\n",
    "**Note: This workflow requires python packages `kfp<2.0.0` and `google-cloud-aiplatform`.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f82908",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install kfp<2.0.0 google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0856cbc0",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36329f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.v2 import dsl\n",
    "from kfp.v2.dsl import Artifact, Metrics, Model, Output\n",
    "from google_cloud_automlops import AutoMLOps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83fcec2",
   "metadata": {},
   "source": [
    "## Clear the cache\n",
    "`AutoMLOps.clear_cache` will remove previous instantiations of AutoMLOps components and pipelines. Use this function if you have previously defined a component that you no longer need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf82f09f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache cleared.\n"
     ]
    }
   ],
   "source": [
    "AutoMLOps.clear_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb34204",
   "metadata": {},
   "source": [
    "## Create Dataset\n",
    "Define a Kubeflow custom component for creating an empty dataset resource. You must specify the `output_component_file` with the name of your component. For `AutoMLOps` to know where to find the Kubeflow component spec, set this variable to the following string `f\"{AutoMLOps.OUTPUT_DIR}/your_component_name.yaml\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89b82a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=[\n",
    "        'google-cloud-bigquery'\n",
    "    ],\n",
    "    output_component_file=f'{AutoMLOps.OUTPUT_DIR}/create_dataset.yaml'\n",
    ")\n",
    "def create_dataset(\n",
    "    bq_table: str,\n",
    "    project_id: str\n",
    "):\n",
    "    \"\"\"Custom component that creates an empty dataset resource.\n",
    "\n",
    "    Args:\n",
    "        bq_table: The source biquery table.\n",
    "        project_id: The project ID.\n",
    "    \"\"\"\n",
    "    from google.cloud import bigquery\n",
    "    bq_client = bigquery.Client(project=project_id)\n",
    "    bq_data_name = bq_table.split('.')[-1]\n",
    "\n",
    "    dataset_query = f'''CREATE SCHEMA {bq_data_name}'''\n",
    "    job = bq_client.query(dataset_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03abc18d",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "Define a Kubeflow custom component for creating and training a BigQuery ML tabular classification model from the public dataset penguins and store the model in your project using the `CREATE MODEL` statement. The model configuration is specified in the `OPTIONS` statement as follows:\n",
    "\n",
    "- `model_type`: The type and archictecture of tabular model to train, e.g., DNN classification. Learn more about supported [model types](https://cloud.google.com/bigquery/docs/vertex-xai).\n",
    "- `labels`: The column which are the labels.\n",
    "\n",
    "Learn more about [The CREATE MODEL statement](https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-create)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f1a3660",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=[\n",
    "        'google-cloud-bigquery'\n",
    "    ],\n",
    "    output_component_file=f'{AutoMLOps.OUTPUT_DIR}/train_model.yaml'\n",
    ")\n",
    "def train_model(\n",
    "    bq_table: str,\n",
    "    labels: str,\n",
    "    model_name: str,\n",
    "    model_type: str,\n",
    "    project_id: str\n",
    "):\n",
    "    \"\"\"Custom component that trains a DNN classifier on the training data.\n",
    "\n",
    "    Args:\n",
    "        bq_table: Full uri of the BQ training data.\n",
    "        labels: The type and archictecture of tabular model to train, e.g., DNN classification.\n",
    "        model_name: Name for the model.\n",
    "        model_type: The column which are the labels.\n",
    "        project_id: Project id.\n",
    "    \"\"\"\n",
    "    from google.cloud import bigquery\n",
    "    bq_client = bigquery.Client(project=project_id)\n",
    "    bq_data_name = bq_table.split('.')[-1]\n",
    "\n",
    "    model_query = f'''\n",
    "    CREATE OR REPLACE MODEL `{bq_data_name}.{model_name}`\n",
    "    OPTIONS(\n",
    "        model_type='{model_type}',\n",
    "        labels = ['{labels}'],\n",
    "        model_registry='vertex_ai'\n",
    "        )\n",
    "    AS\n",
    "    SELECT *\n",
    "    FROM `{bq_table}`\n",
    "    '''\n",
    "\n",
    "    job = bq_client.query(model_query)\n",
    "    print(job.errors, job.state)\n",
    "\n",
    "    while job.running():\n",
    "        from time import sleep\n",
    "\n",
    "        sleep(30)\n",
    "        print('Running ...')\n",
    "    print(job.errors, job.state)\n",
    "\n",
    "    tblname = job.ddl_target_table\n",
    "    tblname = f'{tblname.dataset_id}.{tblname.table_id}'\n",
    "    print(f'{tblname} created in {job.ended - job.started}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d52e522-1ed1-4d9a-9a50-011c3f5cdce8",
   "metadata": {},
   "source": [
    "## Evaluate the trained BigQuery ML model\n",
    "\n",
    "Define a Kubeflow custom component to retrieve the model evaluation for the trained BigQuery ML model.\n",
    "\n",
    "Learn more about [The ML.EVALUATE function](https://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-syntax-evaluate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30ebca16-8569-4891-9d04-6f3eb9e1ac26",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=[\n",
    "        'google-cloud-bigquery', \n",
    "        'pandas',\n",
    "        'pyarrow',\n",
    "        'db_dtypes'\n",
    "    ],\n",
    "    output_component_file=f'{AutoMLOps.OUTPUT_DIR}/evaluate_model.yaml',\n",
    ")\n",
    "def evaluate_model(\n",
    "    bq_table: str,\n",
    "    metrics: Output[Metrics],\n",
    "    model_name: str,\n",
    "    project_id: str\n",
    "):\n",
    "    \"\"\"Custom component that evaluates the model.\n",
    "\n",
    "    Args:\n",
    "        model_name: Name for the model.\n",
    "        project_id: Project id.\n",
    "    \"\"\"\n",
    "    from google.cloud import bigquery\n",
    "    bq_client = bigquery.Client(project=project_id)\n",
    "    bq_data_name = bq_table.split('.')[-1]\n",
    "\n",
    "    eval_query = f'''\n",
    "    SELECT *\n",
    "    FROM\n",
    "      ML.EVALUATE(MODEL {bq_data_name}.{model_name})\n",
    "    ORDER BY  roc_auc desc\n",
    "    LIMIT 1'''\n",
    "\n",
    "    job = bq_client.query(eval_query)\n",
    "    results = job.result().to_dataframe()\n",
    "\n",
    "    for column in results:\n",
    "        metrics.log_metric(column, results[column].values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae46f9f",
   "metadata": {},
   "source": [
    "## Uploading & Deploying the Model\n",
    "Define a Kubeflow custom component for uploading and deploying a model in Vertex AI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa8047d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.component(\n",
    "    packages_to_install=[\n",
    "        'google-cloud-aiplatform'\n",
    "    ],\n",
    "    output_component_file=f'{AutoMLOps.OUTPUT_DIR}/deploy_model.yaml',\n",
    ")\n",
    "def deploy_model(\n",
    "    machine_type: str,\n",
    "    model_name: str,\n",
    "    project_id: str,\n",
    "    region: str,\n",
    "    vertex_endpoint: Output[Artifact],\n",
    "    vertex_model: Output[Model]\n",
    "):\n",
    "    \"\"\"Custom component that deploys the model.\n",
    "\n",
    "    Args:\n",
    "        model_name: Name for the model.\n",
    "        project_id: Project id.\n",
    "        region: Resource region.\n",
    "    \"\"\"\n",
    "    from google.cloud import aiplatform\n",
    "    aiplatform.init(project=project_id, location=region)\n",
    "    model = aiplatform.Model(model_name=model_name)\n",
    "\n",
    "    endpoint = model.deploy(\n",
    "        machine_type=machine_type,\n",
    "        deployed_model_display_name=f'deployed-{model_name}')\n",
    "    vertex_endpoint.uri = endpoint.resource_name\n",
    "    vertex_model.uri = endpoint.resource_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3602954f",
   "metadata": {},
   "source": [
    "## Define the Pipeline\n",
    "Define your pipeline. You can optionally give the pipeline a name and description. Define the structure by listing the components to be called in your pipeline; use `.after` to specify the order of execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01996d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@AutoMLOps.pipeline(name='bqml-automlops-pipeline', description='This is an example demo for BQML.')\n",
    "def pipeline(bq_table: str,\n",
    "             labels: str,\n",
    "             machine_type: str,\n",
    "             model_name: str,\n",
    "             model_type: str,\n",
    "             project_id: str,\n",
    "             region: str\n",
    "            ):\n",
    "\n",
    "    create_dataset_task = create_dataset(\n",
    "        bq_table=bq_table,\n",
    "        project_id=project_id)\n",
    "\n",
    "    train_model_task = train_model(\n",
    "        bq_table=bq_table,\n",
    "        labels=labels,\n",
    "        model_name=model_name,\n",
    "        model_type=model_type,\n",
    "        project_id=project_id).after(create_dataset_task)\n",
    "\n",
    "    evaluate_model_task = evaluate_model(\n",
    "        bq_table=bq_table,\n",
    "        model_name=model_name,\n",
    "        project_id=project_id).after(train_model_task)\n",
    "\n",
    "    deploy_model_task = deploy_model(\n",
    "        machine_type=machine_type,\n",
    "        model_name=model_name,\n",
    "        project_id=project_id,\n",
    "        region=region).after(evaluate_model_task)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe0b0f2",
   "metadata": {},
   "source": [
    "## Define the Pipeline Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc244ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_params = {\n",
    "    'bq_table': 'bigquery-public-data.ml_datasets.penguins',\n",
    "    'labels': 'species',\n",
    "    'machine_type': 'n1-standard-4',\n",
    "    'model_name': 'penguins_dnn',\n",
    "    'model_type': 'DNN_CLASSIFIER',\n",
    "    'project_id': PROJECT_ID,\n",
    "    'region': 'us-central1'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa976ef",
   "metadata": {},
   "source": [
    "## Generate and Run the pipeline\n",
    "`AutoMLOps.generate(...)` generates the MLOps codebase. Users can specify the tooling and technologies they would like to use in their MLOps pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af71f8f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing directories under AutoMLOps/\n",
      "Writing configurations to AutoMLOps/configs/defaults.yaml\n",
      "Writing Kubeflow Pipelines code to AutoMLOps/pipelines, AutoMLOps/components, AutoMLOps/services\n",
      "Writing README.md to AutoMLOps/README.md\n",
      "Writing scripts to AutoMLOps/scripts\n",
      "Writing CloudBuild config to AutoMLOps/cloudbuild.yaml\n",
      "Code Generation Complete.\n"
     ]
    }
   ],
   "source": [
    "AutoMLOps.generate(project_id=PROJECT_ID,\n",
    "                   pipeline_params=pipeline_params,\n",
    "                   use_ci=True,\n",
    "                   naming_prefix=MODEL_ID,\n",
    "                   schedule_pattern='59 11 * * 0' # retrain every Sunday at Midnight\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea6ab9e",
   "metadata": {},
   "source": [
    "`AutoMLOps.provision(...)` runs provisioning scripts to create and maintain necessary infra for MLOps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4589f126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Provisioning requires these permissions:\n",
      "-cloudfunctions.functions.get\n",
      "-serviceusage.services.use\n",
      "-serviceusage.services.enable\n",
      "-cloudfunctions.functions.create\n",
      "-pubsub.subscriptions.list\n",
      "-cloudscheduler.jobs.list\n",
      "-pubsub.topics.create\n",
      "-source.repos.list\n",
      "-artifactregistry.repositories.create\n",
      "-resourcemanager.projects.setIamPolicy\n",
      "-iam.serviceAccounts.listiam.serviceAccounts.create\n",
      "-pubsub.subscriptions.create\n",
      "-cloudscheduler.jobs.create\n",
      "-storage.buckets.create\n",
      "-source.repos.create\n",
      "-artifactregistry.repositories.list\n",
      "-cloudbuild.builds.create\n",
      "-cloudbuild.builds.list\n",
      "-pubsub.topics.list\n",
      "-storage.buckets.get\n",
      "\n",
      "You are currently using: srastatter@google.com. Please check your account permissions.\n",
      "The following are the recommended roles for provisioning:\n",
      "-roles/resourcemanager.projectIamAdmin\n",
      "-roles/cloudfunctions.admin\n",
      "-roles/artifactregistry.admin\n",
      "-roles/iam.serviceAccountAdmin\n",
      "-roles/serviceusage.serviceUsageAdmin\n",
      "-roles/aiplatform.serviceAgent\n",
      "-roles/cloudscheduler.admin\n",
      "-roles/pubsub.editor\n",
      "-roles/source.admin\n",
      "-roles/cloudbuild.builds.editor\n",
      "\n",
      "\u001b[0;32m Setting up API services in project automlops-sandbox \u001b[0m\n",
      "Operation \"operations/acat.p2-45373616427-990bb410-2998-4b37-b37f-09ed724e9519\" finished successfully.\n",
      "\u001b[0;32m Setting up Artifact Registry in project automlops-sandbox \u001b[0m\n",
      "Listing items under project automlops-sandbox, location us-central1.\n",
      "\n",
      "dry-beans-dt-artifact-registry  DOCKER  STANDARD_REPOSITORY  Artifact Registry dry-beans-dt-artifact-registry in us-central1.  us-central1          Google-managed key  2023-09-05T11:25:48  2023-09-05T14:47:39  3200.712\n",
      "Artifact Registry: dry-beans-dt-artifact-registry already exists in project automlops-sandbox\n",
      "\u001b[0;32m Setting up Storage Bucket in project automlops-sandbox \u001b[0m\n",
      "gs://automlops-sandbox-dry-beans-dt-bucket/\n",
      "GS Bucket: automlops-sandbox-dry-beans-dt-bucket already exists in project automlops-sandbox\n",
      "\u001b[0;32m Setting up Pipeline Job Runner Service Account in project automlops-sandbox \u001b[0m\n",
      "Pipeline Runner Service Account         vertex-pipelines@automlops-sandbox.iam.gserviceaccount.com           False\n",
      "Service Account: vertex-pipelines already exists in project automlops-sandbox\n",
      "\u001b[0;32m Setting up IAM roles for Pipeline Job Runner Service Account in project automlops-sandbox \u001b[0m\n",
      "\u001b[0;32m Setting up Cloud Source Repository in project automlops-sandbox \u001b[0m\n",
      "dry-beans-dt-repository  automlops-sandbox  https://source.developers.google.com/p/automlops-sandbox/r/dry-beans-dt-repository\n",
      "Cloud Source Repository: dry-beans-dt-repository already exists in project automlops-sandbox\n",
      "\u001b[0;32m Setting up Queueing Service in project automlops-sandbox \u001b[0m\n",
      "name: projects/automlops-sandbox/topics/dry-beans-dt-queueing-svc\n",
      "Pub/Sub Topic: dry-beans-dt-queueing-svc already exists in project automlops-sandbox\n",
      "\u001b[0;32m Deploying Cloud Functions: dry-beans-dt-job-submission-svc in project automlops-sandbox \u001b[0m\n",
      "Deploying function (may take a while - up to 2 minutes)...\n",
      "..\n",
      "For Cloud Build Logs, visit: https://console.cloud.google.com/cloud-build/builds;region=us-central1/0dcd7601-a440-4c7e-8ea9-28d1cf927991?project=45373616427\n",
      "........................................................................done.\n",
      "availableMemoryMb: 512\n",
      "buildId: 0dcd7601-a440-4c7e-8ea9-28d1cf927991\n",
      "buildName: projects/45373616427/locations/us-central1/builds/0dcd7601-a440-4c7e-8ea9-28d1cf927991\n",
      "dockerRegistry: CONTAINER_REGISTRY\n",
      "entryPoint: process_request\n",
      "eventTrigger:\n",
      "  eventType: google.pubsub.topic.publish\n",
      "  failurePolicy: {}\n",
      "  resource: projects/automlops-sandbox/topics/dry-beans-dt-queueing-svc\n",
      "  service: pubsub.googleapis.com\n",
      "ingressSettings: ALLOW_ALL\n",
      "labels:\n",
      "  deployment-tool: cli-gcloud\n",
      "maxInstances: 3000\n",
      "name: projects/automlops-sandbox/locations/us-central1/functions/dry-beans-dt-job-submission-svc\n",
      "runtime: python39\n",
      "serviceAccountEmail: vertex-pipelines@automlops-sandbox.iam.gserviceaccount.com\n",
      "sourceUploadUrl: https://storage.googleapis.com/uploads-961973632599.us-central1.cloudfunctions.appspot.com/a2ee896a-a9dc-41ee-a129-fc000fc59dc9.zip\n",
      "status: ACTIVE\n",
      "timeout: 540s\n",
      "updateTime: '2023-09-08T03:06:05.246Z'\n",
      "versionId: '2'\n",
      "\u001b[0;32m Setting up Cloud Build Trigger in project automlops-sandbox \u001b[0m\n",
      "name: dry-beans-dt-build-trigger\n",
      "Cloudbuild Trigger already exists in project automlops-sandbox for repo dry-beans-dt-repository\n",
      "\u001b[0;32m Setting up Cloud Scheduler Job in project automlops-sandbox \u001b[0m\n",
      "dry-beans-dt-schedule  us-central1  59 11 * * 0 (Etc/UTC)  Pub/Sub      ENABLED\n",
      "Cloud Scheduler Job: dry-beans-dt-schedule already exists in project automlops-sandbox\n"
     ]
    }
   ],
   "source": [
    "AutoMLOps.provision(hide_warnings=False)            # hide_warnings is optional, defaults to True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebedb777",
   "metadata": {},
   "source": [
    "`AutoMLOps.deploy(...)` builds and pushes component container, then triggers the pipeline job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbad714e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Running precheck for deploying requires these permissions:\n",
      "-artifactregistry.repositories.get\n",
      "-cloudbuild.builds.get\n",
      "-resourcemanager.projects.getIamPolicy\n",
      "-storage.buckets.update\n",
      "-serviceusage.services.get\n",
      "-cloudfunctions.functions.get\n",
      "-pubsub.topics.get\n",
      "-iam.serviceAccounts.get\n",
      "-source.repos.update\n",
      "-pubsub.subscriptions.get\n",
      "\n",
      "You are currently using: srastatter@google.com. Please check your account permissions.\n",
      "The following are the recommended roles for deploying with precheck:\n",
      "-roles/serviceusage.serviceUsageViewer\n",
      "-roles/iam.roleViewer\n",
      "-roles/pubsub.viewer\n",
      "-roles/storage.admin\n",
      "-roles/cloudbuild.builds.editor\n",
      "-roles/source.writer\n",
      "-roles/iam.serviceAccountUser\n",
      "-roles/cloudfunctions.viewer\n",
      "-roles/artifactregistry.reader\n",
      "\n",
      "Checking for required API services in project automlops-sandbox...\n",
      "Checking for Artifact Registry in project automlops-sandbox...\n",
      "Checking for Storage Bucket in project automlops-sandbox...\n",
      "Checking for Pipeline Runner Service Account in project automlops-sandbox...\n",
      "Checking for IAM roles on Pipeline Runner Service Account in project automlops-sandbox...\n",
      "Checking for Cloud Source Repo in project automlops-sandbox...\n",
      "Checking for Pub/Sub Topic in project automlops-sandbox...\n",
      "Checking for Pub/Sub Subscription in project automlops-sandbox...\n",
      "Checking for Cloud Functions Pipeline Job Submission Service in project automlops-sandbox...\n",
      "Checking for Cloud Build Trigger in project automlops-sandbox...\n",
      "Precheck successfully completed, continuing to deployment.\n",
      "\n",
      "[automlops 1fa5664] Run AutoMLOps\n",
      " 7 files changed, 34 insertions(+), 228 deletions(-)\n",
      " delete mode 100644 AutoMLOps/components/component_base/src/custom_train_model.py\n",
      " delete mode 100644 AutoMLOps/components/custom_train_model/component.yaml\n",
      "remote: Waiting for private key checker: 5/5 objects left        \n",
      "To https://source.developers.google.com/p/automlops-sandbox/r/dry-beans-dt-repository\n",
      "   70a23dd..1fa5664  automlops -> automlops\n",
      "Pushing code to automlops branch, triggering build...\n",
      "Cloud Build job running at: https://console.cloud.google.com/cloud-build/builds;region=us-central1\n",
      "Please wait for this build job to complete.\n",
      "\n",
      "#################################################################\n",
      "#                                                               #\n",
      "#                       RESOURCES MANIFEST                      #\n",
      "#---------------------------------------------------------------#\n",
      "#     Generated resources can be found at the following urls    #\n",
      "#                                                               #\n",
      "#################################################################\n",
      "\n",
      "Google Cloud Storage Bucket: https://console.cloud.google.com/storage/automlops-sandbox-dry-beans-dt-bucket\n",
      "Artifact Registry: https://console.cloud.google.com/artifacts/docker/automlops-sandbox/us-central1/dry-beans-dt-artifact-registry\n",
      "Service Accounts: https://console.cloud.google.com/iam-admin/serviceaccounts?project=automlops-sandbox\n",
      "APIs: https://console.cloud.google.com/apis\n",
      "Cloud Source Repository: https://source.cloud.google.com/automlops-sandbox/dry-beans-dt-repository/+/automlops:\n",
      "Cloud Build Jobs: https://console.cloud.google.com/cloud-build/builds;region=us-central1\n",
      "Vertex AI Pipeline Runs: https://console.cloud.google.com/vertex-ai/pipelines/runs\n",
      "Cloud Build Trigger: https://console.cloud.google.com/cloud-build/triggers;region=us-central1\n",
      "Pipeline Job Submission Service (Cloud Functions): https://console.cloud.google.com/functions/details/us-central1/dry-beans-dt-job-submission-svc\n",
      "Pub/Sub Queueing Service Topic: https://console.cloud.google.com/cloudpubsub/topic/detail/dry-beans-dt-queueing-svc\n",
      "Pub/Sub Queueing Service Subscriptions: https://console.cloud.google.com/cloudpubsub/subscription/list\n",
      "Cloud Scheduler Job: https://console.cloud.google.com/cloudscheduler\n"
     ]
    }
   ],
   "source": [
    "AutoMLOps.deploy(precheck=True,                     # precheck is optional, defaults to True\n",
    "                 hide_warnings=False)               # hide_warnings is optional, defaults to True"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m102",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m102"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
