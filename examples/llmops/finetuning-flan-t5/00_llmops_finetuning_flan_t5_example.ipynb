{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b45b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2023 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8790d7ed",
   "metadata": {},
   "source": [
    "# AutoMLOps - LLMOps Finetuning Flan T5 Example\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/automlops/blob/main/examples/training/05_llmops_finetuning_flan_t5_example.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Colab logo\"> Run in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/automlops/blob/main/examples/training/05_llmops_finetuning_flan_t5_example.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\">\n",
    "      View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/automlops/main/examples/training/05_llmops_finetuning_flan_t5_example.ipynb\">\n",
    "        <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\">\n",
    "      Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "</table>\n",
    "<br/><br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f938540",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "In this tutorial you'll learn how to finetune a PyTorch Flan-T5-Base model (stored in Hugging Face) for SAMSum dataset (summary of conversations in English). You'll also learn how to deploy the finetuned model to Vertex Endpoints.\n",
    "\n",
    "This tutorial will walk you through how to use AutoMLOps to define, create and run a MLOps pipeline around this finetuning and deployment. For finetuning the Flan-T5-Base model, we will use NVIDIA V100 GPUs.\n",
    "\n",
    "This tutorial is derived from this [open-source example](https://github.com/rafaelsf80/vertex-flant5base-summarization/tree/master)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22881a6a",
   "metadata": {},
   "source": [
    "# Objective\n",
    "In this tutorial, you will learn how to create and run MLOps pipelines integrated with CI/CD. This tutorial goes through finetuning a PyTorch Flan-T5-Base model using GPU accelerators; the pipeline goes through the following workflow:\n",
    "1. fintune_t5_model: A custom component that finetunes a Flan T5 base model. \n",
    "2. deploy_and_test_model: A custom component that takes a finetuned T5 model, uploads it to Vertex Model Registry, deploys it to an endpoint, and runs a test prediction. \n",
    "\n",
    "# Prerequisites\n",
    "\n",
    "In order to use AutoMLOps, the following are required:\n",
    "\n",
    "- Python 3.7 - 3.10\n",
    "- [Google Cloud SDK 407.0.0](https://cloud.google.com/sdk/gcloud/reference)\n",
    "- [beta 2022.10.21](https://cloud.google.com/sdk/gcloud/reference/beta)\n",
    "- `git` installed\n",
    "- `git` logged-in:\n",
    "```\n",
    "  git config --global user.email \"you@example.com\"\n",
    "  git config --global user.name \"Your Name\"\n",
    "```\n",
    "- [Application Default Credentials (ADC)](https://cloud.google.com/docs/authentication/provide-credentials-adc) are setup. This can be done through the following commands:\n",
    "```\n",
    "gcloud auth application-default login\n",
    "gcloud config set account <account@example.com>\n",
    "```\n",
    "\n",
    "# APIs & IAM\n",
    "Based on the user options selection, AutoMLOps will enable up to the following APIs during the provision step:\n",
    "- [aiplatform.googleapis.com](https://cloud.google.com/vertex-ai/docs/reference/rest)\n",
    "- [artifactregistry.googleapis.com](https://cloud.google.com/artifact-registry/docs/reference/rest)\n",
    "- [cloudbuild.googleapis.com](https://cloud.google.com/build/docs/api/reference/rest)\n",
    "- [cloudfunctions.googleapis.com](https://cloud.google.com/functions/docs/reference/rest)\n",
    "- [cloudresourcemanager.googleapis.com](https://cloud.google.com/resource-manager/reference/rest)\n",
    "- [cloudscheduler.googleapis.com](https://cloud.google.com/scheduler/docs/reference/rest)\n",
    "- [cloudtasks.googleapis.com](https://cloud.google.com/tasks/docs/reference/rest)\n",
    "- [compute.googleapis.com](https://cloud.google.com/compute/docs/reference/rest/v1)\n",
    "- [iam.googleapis.com](https://cloud.google.com/iam/docs/reference/rest)\n",
    "- [iamcredentials.googleapis.com](https://cloud.google.com/iam/docs/reference/credentials/rest)\n",
    "- [ml.googleapis.com](https://cloud.google.com/ai-platform/training/docs/reference/rest)\n",
    "- [pubsub.googleapis.com](https://cloud.google.com/pubsub/docs/reference/rest)\n",
    "- [run.googleapis.com](https://cloud.google.com/run/docs/reference/rest)\n",
    "- [storage.googleapis.com](https://cloud.google.com/storage/docs/apis)\n",
    "- [sourcerepo.googleapis.com](https://cloud.google.com/source-repositories/docs/reference/rest)\n",
    "\n",
    "\n",
    "AutoMLOps will create the following service account and update [IAM permissions](https://cloud.google.com/iam/docs/understanding-roles) during the provision step:\n",
    "1. Pipeline Runner Service Account (defaults to: vertex-pipelines@PROJECT_ID.iam.gserviceaccount.com). Roles added:\n",
    "- roles/aiplatform.serviceAgent\n",
    "\n",
    "# User Guide\n",
    "\n",
    "For a user-guide, please view these [slides](../../../AutoMLOps_User_Guide.pdf).\n",
    "\n",
    "# Costs\n",
    "\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "- Vertex AI\n",
    "- Artifact Registry\n",
    "- Cloud Storage\n",
    "- Cloud Source Repository\n",
    "- Cloud Build\n",
    "- Cloud Run\n",
    "- Cloud Scheduler\n",
    "- Cloud Pub/Sub\n",
    "\n",
    "Learn about [Vertex AI pricing](https://cloud.google.com/vertex-ai/pricing), and use the [Pricing Calculator](https://cloud.google.com/products/calculator/) to generate a cost estimate based on your projected usage.\n",
    "\n",
    "# Ground-rules for using AutoMLOps\n",
    "1. Do not use variables, functions, code, etc. not defined within the scope of a custom component. These custom components will become containers and will have no reference to the out of scope code.\n",
    "2. Import statements and helper functions must be added inside the function. Provide parameter type hints.\n",
    "3. Test each of your components for accuracy and correctness before running them using AutoMLOps. We cannot fix bugs automatically; bugs are much more difficult to fix once they are made into pipelines.\n",
    "4. If you are using Kubeflow, be sure to define all the requirements needed to run the custom component - it can be easy to leave out packages which will cause the container to fail when running within a pipeline. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12381413",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "For training data, we are using the [SAMSum dataset](https://huggingface.co/datasets/samsum) which contains about 16k messenger-like conversations with summaries. Conversations were created and written down by linguists fluent in English. Linguists were asked to create conversations similar to those they write on a daily basis, reflecting the proportion of topics of their real-life messenger convesations. The style and register are diversified - conversations could be informal, semi-formal or formal, they may contain slang words, emoticons and typos. Then, the conversations were annotated with summaries. It was assumed that summaries should be a concise brief of what people talked about in the conversation in third person. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a231b629",
   "metadata": {},
   "source": [
    "# Setup Git\n",
    "Set up your git configuration below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f90b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git config --global user.email 'you@example.com'\n",
    "!git config --global user.name 'Your Name'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e4d190",
   "metadata": {},
   "source": [
    "# Install AutoMLOps\n",
    "\n",
    "Install AutoMLOps from [PyPI](https://pypi.org/project/google-cloud-automlops/), or locally by cloning the repo and running `pip install .`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94451868",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip3 install google-cloud-automlops --user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17db55d5",
   "metadata": {},
   "source": [
    "# Restart the kernel\n",
    "Once you've installed the AutoMLOps package, you need to restart the notebook kernel so it can find the package.\n",
    "\n",
    "**Note: Once this cell has finished running, continue on. You do not need to re-run any of the cells above.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c53b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.getenv('IS_TESTING'):\n",
    "    # Automatically restart kernel after installs\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250d511b",
   "metadata": {},
   "source": [
    "# Set variables\n",
    "Set variables. If you don't know your project ID, leave the field blank and the following cells may be able to find it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "931ff517",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = '[your-project-id]'  # @param {type:\"string\"}\n",
    "\n",
    "BUCKET_NAME = 'automlops-sandbox-bucket'  # @param {type:\"string\"}\n",
    "BUCKET_URI = f'gs://{BUCKET_NAME}'\n",
    "PREFIX = 'flan_t5_model/'\n",
    "MODEL_DIR = BUCKET_URI + '/' + PREFIX\n",
    "AF_REGISTRY_NAME = 'vertex-mlops-af'   # Artifact Registry name\n",
    "REGION = 'us-central1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be0be295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project ID: automlops-sandbox\n"
     ]
    }
   ],
   "source": [
    "if PROJECT_ID == '' or PROJECT_ID is None or PROJECT_ID == '[your-project-id]':\n",
    "    # Get your GCP project id from gcloud\n",
    "    shell_output = !gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print('Project ID:', PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c36482d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\r\n"
     ]
    }
   ],
   "source": [
    "! gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90774b2a",
   "metadata": {},
   "source": [
    "Set your Model_ID below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ee3fdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = 'flan-t5-samsum'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba66e6f",
   "metadata": {},
   "source": [
    "Set service account and training/serving images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e55ef42",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_IMAGE = 'us-docker.pkg.dev/vertex-ai/training/pytorch-gpu.1-10:latest' # includes required cuda packages\n",
    "SERVING_IMAGE = f'{REGION}-docker.pkg.dev/{PROJECT_ID}/{AF_REGISTRY_NAME}/finetuning_flan_t5_base:latest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7796413",
   "metadata": {},
   "outputs": [],
   "source": [
    "SERVICE_ACCOUNT = f'vertex-pipelines@{PROJECT_ID}.iam.gserviceaccount.com'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c833c551",
   "metadata": {},
   "source": [
    "## Build the Custom Serving image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7504586",
   "metadata": {},
   "source": [
    "Create a custom serving image for running predictions using FastAPI. **Update [the server](serving/app/main.py) code with your bucket name and model_dir prefix from above.** Then build and push the custom serving image.\n",
    "\n",
    "The Artifact Registry resource AF_REGISTRY_NAME must exist prior to submitting this build job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf2530f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!gcloud builds submit serving --region $REGION --tag $SERVING_IMAGE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41dd830",
   "metadata": {},
   "source": [
    "## Create a Tensorboard instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45446482",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "343ecd63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Tensorboard\n",
      "Create Tensorboard backing LRO: projects/45373616427/locations/us-central1/tensorboards/7169299598215741440/operations/3844307922502811648\n",
      "Tensorboard created. Resource name: projects/45373616427/locations/us-central1/tensorboards/7169299598215741440\n",
      "To use this Tensorboard in another session:\n",
      "tb = aiplatform.Tensorboard('projects/45373616427/locations/us-central1/tensorboards/7169299598215741440')\n",
      "flan-t5-tensorboard\n",
      "projects/45373616427/locations/us-central1/tensorboards/7169299598215741440\n"
     ]
    }
   ],
   "source": [
    "aiplatform.init(project=PROJECT_ID, location=REGION)\n",
    "\n",
    "tensorboard = aiplatform.Tensorboard.create(\n",
    "    display_name='flan-t5-tensorboard',\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    ")\n",
    "\n",
    "print(tensorboard.display_name)\n",
    "print(tensorboard.resource_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ecec8ba",
   "metadata": {},
   "source": [
    "# 1. AutoMLOps Pipeline\n",
    "This workflow will define and generate a pipeline using AutoMLOps. AutoMLOps provides 2 functions for defining MLOps pipelines:\n",
    "\n",
    "- `AutoMLOps.component(...)`: Defines a component, which is a containerized python function.\n",
    "- `AutoMLOps.pipeline(...)`: Defines a pipeline, which is a series of components.\n",
    "\n",
    "AutoMLOps provides 5 functions for building and maintaining MLOps pipelines:\n",
    "\n",
    "- `AutoMLOps.generate(...)`: Generates the MLOps codebase. Users can specify the tooling and technologies they would like to use in their MLOps pipeline.\n",
    "- `AutoMLOps.provision(...)`: Runs provisioning scripts to create and maintain necessary infra for MLOps.\n",
    "- `AutoMLOps.deprovision(...)`: Runs deprovisioning scripts to tear down MLOps infra created using AutoMLOps.\n",
    "- `AutoMLOps.deploy(...)`: Builds and pushes component container, then triggers the pipeline job.\n",
    "- `AutoMLOps.launchAll(...)`: Runs `generate()`, `provision()`, and `deploy()` all in succession. \n",
    "\n",
    "Please see the [readme](https://github.com/GoogleCloudPlatform/automlops/blob/main/README.md) for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55db8c4",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "781e2b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google_cloud_automlops import AutoMLOps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4aef7e6",
   "metadata": {},
   "source": [
    "## Clear the cache\n",
    "Remove previous instantiations of AutoMLOps components and pipelines left over from other runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce8163a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache cleared.\n"
     ]
    }
   ],
   "source": [
    "AutoMLOps.clear_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51d294e",
   "metadata": {},
   "source": [
    "## Finetuning Flan-T5-Base Model\n",
    "Define a Kubeflow custom component for finetuning the [Flan-T5-Base model](https://huggingface.co/google/flan-t5-base). Import statements and helper functions must be added inside the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e75ae116",
   "metadata": {},
   "outputs": [],
   "source": [
    "@AutoMLOps.component(\n",
    "    packages_to_install=[\n",
    "        'accelerate==0.20.1',\n",
    "        'py7zr==0.20.4',\n",
    "        'nltk==3.7',\n",
    "        'evaluate==0.4.0',\n",
    "        'rouge_score==0.1.2', \n",
    "        'transformers==4.30.0',\n",
    "        'tensorboard==2.11.2',\n",
    "        'datasets==2.9.0',\n",
    "        'google-cloud-storage==2.7.0'\n",
    "    ]\n",
    ")\n",
    "def finetune_t5_model(\n",
    "    model_dir: str,\n",
    "    epochs: int,\n",
    "    eval_batch: int,\n",
    "    logging_steps: int,\n",
    "    lr: float,\n",
    "    train_batch: int\n",
    "):\n",
    "    \"\"\"Custom component that finetunes a Flan T5 base model.\n",
    "\n",
    "    Args:\n",
    "        model_dir: GCS directory to save the model and training artifacts.\n",
    "        epochs: Total number of training epochs to perform.\n",
    "        eval_batch: The batch size per GPU/TPU core/CPU for evaluation.\n",
    "        logging_steps: Number of update steps between two logs.\n",
    "        lr: The initial learning rate for AdamW optimizer.\n",
    "        train_batch: The batch size per GPU/TPU core/CPU for training.\n",
    "    \"\"\"\n",
    "    import glob\n",
    "    import logging\n",
    "    import os\n",
    "\n",
    "    from google.cloud import storage\n",
    "\n",
    "    from datasets import concatenate_datasets, load_dataset\n",
    "    from huggingface_hub import HfFolder\n",
    "    from transformers import (\n",
    "        AutoTokenizer,\n",
    "        AutoModelForSeq2SeqLM,\n",
    "        DataCollatorForSeq2Seq,\n",
    "        Seq2SeqTrainer,\n",
    "        Seq2SeqTrainingArguments\n",
    "    )\n",
    "    from transformers.integrations import TensorBoardCallback\n",
    "    import evaluate\n",
    "    import nltk\n",
    "    import numpy as np\n",
    "    from nltk.tokenize import sent_tokenize\n",
    "\n",
    "    MODEL_ID='google/flan-t5-base'\n",
    "    DATASET_ID = 'samsum'\n",
    "\n",
    "    def preprocess_function(sample, padding='max_length'):\n",
    "        # add prefix to the input for t5\n",
    "        inputs = ['summarize: ' + item for item in sample['dialogue']]\n",
    "\n",
    "        # tokenize inputs\n",
    "        model_inputs = tokenizer(inputs, max_length=max_source_length, padding=padding, truncation=True)\n",
    "\n",
    "        # Tokenize targets with the `text_target` keyword argument\n",
    "        labels = tokenizer(text_target=sample['summary'], max_length=max_target_length, padding=padding, truncation=True)\n",
    "\n",
    "        # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n",
    "        # padding in the loss.\n",
    "        if padding == 'max_length':\n",
    "            labels['input_ids'] = [\n",
    "                [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels['input_ids']\n",
    "            ]\n",
    "\n",
    "        model_inputs['labels'] = labels['input_ids']\n",
    "        return model_inputs\n",
    "\n",
    "    # helper function to postprocess text\n",
    "    def postprocess_text(preds, labels):\n",
    "        preds = [pred.strip() for pred in preds]\n",
    "        labels = [label.strip() for label in labels]\n",
    "\n",
    "        # rougeLSum expects newline after each sentence\n",
    "        preds = ['\\n'.join(sent_tokenize(pred)) for pred in preds]\n",
    "        labels = ['\\n'.join(sent_tokenize(label)) for label in labels]\n",
    "\n",
    "        return preds, labels\n",
    "\n",
    "    def compute_metrics(eval_preds):\n",
    "        preds, labels = eval_preds\n",
    "        if isinstance(preds, tuple):\n",
    "            preds = preds[0]\n",
    "        decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "        # Replace -100 in the labels as we can't decode them.\n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "        # Some simple post-processing\n",
    "        decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "        result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "        result = {k: round(v * 100, 4) for k, v in result.items()}\n",
    "        prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "        result['gen_len'] = np.mean(prediction_lens)\n",
    "        return result\n",
    "\n",
    "    def upload_to_gcs(local_directory_path: str, gs_directory_path: str):\n",
    "        client = storage.Client()\n",
    "\n",
    "        # extract GCS bucket_name\n",
    "        bucket_name = gs_directory_path.split('/')[2] # without gs://\n",
    "        # extract GCS object_name\n",
    "        object_name = '/'.join(gs_directory_path.split('/')[3:])\n",
    "\n",
    "        rel_paths = glob.glob(local_directory_path + '/**', recursive=True)\n",
    "        bucket = client.get_bucket(bucket_name)\n",
    "        for local_file in rel_paths:\n",
    "            remote_path = f'''{object_name}{'/'.join(local_file.split(os.sep)[1:])}'''\n",
    "            logging.info(remote_path)\n",
    "            if os.path.isfile(local_file):\n",
    "                blob = bucket.blob(remote_path)\n",
    "                blob.upload_from_filename(local_file)\n",
    "\n",
    "    # Load dataset\n",
    "    dataset = load_dataset(DATASET_ID)\n",
    "    # Load tokenizer of FLAN-t5-base\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "    # load model from the hub\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_ID)\n",
    "    \n",
    "    nltk.download('punkt')\n",
    "    # Metric\n",
    "    metric = evaluate.load('rouge')\n",
    "\n",
    "    # Hugging Face repository id\n",
    "    repository_id = f'''{MODEL_ID.split('/')[1]}-{DATASET_ID}'''\n",
    "\n",
    "    # The maximum total input sequence length after tokenization.\n",
    "    # Sequences longer than this will be truncated, sequences shorter will be padded.\n",
    "    tokenized_inputs = concatenate_datasets([dataset['train'],\n",
    "                                             dataset['test']]).map(lambda x: tokenizer(x['dialogue'],truncation=True),\n",
    "                                                                   batched=True, remove_columns=['dialogue', 'summary'])\n",
    "    max_source_length = max([len(x) for x in tokenized_inputs['input_ids']])\n",
    "    print(f'Max source length: {max_source_length}')\n",
    "\n",
    "    # The maximum total sequence length for target text after tokenization.\n",
    "    # Sequences longer than this will be truncated, sequences shorter will be padded.\"\n",
    "    tokenized_targets = concatenate_datasets([dataset['train'],\n",
    "                                              dataset['test']]).map(lambda x: tokenizer(x['summary'], truncation=True),\n",
    "                                                                    batched=True, remove_columns=['dialogue', 'summary'])\n",
    "    max_target_length = max([len(x) for x in tokenized_targets['input_ids']])\n",
    "    print(f'Max target length: {max_target_length}')\n",
    "\n",
    "    tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=['dialogue', 'summary', 'id'])\n",
    "    print(f'''Keys of tokenized dataset: {list(tokenized_dataset['train'].features)}''')\n",
    "\n",
    "    # we want to ignore tokenizer pad token in the loss\n",
    "    label_pad_token_id = -100\n",
    "    # Data collator\n",
    "    data_collator = DataCollatorForSeq2Seq(\n",
    "        tokenizer,\n",
    "        model=model,\n",
    "        label_pad_token_id=label_pad_token_id,\n",
    "        pad_to_multiple_of=8\n",
    "    )\n",
    "\n",
    "    # Define training args\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=repository_id,\n",
    "        per_device_train_batch_size=train_batch,\n",
    "        per_device_eval_batch_size=eval_batch,\n",
    "        predict_with_generate=True,\n",
    "        fp16=False, # Overflows with fp16\n",
    "        learning_rate=lr,\n",
    "        num_train_epochs=epochs,\n",
    "        # logging & evaluation strategies\n",
    "        logging_dir=os.environ['AIP_TENSORBOARD_LOG_DIR'],\n",
    "        #logging_dir=f'{repository_id}/logs',\n",
    "        logging_strategy='steps',\n",
    "        logging_steps=logging_steps,\n",
    "        evaluation_strategy='epoch',\n",
    "        save_strategy='epoch',\n",
    "        save_total_limit=2,\n",
    "        load_best_model_at_end=True,\n",
    "        # metric_for_best_model=\"overall_f1\",\n",
    "        # push to hub parameters\n",
    "        report_to='tensorboard',\n",
    "        push_to_hub=False,\n",
    "        hub_strategy='every_save',\n",
    "        hub_model_id=repository_id,\n",
    "        hub_token=HfFolder.get_token(),\n",
    "    )\n",
    "\n",
    "    # Create Trainer instance\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_dataset['train'],\n",
    "        eval_dataset=tokenized_dataset['test'],\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[TensorBoardCallback()]\n",
    "    )\n",
    "\n",
    "    # Start training\n",
    "    logging.info('Training ....')\n",
    "    trainer.train()\n",
    "    trainer.evaluate()\n",
    "\n",
    "    # Save tokenizer and model locally\n",
    "    tokenizer.save_pretrained(f'model_tokenizer')\n",
    "    trainer.save_model(f'model_output')\n",
    "\n",
    "    logging.info('Saving model and tokenizer to GCS ....')\n",
    "\n",
    "    # Upload model to GCS\n",
    "    upload_to_gcs('model_output', model_dir)\n",
    "    # Upload tokenizer to GCS\n",
    "    upload_to_gcs('model_tokenizer', model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a690f88f",
   "metadata": {},
   "source": [
    "## Uploading, Deploying, & Testing the Model\n",
    "Define a custom component for uploading and deploying a model in Vertex AI. Test the predictions of the model.\n",
    "Import statements and helper functions must be added inside the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39c4d7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@AutoMLOps.component(\n",
    "    packages_to_install=[\n",
    "        'datasets==2.9.0',\n",
    "        'google-cloud-aiplatform==1.26.0'\n",
    "    ]\n",
    ")\n",
    "def deploy_and_test_model(\n",
    "    endpoint_sa: str,\n",
    "    project_id: str,\n",
    "    region: str,\n",
    "    serving_image_tag: str\n",
    "):\n",
    "    \"\"\"Custom component that uploads a finetuned Flan-T5 from GCS to Vertex Model Registry,\n",
    "       deploys the model to an endpoint for online prediction, and runs a prediction test.\n",
    "\n",
    "    Args:\n",
    "        endpoint_sa: Service account to run the endpoint prediction service with.\n",
    "        project_id: Project_id.\n",
    "        region: Region.\n",
    "        serving_image_tag: Custom serving image uri.\n",
    "    \"\"\"\n",
    "    import pprint as pp\n",
    "    from random import randrange\n",
    "\n",
    "    from google.cloud import aiplatform\n",
    "\n",
    "    from datasets import load_dataset\n",
    "\n",
    "    DATASET_ID = 'samsum'\n",
    "\n",
    "    aiplatform.init(project=project_id, location=region)\n",
    "    # Check if model exists\n",
    "    models = aiplatform.Model.list()\n",
    "    model_name = 'finetuned-flan-t5'\n",
    "    if 'finetuned-flan-t5' in (m.name for m in models):\n",
    "        parent_model = model_name\n",
    "        model_id = None\n",
    "        is_default_version=False\n",
    "        version_aliases=['experimental', 'finetuned', 'flan-t5']\n",
    "        version_description='experimental version'\n",
    "    else:\n",
    "        parent_model = None\n",
    "        model_id = model_name\n",
    "        is_default_version=True\n",
    "        version_aliases=['live', 'finetuned', 'flan-t5']\n",
    "        version_description='live version'\n",
    "\n",
    "    uploaded_model = aiplatform.Model.upload(\n",
    "        model_id=model_id,\n",
    "        display_name=model_name,\n",
    "        parent_model=parent_model,\n",
    "        is_default_version=is_default_version,\n",
    "        version_aliases=version_aliases,\n",
    "        version_description=version_description,\n",
    "        serving_container_image_uri=serving_image_tag,\n",
    "        serving_container_predict_route='/predict',\n",
    "        serving_container_health_route='/health',\n",
    "        serving_container_ports=[8080],\n",
    "        labels={'created_by': 'automlops-team'},\n",
    "    )\n",
    "\n",
    "    endpoint = uploaded_model.deploy(\n",
    "        machine_type='n1-standard-8',\n",
    "        min_replica_count=1,\n",
    "        max_replica_count=1,\n",
    "        accelerator_type='NVIDIA_TESLA_V100',    \n",
    "        accelerator_count=1,\n",
    "        service_account=endpoint_sa, # This SA needs gcs permissions\n",
    "        sync=True\n",
    "    )\n",
    "\n",
    "    # Load dataset from the hub\n",
    "    dataset = load_dataset(DATASET_ID)\n",
    "    # select a random test sample\n",
    "    sample = dataset['test'][randrange(len(dataset[\"test\"]))]\n",
    "\n",
    "    # Test predictions\n",
    "    print('running prediction test...')\n",
    "    try:\n",
    "        resp = endpoint.predict([[sample['dialogue']]])\n",
    "        print(sample['dialogue'])\n",
    "        pp.pprint(resp)\n",
    "    except Exception as ex:\n",
    "        print('prediction request failed', ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3602954f",
   "metadata": {},
   "source": [
    "## Define the Pipeline\n",
    "Define your pipeline. You can optionally give the pipeline a name and description. Define the structure by listing the components to be called in your pipeline; use `.after` to specify the order of execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "01996d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@AutoMLOps.pipeline(name='finetune-flan-t5-pipeline')\n",
    "def pipeline(\n",
    "    endpoint_sa: str,\n",
    "    project_id: str,\n",
    "    eval_batch: int,\n",
    "    train_batch: int,\n",
    "    model_dir: str,\n",
    "    lr: float,\n",
    "    epochs: int,\n",
    "    logging_steps: int,\n",
    "    serving_image_tag: str,\n",
    "    region: str):\n",
    "\n",
    "    finetune_t5_model_task = finetune_t5_model(\n",
    "        model_dir=model_dir,\n",
    "        epochs=epochs,\n",
    "        eval_batch=eval_batch,\n",
    "        lr=lr,\n",
    "        logging_steps=logging_steps,\n",
    "        train_batch=train_batch)\n",
    "\n",
    "    deploy_and_test_model_task = deploy_and_test_model(\n",
    "        endpoint_sa=endpoint_sa,\n",
    "        project_id=project_id,\n",
    "        region=region,\n",
    "        serving_image_tag=serving_image_tag).after(finetune_t5_model_task)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe0b0f2",
   "metadata": {},
   "source": [
    "## Define the Pipeline Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc244ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_params = {\n",
    "    'endpoint_sa': SERVICE_ACCOUNT,\n",
    "    'project_id': PROJECT_ID,\n",
    "    'model_dir': MODEL_DIR,\n",
    "    'lr': 5e-5,\n",
    "    'epochs': 5,\n",
    "    'logging_steps': 500,\n",
    "    'serving_image_tag': SERVING_IMAGE,\n",
    "    'eval_batch': 4,\n",
    "    'region': 'us-central1',\n",
    "    'train_batch': 4,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0c815c",
   "metadata": {},
   "source": [
    "## Generate and Run the pipeline\n",
    "`AutoMLOps.go` generates the code and runs the pipeline. In this case, we are specifying a custom job spec, where we will use Nvidia V100 GPUs to accelerate the finetuning of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d3162f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing directories under AutoMLOps/\n",
      "Writing configurations to AutoMLOps/configs/defaults.yaml\n",
      "Writing README.md to AutoMLOps/README.md\n",
      "Writing kubeflow pipelines code to AutoMLOps/pipelines, AutoMLOps/components\n",
      "Writing scripts to AutoMLOps/scripts\n",
      "Writing submission service code to AutoMLOps/services\n",
      "Writing gcloud provisioning code to AutoMLOps/provision\n",
      "Writing cloud build config to AutoMLOps/cloudbuild.yaml\n",
      "Code Generation Complete.\n"
     ]
    }
   ],
   "source": [
    "AutoMLOps.generate(project_id=PROJECT_ID, \n",
    "                   pipeline_params=pipeline_params, \n",
    "                   use_ci=True, \n",
    "                   schedule_pattern='59 11 * * 0', # retrain every Sunday at Midnight\n",
    "                   naming_prefix=MODEL_ID,\n",
    "                   base_image=TRAINING_IMAGE,                   \n",
    "                   custom_training_job_specs = [{\n",
    "                    'component_spec': 'finetune_t5_model',\n",
    "                    'display_name': 'flan-t5-base-finetuning-gpu-tensorboard',\n",
    "                    'machine_type': 'n1-standard-32',\n",
    "                    'accelerator_type': 'NVIDIA_TESLA_V100',\n",
    "                    'accelerator_count': 4,\n",
    "                    'replica_count': 1,\n",
    "                    'service_account': SERVICE_ACCOUNT,\n",
    "                    'tensorboard': tensorboard.resource_name,\n",
    "                    'base_output_directory': f'{BUCKET_URI}/finetune_t5_model/'\n",
    "                   }]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d26d90fe",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Provisioning requires these permissions:\n",
      "-resourcemanager.projects.setIamPolicy\n",
      "-pubsub.topics.list\n",
      "-serviceusage.services.enable\n",
      "-pubsub.subscriptions.list\n",
      "-pubsub.subscriptions.create\n",
      "-cloudfunctions.functions.get\n",
      "-cloudscheduler.jobs.list\n",
      "-source.repos.list\n",
      "-iam.serviceAccounts.listiam.serviceAccounts.create\n",
      "-artifactregistry.repositories.list\n",
      "-artifactregistry.repositories.create\n",
      "-storage.buckets.get\n",
      "-storage.buckets.create\n",
      "-source.repos.create\n",
      "-cloudfunctions.functions.create\n",
      "-cloudbuild.builds.create\n",
      "-serviceusage.services.use\n",
      "-pubsub.topics.create\n",
      "-cloudbuild.builds.list\n",
      "-cloudscheduler.jobs.create\n",
      "\n",
      "You are currently using: srastatter@google.com. Please check your account permissions.\n",
      "The following are the recommended roles for provisioning:\n",
      "-roles/pubsub.editor\n",
      "-roles/resourcemanager.projectIamAdmin\n",
      "-roles/serviceusage.serviceUsageAdmin\n",
      "-roles/iam.serviceAccountAdmin\n",
      "-roles/source.admin\n",
      "-roles/cloudfunctions.admin\n",
      "-roles/cloudbuild.builds.editor\n",
      "-roles/artifactregistry.admin\n",
      "-roles/cloudscheduler.admin\n",
      "-roles/aiplatform.serviceAgent\n",
      "\n",
      "\u001b[0;32m Setting up API services in project automlops-sandbox \u001b[0m\n",
      "Operation \"operations/acat.p2-45373616427-52418ef9-25d9-44eb-8afb-80a3ca19640d\" finished successfully.\n",
      "\u001b[0;32m Setting up Artifact Registry in project automlops-sandbox \u001b[0m\n",
      "Listing items under project automlops-sandbox, location us-central1.\n",
      "\n",
      "flan-t5-samsum-artifact-registry  DOCKER  STANDARD_REPOSITORY  Artifact Registry flan-t5-samsum-artifact-registry in us-central1.  us-central1          Google-managed key  2023-09-18T11:05:47  2023-09-18T12:46:01  8226.709\n",
      "Artifact Registry: flan-t5-samsum-artifact-registry already exists in project automlops-sandbox\n",
      "\u001b[0;32m Setting up Storage Bucket in project automlops-sandbox \u001b[0m\n",
      "gs://automlops-sandbox-flan-t5-samsum-bucket/\n",
      "GS Bucket: automlops-sandbox-flan-t5-samsum-bucket already exists in project automlops-sandbox\n",
      "\u001b[0;32m Setting up Pipeline Job Runner Service Account in project automlops-sandbox \u001b[0m\n",
      "Pipeline Runner Service Account         vertex-pipelines@automlops-sandbox.iam.gserviceaccount.com           False\n",
      "Service Account: vertex-pipelines already exists in project automlops-sandbox\n",
      "\u001b[0;32m Setting up IAM roles for Pipeline Job Runner Service Account in project automlops-sandbox \u001b[0m\n",
      "\u001b[0;32m Setting up Cloud Source Repository in project automlops-sandbox \u001b[0m\n",
      "flan-t5-samsum-repository  automlops-sandbox  https://source.developers.google.com/p/automlops-sandbox/r/flan-t5-samsum-repository\n",
      "Cloud Source Repository: flan-t5-samsum-repository already exists in project automlops-sandbox\n",
      "\u001b[0;32m Setting up Queueing Service in project automlops-sandbox \u001b[0m\n",
      "name: projects/automlops-sandbox/topics/flan-t5-samsum-queueing-svc\n",
      "Pub/Sub Topic: flan-t5-samsum-queueing-svc already exists in project automlops-sandbox\n",
      "\u001b[0;32m Deploying Cloud Functions: flan-t5-samsum-job-submission-svc in project automlops-sandbox \u001b[0m\n",
      "Deploying function (may take a while - up to 2 minutes)...\n",
      "..\n",
      "For Cloud Build Logs, visit: https://console.cloud.google.com/cloud-build/builds;region=us-central1/f3249516-f66b-416f-b39b-4a13c74698e8?project=45373616427\n",
      ".................................................................done.\n",
      "availableMemoryMb: 512\n",
      "buildId: f3249516-f66b-416f-b39b-4a13c74698e8\n",
      "buildName: projects/45373616427/locations/us-central1/builds/f3249516-f66b-416f-b39b-4a13c74698e8\n",
      "dockerRegistry: CONTAINER_REGISTRY\n",
      "entryPoint: process_request\n",
      "eventTrigger:\n",
      "  eventType: google.pubsub.topic.publish\n",
      "  failurePolicy: {}\n",
      "  resource: projects/automlops-sandbox/topics/flan-t5-samsum-queueing-svc\n",
      "  service: pubsub.googleapis.com\n",
      "ingressSettings: ALLOW_ALL\n",
      "labels:\n",
      "  deployment-tool: cli-gcloud\n",
      "maxInstances: 3000\n",
      "name: projects/automlops-sandbox/locations/us-central1/functions/flan-t5-samsum-job-submission-svc\n",
      "runtime: python39\n",
      "serviceAccountEmail: vertex-pipelines@automlops-sandbox.iam.gserviceaccount.com\n",
      "sourceUploadUrl: https://storage.googleapis.com/uploads-961973632599.us-central1.cloudfunctions.appspot.com/7deec24d-6f1f-4ef3-991e-5058eec4b776.zip\n",
      "status: ACTIVE\n",
      "timeout: 540s\n",
      "updateTime: '2023-09-18T17:15:39.455Z'\n",
      "versionId: '4'\n",
      "\u001b[0;32m Setting up Cloud Build Trigger in project automlops-sandbox \u001b[0m\n",
      "name: flan-t5-samsum-build-trigger\n",
      "Cloudbuild Trigger already exists in project automlops-sandbox for repo flan-t5-samsum-repository\n",
      "\u001b[0;32m Setting up Cloud Scheduler Job in project automlops-sandbox \u001b[0m\n",
      "flan-t5-samsum-schedule  us-central1  59 11 * * 0 (Etc/UTC)  Pub/Sub      ENABLED\n",
      "Cloud Scheduler Job: flan-t5-samsum-schedule already exists in project automlops-sandbox\n"
     ]
    }
   ],
   "source": [
    "AutoMLOps.provision(hide_warnings=False)            # hide_warnings is optional, defaults to True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6352dd34",
   "metadata": {},
   "source": [
    "`AutoMLOps.deploy(...)` builds and pushes component container, then triggers the pipeline job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "17ceb009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Running precheck for deploying requires these permissions:\n",
      "-artifactregistry.repositories.get\n",
      "-pubsub.topics.get\n",
      "-resourcemanager.projects.getIamPolicy\n",
      "-cloudfunctions.functions.get\n",
      "-storage.buckets.update\n",
      "-source.repos.update\n",
      "-cloudbuild.builds.get\n",
      "-serviceusage.services.get\n",
      "-iam.serviceAccounts.get\n",
      "-pubsub.subscriptions.get\n",
      "\n",
      "You are currently using: srastatter@google.com. Please check your account permissions.\n",
      "The following are the recommended roles for deploying with precheck:\n",
      "-roles/iam.roleViewer\n",
      "-roles/iam.serviceAccountUser\n",
      "-roles/serviceusage.serviceUsageViewer\n",
      "-roles/pubsub.viewer\n",
      "-roles/storage.admin\n",
      "-roles/source.writer\n",
      "-roles/artifactregistry.reader\n",
      "-roles/cloudbuild.builds.editor\n",
      "-roles/cloudfunctions.viewer\n",
      "\n",
      "Checking for required API services in project automlops-sandbox...\n",
      "Checking for Artifact Registry in project automlops-sandbox...\n",
      "Checking for Storage Bucket in project automlops-sandbox...\n",
      "Checking for Pipeline Runner Service Account in project automlops-sandbox...\n",
      "Checking for IAM roles on Pipeline Runner Service Account in project automlops-sandbox...\n",
      "Checking for Cloud Source Repo in project automlops-sandbox...\n",
      "Checking for Pub/Sub Topic in project automlops-sandbox...\n",
      "Checking for Pub/Sub Subscription in project automlops-sandbox...\n",
      "Checking for Cloud Functions Pipeline Job Submission Service in project automlops-sandbox...\n",
      "Checking for Cloud Build Trigger in project automlops-sandbox...\n",
      "Precheck successfully completed, continuing to deployment.\n",
      "\n",
      "[automlops d4ac2e3] Run AutoMLOps\n",
      " 3 files changed, 15 insertions(+), 14 deletions(-)\n",
      "remote: Waiting for private key checker: 3/3 objects left        \n",
      "To https://source.developers.google.com/p/automlops-sandbox/r/flan-t5-samsum-repository\n",
      "   4788c2d..d4ac2e3  automlops -> automlops\n",
      "Pushing code to automlops branch, triggering build...\n",
      "Cloud Build job running at: https://console.cloud.google.com/cloud-build/builds;region=us-central1\n",
      "Please wait for this build job to complete.\n",
      "\n",
      "#################################################################\n",
      "#                                                               #\n",
      "#                       RESOURCES MANIFEST                      #\n",
      "#---------------------------------------------------------------#\n",
      "#     Generated resources can be found at the following urls    #\n",
      "#                                                               #\n",
      "#################################################################\n",
      "\n",
      "Google Cloud Storage Bucket: https://console.cloud.google.com/storage/automlops-sandbox-flan-t5-samsum-bucket\n",
      "Artifact Registry: https://console.cloud.google.com/artifacts/docker/automlops-sandbox/us-central1/flan-t5-samsum-artifact-registry\n",
      "Service Accounts: https://console.cloud.google.com/iam-admin/serviceaccounts?project=automlops-sandbox\n",
      "APIs: https://console.cloud.google.com/apis\n",
      "Cloud Source Repository: https://source.cloud.google.com/automlops-sandbox/flan-t5-samsum-repository/+/automlops:\n",
      "Cloud Build Jobs: https://console.cloud.google.com/cloud-build/builds;region=us-central1\n",
      "Vertex AI Pipeline Runs: https://console.cloud.google.com/vertex-ai/pipelines/runs\n",
      "Cloud Build Trigger: https://console.cloud.google.com/cloud-build/triggers;region=us-central1\n",
      "Pipeline Job Submission Service (Cloud Functions): https://console.cloud.google.com/functions/details/us-central1/flan-t5-samsum-job-submission-svc\n",
      "Pub/Sub Queueing Service Topic: https://console.cloud.google.com/cloudpubsub/topic/detail/flan-t5-samsum-queueing-svc\n",
      "Pub/Sub Queueing Service Subscriptions: https://console.cloud.google.com/cloudpubsub/subscription/list\n",
      "Cloud Scheduler Job: https://console.cloud.google.com/cloudscheduler\n"
     ]
    }
   ],
   "source": [
    "AutoMLOps.deploy(precheck=True,                     # precheck is optional, defaults to True\n",
    "                 hide_warnings=False)               # hide_warnings is optional, defaults to True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb21b9e-deec-4288-a234-feeee8289728",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m102",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m102"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
